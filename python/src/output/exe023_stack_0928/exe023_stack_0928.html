<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>exe023_stack_0928</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iLTEwIC0xMCAxMzEuMTYxMzYxNjk0MzM1OTQgMTMyLjM4ODk5OTkzODk2NDg0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzA2OTk4IiBkPSJNIDU0LjkxODc4NSw5LjE5Mjc0MjFlLTQgQyA1MC4zMzUxMzIsMC4wMjIyMTcyNyA0NS45NTc4NDYsMC40MTMxMzY5NyA0Mi4xMDYyODUsMS4wOTQ2NjkzIDMwLjc2MDA2OSwzLjA5OTE3MzEgMjguNzAwMDM2LDcuMjk0NzcxNCAyOC43MDAwMzUsMTUuMDMyMTY5IHYgMTAuMjE4NzUgaCAyNi44MTI1IHYgMy40MDYyNSBoIC0yNi44MTI1IC0xMC4wNjI1IGMgLTcuNzkyNDU5LDAgLTE0LjYxNTc1ODgsNC42ODM3MTcgLTE2Ljc0OTk5OTgsMTMuNTkzNzUgLTIuNDYxODE5OTgsMTAuMjEyOTY2IC0yLjU3MTAxNTA4LDE2LjU4NjAyMyAwLDI3LjI1IDEuOTA1OTI4Myw3LjkzNzg1MiA2LjQ1NzU0MzIsMTMuNTkzNzQ4IDE0LjI0OTk5OTgsMTMuNTkzNzUgaCA5LjIxODc1IHYgLTEyLjI1IGMgMCwtOC44NDk5MDIgNy42NTcxNDQsLTE2LjY1NjI0OCAxNi43NSwtMTYuNjU2MjUgaCAyNi43ODEyNSBjIDcuNDU0OTUxLDAgMTMuNDA2MjUzLC02LjEzODE2NCAxMy40MDYyNSwtMTMuNjI1IHYgLTI1LjUzMTI1IGMgMCwtNy4yNjYzMzg2IC02LjEyOTk4LC0xMi43MjQ3NzcxIC0xMy40MDYyNSwtMTMuOTM3NDk5NyBDIDY0LjI4MTU0OCwwLjMyNzk0Mzk3IDU5LjUwMjQzOCwtMC4wMjAzNzkwMyA1NC45MTg3ODUsOS4xOTI3NDIxZS00IFogbSAtMTQuNSw4LjIxODc1MDEyNTc5IGMgMi43Njk1NDcsMCA1LjAzMTI1LDIuMjk4NjQ1NiA1LjAzMTI1LDUuMTI0OTk5NiAtMmUtNiwyLjgxNjMzNiAtMi4yNjE3MDMsNS4wOTM3NSAtNS4wMzEyNSw1LjA5Mzc1IC0yLjc3OTQ3NiwtMWUtNiAtNS4wMzEyNSwtMi4yNzc0MTUgLTUuMDMxMjUsLTUuMDkzNzUgLTEwZS03LC0yLjgyNjM1MyAyLjI1MTc3NCwtNS4xMjQ5OTk2IDUuMDMxMjUsLTUuMTI0OTk5NiB6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2ZmZDQzYiIgZD0ibSA4NS42Mzc1MzUsMjguNjU3MTY5IHYgMTEuOTA2MjUgYyAwLDkuMjMwNzU1IC03LjgyNTg5NSwxNi45OTk5OTkgLTE2Ljc1LDE3IGggLTI2Ljc4MTI1IGMgLTcuMzM1ODMzLDAgLTEzLjQwNjI0OSw2LjI3ODQ4MyAtMTMuNDA2MjUsMTMuNjI1IHYgMjUuNTMxMjQ3IGMgMCw3LjI2NjM0NCA2LjMxODU4OCwxMS41NDAzMjQgMTMuNDA2MjUsMTMuNjI1MDA0IDguNDg3MzMxLDIuNDk1NjEgMTYuNjI2MjM3LDIuOTQ2NjMgMjYuNzgxMjUsMCA2Ljc1MDE1NSwtMS45NTQzOSAxMy40MDYyNTMsLTUuODg3NjEgMTMuNDA2MjUsLTEzLjYyNTAwNCBWIDg2LjUwMDkxOSBoIC0yNi43ODEyNSB2IC0zLjQwNjI1IGggMjYuNzgxMjUgMTMuNDA2MjU0IGMgNy43OTI0NjEsMCAxMC42OTYyNTEsLTUuNDM1NDA4IDEzLjQwNjI0MSwtMTMuNTkzNzUgMi43OTkzMywtOC4zOTg4ODYgMi42ODAyMiwtMTYuNDc1Nzc2IDAsLTI3LjI1IC0xLjkyNTc4LC03Ljc1NzQ0MSAtNS42MDM4NywtMTMuNTkzNzUgLTEzLjQwNjI0MSwtMTMuNTkzNzUgeiBtIC0xNS4wNjI1LDY0LjY1NjI1IGMgMi43Nzk0NzgsM2UtNiA1LjAzMTI1LDIuMjc3NDE3IDUuMDMxMjUsNS4wOTM3NDcgLTJlLTYsMi44MjYzNTQgLTIuMjUxNzc1LDUuMTI1MDA0IC01LjAzMTI1LDUuMTI1MDA0IC0yLjc2OTU1LDAgLTUuMDMxMjUsLTIuMjk4NjUgLTUuMDMxMjUsLTUuMTI1MDA0IDJlLTYsLTIuODE2MzMgMi4yNjE2OTcsLTUuMDkzNzQ3IDUuMDMxMjUsLTUuMDkzNzQ3IHoiLz4KPC9zdmc+Cg==);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>.venv (Python 3.12.6) </p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=9b590716-b7e1-4668-8cfe-a4fe169fb988">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="%E7%8F%BE%E4%B8%80%E7%95%AA%E3%81%84%E3%81%84%E5%88%B6%E5%BA%A6%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%A7%E3%82%82%E3%81%A3%E3%81%8B%E3%81%84%E3%82%B9%E3%82%BF%E3%83%83%E3%82%AD%E3%83%B3%E3%82%B0%EF%BC%81-svm%E5%A4%96%E3%81%99"> svm<a class="anchor-link" href="#%E7%8F%BE%E4%B8%80%E7%95%AA%E3%81%84%E3%81%84%E5%88%B6%E5%BA%A6%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%A7%E3%82%82%E3%81%A3%E3%81%8B%E3%81%84%E3%82%B9%E3%82%BF%E3%83%83%E3%82%AD%E3%83%B3%E3%82%B0%EF%BC%81-svm%E5%A4%96%E3%81%99"></a></h1><p>=================================================
tf,lgbm</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=059d2235-4efb-4b23-9d74-ab9bbcfd0bf2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># =================================================</span>
<span class="kn">import</span> <span class="nn">datetime</span> <span class="k">as</span> <span class="nn">dt</span>

<span class="c1"># import gc</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>

<span class="c1"># import json</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># import re</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="c1"># import zipfile</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="c1"># </span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">japanize_matplotlib</span>


<span class="c1"># sckit-learn</span>
<span class="c1"># </span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># , MinMaxScaler, LabelEncoder, OneHotEncoder</span>

<span class="c1"># </span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>  <span class="c1"># ,train_test_split, KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>  <span class="c1"># accuracy_score,confusion_matrix</span>

<span class="c1"># tensorflow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.python.keras.backend</span> <span class="k">as</span> <span class="nn">K</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Model</span>  <span class="c1"># type:ignore</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">BatchNormalization</span>  <span class="c1"># type:ignore</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Concatenate</span>  <span class="c1"># type:ignore</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">EarlyStopping</span><span class="p">,</span>
    <span class="n">ModelCheckpoint</span><span class="p">,</span>
    <span class="n">ReduceLROnPlateau</span><span class="p">,</span>
    <span class="n">LearningRateScheduler</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># type:ignore</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span><span class="p">,</span> <span class="n">SGD</span>  <span class="c1"># type:ignore</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.metrics</span> <span class="kn">import</span> <span class="n">AUC</span>

<span class="c1"># </span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="c1"># from sklearn.manifold import TSNE</span>
<span class="c1"># import umap</span>

<span class="c1"># </span>
<span class="c1"># from sklearn.svm import SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="c1"># lightGBM</span>
<span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>
<span class="c1"># lightGBM</span>
<span class="c1"># import shap</span>

<span class="c1"># </span>
<span class="c1"># import optuna</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>2024-09-28 11:33:22.872728: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-28 11:33:24.217244: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-28 11:33:25.574781: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 11:33:26.686274: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 11:33:26.981081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 11:33:29.099432: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 11:33:43.082386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=4e2763a7-128d-4ac0-95a0-09f3a46e7cff">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># Config</span>
<span class="c1"># =================================================</span>

<span class="c1">######################</span>
<span class="c1"># serial #</span>
<span class="c1">######################</span>
<span class="n">serial_number</span> <span class="o">=</span> <span class="mi">27</span>  <span class="c1"># A</span>

<span class="c1">######################</span>
<span class="c1"># Data #</span>
<span class="c1">######################</span>
<span class="n">comp_name</span> <span class="o">=</span> <span class="s2">"Chronic_liver_disease"</span>
<span class="c1"># AUCArea Under the Curve</span>

<span class="n">skip_run</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># -&gt;True-&gt;False</span>

<span class="c1">######################</span>
<span class="c1"># filename</span>
<span class="c1">######################</span>
<span class="c1"># vscode</span>
<span class="n">abs_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>  <span class="c1"># /tmp/work/src/exp/_.py'</span>
<span class="n">name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">abs_path</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># Google Colab</span>
<span class="c1"># name = 'run001'</span>

<span class="c1">######################</span>
<span class="c1"># set dirs #</span>
<span class="c1">######################</span>
<span class="n">DRIVE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>  <span class="c1"># (scr)</span>
<span class="n">INPUT_PATH</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"../input/</span><span class="si">{</span><span class="n">comp_name</span><span class="si">}</span><span class="s2">/"</span>  <span class="c1"># </span>
<span class="n">OUTPUT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DRIVE</span><span class="p">,</span> <span class="s2">"output"</span><span class="p">)</span>
<span class="n">OUTPUT_EXP</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">OUTPUT</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>  <span class="c1"># </span>
<span class="n">EXP_MODEL</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">OUTPUT_EXP</span><span class="p">,</span> <span class="s2">"model"</span><span class="p">)</span>  <span class="c1"># </span>

<span class="c1">######################</span>
<span class="c1"># Dataset #</span>
<span class="c1">######################</span>
<span class="n">target_columns</span> <span class="o">=</span> <span class="s2">"disease"</span>
<span class="c1"># sub_index = "index"</span>

<span class="c1">######################</span>
<span class="c1"># </span>
<span class="c1">######################</span>
<span class="c1"># lgbm</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"boosting_type"</span><span class="p">:</span> <span class="s2">"gbdt"</span><span class="p">,</span>
    <span class="s2">"objective"</span><span class="p">:</span> <span class="s2">"binary"</span><span class="p">,</span>
    <span class="s2">"metric"</span><span class="p">:</span> <span class="s2">"auc"</span><span class="p">,</span>
    <span class="s2">"learning_rate"</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="s2">"num_leaves"</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">"n_estimators"</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="s2">"random_state"</span><span class="p">:</span> <span class="mi">123</span><span class="p">,</span>
    <span class="s2">"importance_type"</span><span class="p">:</span> <span class="s2">"gain"</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6aaf81ee-899f-42db-8624-d3d6608f167d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># Utilities #</span>
<span class="c1"># =================================================</span>


<span class="c1"># </span>
<span class="k">def</span> <span class="nf">dt_now</span><span class="p">():</span>
    <span class="n">dt_now</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">dt_now</span>


<span class="c1"># stdout  stderr </span>
<span class="k">class</span> <span class="nc">StreamToLogger</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logger</span><span class="p">,</span> <span class="n">level</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logger</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">level</span> <span class="o">=</span> <span class="n">level</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linebuf</span> <span class="o">=</span> <span class="s2">""</span>

    <span class="k">def</span> <span class="nf">write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">buf</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">buf</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">level</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">flush</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0b311420-17ff-4714-99fd-97e56b585af8">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># make dirs</span>
<span class="c1"># =================================================</span>
<span class="k">def</span> <span class="nf">make_dirs</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="n">EXP_MODEL</span><span class="p">]:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=62f74a16-1465-473c-abc7-faf25694b56f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># =================================================</span>
<span class="k">def</span> <span class="nf">file_list</span><span class="p">(</span><span class="n">input_path</span><span class="p">):</span>
    <span class="n">file_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">dirname</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_filenames</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="n">input_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_datafilename</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_filenames</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">_datafilename</span><span class="p">)</span>
            <span class="n">file_list</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">_datafilename</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirname</span><span class="p">,</span> <span class="n">_datafilename</span><span class="p">)])</span>
    <span class="n">file_list</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">file_list</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">""</span><span class="p">,</span> <span class="s2">""</span><span class="p">])</span>
    <span class="n">display</span><span class="p">(</span><span class="n">file_list</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">file_list</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=54117367-7935-48a1-9fab-2f73c70324f3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># =================================================</span>
<span class="k">def</span> <span class="nf">reduce_mem_usage</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">start_mem</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">memory_usage</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">2</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Memory usage of dataframe is </span><span class="si">{</span><span class="n">start_mem</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB"</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">col_type</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>

        <span class="k">if</span> <span class="n">col_type</span> <span class="o">!=</span> <span class="nb">object</span><span class="p">:</span>  <span class="c1"># noqa: E721</span>
            <span class="n">c_min</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
            <span class="n">c_max</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">str</span><span class="p">(</span><span class="n">col_type</span><span class="p">)[:</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"int"</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">c_min</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">min</span> <span class="ow">and</span> <span class="n">c_max</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
                    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">c_min</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span><span class="o">.</span><span class="n">min</span> <span class="ow">and</span> <span class="n">c_max</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
                    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">c_min</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">min</span> <span class="ow">and</span> <span class="n">c_max</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
                    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">c_min</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">min</span> <span class="ow">and</span> <span class="n">c_max</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
                    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">c_min</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
                    <span class="ow">and</span> <span class="n">c_max</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
                <span class="p">):</span>
                    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
                <span class="k">elif</span> <span class="p">(</span>
                    <span class="n">c_min</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
                    <span class="ow">and</span> <span class="n">c_max</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
                <span class="p">):</span>
                    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astypez</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="n">end_mem</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">memory_usage</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">2</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Memory usage after optimization is: </span><span class="si">{</span><span class="n">end_mem</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Decreased by </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">((</span><span class="n">start_mem</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">end_mem</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">start_mem</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2b746a8d-fa28-49ce-844c-367ec17a44f5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># =================================================</span>
<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">file_index</span><span class="p">):</span>
    <span class="c1"># file_index</span>
    <span class="k">if</span> <span class="n">file_list</span><span class="p">[</span><span class="s2">""</span><span class="p">][</span><span class="n">file_index</span><span class="p">][</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span> <span class="o">==</span> <span class="s2">"csv"</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">file_list</span><span class="p">[</span><span class="s1">''</span><span class="p">][</span><span class="n">file_index</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">reduce_mem_usage</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file_list</span><span class="p">[</span><span class="s2">""</span><span class="p">][</span><span class="n">file_index</span><span class="p">]))</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

    <span class="k">elif</span> <span class="n">file_list</span><span class="p">[</span><span class="s2">""</span><span class="p">][</span><span class="n">file_index</span><span class="p">][</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span> <span class="o">==</span> <span class="s2">"pkl"</span> <span class="ow">or</span> <span class="s2">"pickle"</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">file_list</span><span class="p">[</span><span class="s1">''</span><span class="p">][</span><span class="n">file_index</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">reduce_mem_usage</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="n">file_list</span><span class="p">[</span><span class="s2">""</span><span class="p">][</span><span class="n">file_index</span><span class="p">]))</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c071c299-9484-4c63-8792-5ed8dd80c181">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1">#  category</span>
<span class="c1"># =================================================</span>
<span class="k">def</span> <span class="nf">data_pre00</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">"O"</span><span class="p">:</span>
            <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">"category"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"category"</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c87f2ee3-4483-47d3-83ce-5e21b432dc80">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1">#  exp002</span>
<span class="c1"># =================================================</span>
<span class="k">def</span> <span class="nf">data_pre01</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># 1:/D/T 3/2</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"D/T_ex2"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"D_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span>

    <span class="c1"># 2:AST/ALTDe Ritis # 6/5</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"AST/ALT_ex2"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AST_GOT"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>

    <span class="c1"># 3:./AST  7/6</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TP/AST_ex2"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"TP"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AST_GOT"</span><span class="p">]</span>

    <span class="c1"># 4:.  1/(8*9)</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"Globulin_ex2"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">"Alb"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AG_ratio"</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">":"</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"()"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=7129826e-8a9c-41b7-b1aa-25b750165b57">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1">#  exp03</span>
<span class="c1"># =================================================</span>
<span class="k">def</span> <span class="nf">data_pre02</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># 1:/ / ALT   / AST 2/5 2/6</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TB/ALT_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TB/AST_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AST_GOT"</span><span class="p">]</span>

    <span class="c1"># 2: /ALP  2/4</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TB/ALP_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALP"</span><span class="p">]</span>

    <span class="c1"># 3:/ALT 8/5</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"Alb/ALT_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"Alb"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>

    <span class="c1"># 4:/ALT 7/5</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TP/ALT_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"TP"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>

    <span class="c1"># 5:ALP/ASTALP/ALT 4/6 4/5</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"ALP/AST_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALP"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AST_GOT"</span><span class="p">]</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"ALP/ALT_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALP"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>

    <span class="c1"># 6: /  2/8</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TB/Alb_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"Alb"</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">":"</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"()"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=42a0e059-760e-40a1-92ad-91ef78c99d06">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">data_pre03</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># Gender</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"Gender"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">"Gender"</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">"uint8"</span><span class="p">)</span>

    <span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">,</span> <span class="s2">"D_Bil"</span><span class="p">,</span> <span class="s2">"ALP"</span><span class="p">,</span> <span class="s2">"ALT_GPT"</span><span class="p">,</span> <span class="s2">"AST_GOT"</span><span class="p">]:</span>
        <span class="n">df2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1">#  pca,UMAP </span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span>
    <span class="n">df_std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

    <span class="n">n_components</span> <span class="o">=</span> <span class="mi">8</span>

    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_std</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_components</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_components</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"components"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"components"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"cumulative explained variance"</span><span class="p">)</span>

    <span class="n">X_pc</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_std</span><span class="p">)</span>

    <span class="n">df_pc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X_pc</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">df_pc</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"pc01"</span><span class="p">,</span> <span class="s2">"pc02"</span><span class="p">,</span> <span class="s2">"pc03"</span><span class="p">,</span> <span class="s2">"pc04"</span><span class="p">,</span> <span class="s2">"pc05"</span><span class="p">,</span> <span class="s2">"pc06"</span><span class="p">,</span> <span class="s2">"pc07"</span><span class="p">,</span> <span class="s2">"pc08"</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">":"</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">df_pc</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">":"</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=de4f5759-e35a-4dee-99dd-e478ff3d922b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># =================================================</span>
<span class="c1"># lgbm</span>
<span class="k">def</span> <span class="nf">train_lgb</span><span class="p">(</span>
    <span class="n">input_x</span><span class="p">,</span>
    <span class="n">input_y</span><span class="p">,</span>
    <span class="n">input_id</span><span class="p">,</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">imp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">))</span>

    <span class="c1"># cross-validation</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># 1.</span>
    <span class="k">for</span> <span class="n">nfold</span> <span class="ow">in</span> <span class="n">list_nfold</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>

        <span class="n">idx_tr</span><span class="p">,</span> <span class="n">idx_va</span> <span class="o">=</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_va</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_va</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="n">x_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_va</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># </span>
        <span class="n">fname_lgb</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">EXP_MODEL</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"model_lgb_fold</span><span class="si">{</span><span class="n">nfold</span><span class="si">}</span><span class="s2">.pickle"</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">EXP_MODEL</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"model_lgb_fold</span><span class="si">{</span><span class="n">nfold</span><span class="si">}</span><span class="s2">.pickle"</span><span class="p">)</span>
        <span class="p">):</span>  <span class="c1"># if trained model, no training</span>
            <span class="c1"># train</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"-------training start-------"</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">x_tr</span><span class="p">,</span>
                <span class="n">y_tr</span><span class="p">,</span>
                <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">),</span> <span class="p">(</span><span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">)],</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="n">stopping_rounds</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                    <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
                <span class="p">],</span>
            <span class="p">)</span>

            <span class="c1"># </span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname_lgb</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname_lgb</span><span class="p">,</span> <span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="c1"># evaluate</span>
        <span class="n">y_tr_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_tr</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">y_va_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_va</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">metric_tr</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">y_tr_pred</span><span class="p">)</span>
        <span class="n">metric_va</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_va</span><span class="p">,</span> <span class="n">y_va_pred</span><span class="p">)</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">nfold</span><span class="p">,</span> <span class="n">metric_tr</span><span class="p">,</span> <span class="n">metric_va</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[auc] tr:</span><span class="si">{</span><span class="n">metric_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, va:</span><span class="si">{</span><span class="n">metric_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="c1"># oof</span>
        <span class="n">train_oof</span><span class="p">[</span><span class="n">idx_va</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_va_pred</span>

        <span class="c1"># imp</span>
        <span class="n">_imp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">"col"</span><span class="p">:</span> <span class="n">input_x</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="s2">"imp"</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="s2">"nfold"</span><span class="p">:</span> <span class="n">nfold</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="n">imp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">imp</span><span class="p">,</span> <span class="n">_imp</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>

    <span class="c1"># metric</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">oof</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"[oof]</span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span><span class="w"> </span><span class="n">train_oof</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># oof</span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">input_id</span><span class="p">,</span>
            <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"pred"</span><span class="p">:</span> <span class="n">train_oof</span><span class="p">}),</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># importance</span>
    <span class="n">imp</span> <span class="o">=</span> <span class="n">imp</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">"col"</span><span class="p">)[</span><span class="s2">"imp"</span><span class="p">]</span><span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="s2">"mean"</span><span class="p">,</span> <span class="s2">"std"</span><span class="p">])</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">imp</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"col"</span><span class="p">,</span> <span class="s2">"imp"</span><span class="p">,</span> <span class="s2">"imp_std"</span><span class="p">]</span>

    <span class="c1"># stdout  stderr </span>
    <span class="n">stdout_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDOUT"</span><span class="p">)</span>
    <span class="n">stderr_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDERR"</span><span class="p">)</span>

    <span class="n">sys_stdout_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span>
    <span class="n">sys_stderr_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span>

    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stdout_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stderr_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"importance"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">imp</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">"imp"</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:</span><span class="mi">10</span><span class="p">])</span>

    <span class="c1"># </span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">sys_stdout_backup</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">sys_stderr_backup</span>

    <span class="k">return</span> <span class="n">train_oof</span><span class="p">,</span> <span class="n">imp</span><span class="p">,</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=576ddef3-1667-4e8b-a995-1528c4db1df6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">input_num</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,))</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">input_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"sigmoid"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">input_num</span><span class="p">,</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="s2">"Adam"</span><span class="p">,</span>
        <span class="n">loss</span><span class="o">=</span><span class="s2">"binary_crossentropy"</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">AUC</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"roc_auc"</span><span class="p">)],</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>


<span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"PYTHONHASHSEED"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">session_conf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
        <span class="n">intra_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inter_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span>
        <span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">(),</span> <span class="n">config</span><span class="o">=</span><span class="n">session_conf</span>
    <span class="p">)</span>
    <span class="c1"># tf.compat.v1.keras.backend.set_session(sess)</span>
    <span class="n">K</span><span class="o">.</span><span class="n">set_session</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_tf</span><span class="p">(</span>
    <span class="n">input_x</span><span class="p">,</span>
    <span class="n">input_y</span><span class="p">,</span>
    <span class="n">input_id</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># </span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">))</span>
    <span class="c1"># </span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># validation</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># 1.</span>
    <span class="k">for</span> <span class="n">nfold</span> <span class="ow">in</span> <span class="n">list_nfold</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>

        <span class="n">idx_tr</span><span class="p">,</span> <span class="n">idx_va</span> <span class="o">=</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_va</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_va</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="n">x_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_va</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># </span>
        <span class="n">fname_tf</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">EXP_MODEL</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"model_tf_fold</span><span class="si">{</span><span class="n">nfold</span><span class="si">}</span><span class="s2">.weights.h5"</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">):</span>
            <span class="c1"># if mode_train == 'train':</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"trainning start!"</span><span class="p">)</span>
            <span class="n">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x_tr</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span>
                <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">),</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">ModelCheckpoint</span><span class="p">(</span>
                        <span class="n">filepath</span><span class="o">=</span><span class="n">fname_tf</span><span class="p">,</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">EarlyStopping</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">min_delta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">ReduceLROnPlateau</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">],</span>
                <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"model load."</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">)</span>

        <span class="c1"># valid</span>
        <span class="n">y_tr_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_tr</span><span class="p">)</span>
        <span class="n">y_va_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_va</span><span class="p">)</span>

        <span class="n">metric_tr</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">y_tr_pred</span><span class="p">)</span>
        <span class="n">metric_va</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_va</span><span class="p">,</span> <span class="n">y_va_pred</span><span class="p">)</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">nfold</span><span class="p">,</span> <span class="n">metric_tr</span><span class="p">,</span> <span class="n">metric_va</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[auc] tr:</span><span class="si">{</span><span class="n">metric_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, va:</span><span class="si">{</span><span class="n">metric_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="c1"># oof</span>
        <span class="n">train_oof</span><span class="p">[</span><span class="n">idx_va</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_va_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
    <span class="c1"># metric</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">oof</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"[oof]</span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span><span class="w"> </span><span class="n">train_oof</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># oof</span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">input_id</span><span class="p">,</span>
            <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"pred"</span><span class="p">:</span> <span class="n">train_oof</span><span class="p">}),</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># stdout  stderr </span>
    <span class="n">stdout_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDOUT"</span><span class="p">)</span>
    <span class="n">stderr_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDERR"</span><span class="p">)</span>

    <span class="n">sys_stdout_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span>
    <span class="n">sys_stderr_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span>

    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stdout_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stderr_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># </span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">sys_stdout_backup</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">sys_stderr_backup</span>

    <span class="k">return</span> <span class="n">train_oof</span><span class="p">,</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=541a6a83-6c5a-4faa-94fb-f131b712eb0a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># svm</span>
<span class="c1"># def train_svm(</span>
<span class="c1">#     input_x,</span>
<span class="c1">#     input_y,</span>
<span class="c1">#     input_id,</span>
<span class="c1">#     list_nfold=[0, 1, 2, 3, 4],</span>
<span class="c1">#     n_splits=5,</span>
<span class="c1"># ):</span>
<span class="c1">#     metrics = []</span>
<span class="c1">#     train_oof = np.zeros(len(input_x))</span>

<span class="c1">#     # cross-validation</span>
<span class="c1">#     cv = list(</span>
<span class="c1">#         StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123).split(</span>
<span class="c1">#             input_x, input_y</span>
<span class="c1">#         )</span>
<span class="c1">#     )</span>

<span class="c1">#     # 1.</span>
<span class="c1">#     for nfold in list_nfold:</span>
<span class="c1">#         print("-" * 20, nfold, "-" * 20)</span>
<span class="c1">#         print(dt_now().strftime("%Y%m%d %H:%M:%S"))</span>

<span class="c1">#         idx_tr, idx_va = cv[nfold][0], cv[nfold][1]</span>

<span class="c1">#         x_tr, y_tr = (</span>
<span class="c1">#             input_x.loc[idx_tr, :],</span>
<span class="c1">#             input_y[idx_tr],</span>
<span class="c1">#         )</span>
<span class="c1">#         x_va, y_va = (</span>
<span class="c1">#             input_x.loc[idx_va, :],</span>
<span class="c1">#             input_y[idx_va],</span>
<span class="c1">#         )</span>

<span class="c1">#         print(x_tr.shape, x_va.shape)</span>

<span class="c1">#         # </span>
<span class="c1">#         fname_svm = os.path.join(EXP_MODEL, f"model_svm_fold{nfold}.pickle")</span>

<span class="c1">#         if not os.path.isfile(fname_svm):  # if trained model, no training</span>
<span class="c1">#             # train</span>
<span class="c1">#             print("-------training start-------")</span>
<span class="c1">#             model_svm = SVC(C=1.0, random_state=123, probability=True)</span>
<span class="c1">#             model_svm.fit(x_tr, y_tr)</span>

<span class="c1">#             # </span>
<span class="c1">#             with open(fname_svm, "wb") as f:</span>
<span class="c1">#                 pickle.dump(model_svm, f, protocol=4)</span>

<span class="c1">#         else:</span>
<span class="c1">#             print("")</span>
<span class="c1">#             with open(fname_svm, "rb") as f:</span>
<span class="c1">#                 model_svm = pickle.load(f)</span>

<span class="c1">#         # evaluate</span>
<span class="c1">#         y_tr_pred = model_svm.predict_proba(x_tr)[:, 1]</span>
<span class="c1">#         y_va_pred = model_svm.predict_proba(x_va)[:, 1]</span>
<span class="c1">#         metric_tr = roc_auc_score(y_tr, y_tr_pred)</span>
<span class="c1">#         metric_va = roc_auc_score(y_va, y_va_pred)</span>
<span class="c1">#         metrics.append([nfold, metric_tr, metric_va])</span>
<span class="c1">#         print(f"[auc] tr:{metric_tr:.4f}, va:{metric_va:.4f}")</span>

<span class="c1">#         # oof</span>
<span class="c1">#         train_oof[idx_va] = y_va_pred</span>

<span class="c1">#     print("-" * 20, "result", "-" * 20)</span>

<span class="c1">#     # metric</span>
<span class="c1">#     metrics = np.array(metrics)</span>
<span class="c1">#     print(metrics)</span>
<span class="c1">#     print(f"[cv] tr:{metrics[:,1].mean():.4f}+-{metrics[:,1].std():.4f}, \</span>
<span class="c1">#         va:{metrics[:,2].mean():.4f}+-{metrics[:,1].std():.4f}")</span>

<span class="c1">#     oof = f"[oof]{roc_auc_score(input_y, train_oof):.4f}"</span>

<span class="c1">#     print(oof)</span>

<span class="c1">#     # oof</span>
<span class="c1">#     train_oof = pd.concat(</span>
<span class="c1">#         [</span>
<span class="c1">#             input_id,</span>
<span class="c1">#             pd.DataFrame({"pred": train_oof}),</span>
<span class="c1">#         ],</span>
<span class="c1">#         axis=1,</span>
<span class="c1">#     )</span>

<span class="c1">#     # stdout  stderr </span>
<span class="c1">#     stdout_logger = logging.getLogger("STDOUT")</span>
<span class="c1">#     stderr_logger = logging.getLogger("STDERR")</span>

<span class="c1">#     sys_stdout_backup = sys.stdout</span>
<span class="c1">#     sys_stderr_backup = sys.stderr</span>

<span class="c1">#     sys.stdout = StreamToLogger(stdout_logger, logging.INFO)</span>
<span class="c1">#     sys.stderr = StreamToLogger(stderr_logger, logging.ERROR)</span>
<span class="c1">#     print("-" * 20, "result", "-" * 20)</span>
<span class="c1">#     print(dt_now().strftime("%Y%m%d %H:%M:%S"))</span>
<span class="c1">#     print(name)</span>
<span class="c1">#     print(input_x.shape)</span>
<span class="c1">#     print(metrics)</span>
<span class="c1">#     print(f"[cv] tr:{metrics[:,1].mean():.4f}+-{metrics[:,1].std():.4f}, \</span>
<span class="c1">#         va:{metrics[:,2].mean():.4f}+-{metrics[:,1].std():.4f}")</span>

<span class="c1">#     print(oof)</span>

<span class="c1">#     # </span>
<span class="c1">#     sys.stdout = sys_stdout_backup</span>
<span class="c1">#     sys.stderr = sys_stderr_backup</span>

<span class="c1">#     return train_oof, metrics</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=48f92459-bcc3-42ff-a24f-91e71cc15542">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># set up</span>
<span class="c1"># =================================================</span>
<span class="c1"># utils</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font</span><span class="o">=</span><span class="s2">"IPAexGothic"</span><span class="p">)</span>
<span class="c1">###!%matplotlib inline</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s2">"</span><span class="si">{:10.4f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span>  <span class="c1"># </span>

<span class="c1"># </span>
<span class="n">make_dirs</span><span class="p">()</span>
<span class="c1"># </span>
<span class="n">file_list</span> <span class="o">=</span> <span class="n">file_list</span><span class="p">(</span><span class="n">INPUT_PATH</span><span class="p">)</span>

<span class="c1"># utils</span>
<span class="c1"># </span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
    <span class="n">filename</span><span class="o">=</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">OUTPUT_EXP</span><span class="si">}</span><span class="s2">/log_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.txt"</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">"</span><span class="si">%(message)s</span><span class="s2">"</span>
<span class="p">)</span>
<span class="c1"># </span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>


<span class="c1"># </span>
<span class="c1"># pd.set_option('display.max_rows',None)</span>
<span class="c1"># pd.set_option('display.max_columns',None)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
====================
0 sample_submit.csv
====================
1 test.csv
====================
2 train.csv
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>sample_submit.csv</td>
<td>../input/Chronic_liver_disease/sample_submit.csv</td>
</tr>
<tr>
<th>1</th>
<td>test.csv</td>
<td>../input/Chronic_liver_disease/test.csv</td>
</tr>
<tr>
<th>2</th>
<td>train.csv</td>
<td>../input/Chronic_liver_disease/train.csv</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=e05ee55e-d9c1-421b-ac0c-17c589c5600f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># Load Data</span>
<span class="c1"># =================================================</span>
<span class="c1"># train</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>train.csv
Memory usage of dataframe is 0.07 MB
Memory usage after optimization is: 0.02 MB
Decreased by 70.33%
(850, 11)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Age</th>
<th>Gender</th>
<th>T_Bil</th>
<th>D_Bil</th>
<th>ALP</th>
<th>ALT_GPT</th>
<th>AST_GOT</th>
<th>TP</th>
<th>Alb</th>
<th>AG_ratio</th>
<th>disease</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>59</td>
<td>Male</td>
<td>0.7871</td>
<td>0.1505</td>
<td>220.1250</td>
<td>13.4688</td>
<td>21.7344</td>
<td>6.8164</td>
<td>3.1113</td>
<td>1.0068</td>
<td>0</td>
</tr>
<tr>
<th>1</th>
<td>69</td>
<td>Male</td>
<td>1.0039</td>
<td>0.1957</td>
<td>221.2500</td>
<td>51.0312</td>
<td>64.7500</td>
<td>6.8906</td>
<td>3.0508</td>
<td>0.7515</td>
<td>1</td>
</tr>
<tr>
<th>2</th>
<td>65</td>
<td>Male</td>
<td>0.6572</td>
<td>0.0813</td>
<td>320.7500</td>
<td>12.6250</td>
<td>30.6094</td>
<td>5.9492</td>
<td>2.4883</td>
<td>0.7749</td>
<td>0</td>
</tr>
<tr>
<th>3</th>
<td>65</td>
<td>Male</td>
<td>0.9067</td>
<td>0.2142</td>
<td>369.2500</td>
<td>34.3438</td>
<td>54.5000</td>
<td>6.9688</td>
<td>3.6133</td>
<td>0.9883</td>
<td>1</td>
</tr>
<tr>
<th>4</th>
<td>22</td>
<td>Female</td>
<td>1.7354</td>
<td>0.1978</td>
<td>222.7500</td>
<td>20.5781</td>
<td>170.0000</td>
<td>5.8359</td>
<td>3.0684</td>
<td>1.0264</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=ad82ede5-08b1-45fd-bd0c-eaf20551c30c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">df_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">df_train</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>(850, 11)</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 850 entries, 0 to 849
Data columns (total 11 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   Age       850 non-null    int8   
 1   Gender    850 non-null    object 
 2   T_Bil     850 non-null    float16
 3   D_Bil     850 non-null    float16
 4   ALP       850 non-null    float16
 5   ALT_GPT   850 non-null    float16
 6   AST_GOT   850 non-null    float16
 7   TP        850 non-null    float16
 8   Alb       850 non-null    float16
 9   AG_ratio  850 non-null    float16
 10  disease   850 non-null    int8   
dtypes: float16(8), int8(2), object(1)
memory usage: 21.7+ KB
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=3f732e4d-c1b7-44e1-8c95-f409fd59cfe0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># ==========================================================</span>
<span class="c1"># 4</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">data_pre01</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>

<span class="c1"># 8</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">data_pre02</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>: (850, 15)
()
========================================
: (850, 23)
()
========================================
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=7182aff8-76c0-433a-b0e5-42ffa9352547">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Main-%E5%88%86%E6%9E%90start!">Main start!<a class="anchor-link" href="#Main-%E5%88%86%E6%9E%90start!"></a></h1><p>=========================================================</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=871f1b9d-8b6c-4dcf-b0f4-d29a8be78b7c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># =================================================</span>
<span class="n">set_file</span> <span class="o">=</span> <span class="n">df_train</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">set_file</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="n">target_columns</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">set_file</span><span class="p">[</span><span class="n">target_columns</span><span class="p">]</span>
<span class="n">id_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">set_file</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">id_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>(850, 22) (850,) (850, 1)
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2202a1dd-d33b-4082-868c-bb3a4c514ed2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># category</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data_pre00</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

<span class="c1"># =&gt;PCA</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data_pre03</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>category
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 850 entries, 0 to 849
Data columns (total 22 columns):
 #   Column        Non-Null Count  Dtype   
---  ------        --------------  -----   
 0   Age           850 non-null    int8    
 1   Gender        850 non-null    category
 2   T_Bil         850 non-null    float16 
 3   D_Bil         850 non-null    float16 
 4   ALP           850 non-null    float16 
 5   ALT_GPT       850 non-null    float16 
 6   AST_GOT       850 non-null    float16 
 7   TP            850 non-null    float16 
 8   Alb           850 non-null    float16 
 9   AG_ratio      850 non-null    float16 
 10  D/T_ex2       850 non-null    float16 
 11  AST/ALT_ex2   850 non-null    float16 
 12  TP/AST_ex2    850 non-null    float16 
 13  Globulin_ex2  850 non-null    float16 
 14  TB/ALT_ex3    850 non-null    float16 
 15  TB/AST_ex3    850 non-null    float16 
 16  TB/ALP_ex3    850 non-null    float16 
 17  Alb/ALT_ex3   850 non-null    float16 
 18  TP/ALT_ex3    850 non-null    float16 
 19  ALP/AST_ex3   850 non-null    float16 
 20  ALP/ALT_ex3   850 non-null    float16 
 21  TB/Alb_ex3    850 non-null    float16 
dtypes: category(1), float16(20), int8(1)
memory usage: 35.1 KB
[0.27742435 0.45382265 0.55682828 0.64615944 0.72397803 0.78474861
 0.83761075 0.88547489]
: (850, 22)
: (850, 30)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=4def0755-257b-45f1-b3bb-686211fd4fa9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">x_train_lgb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span>
    <span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">,</span> <span class="s2">"pc01"</span><span class="p">,</span> <span class="s2">"AST_GOT"</span><span class="p">,</span> <span class="s2">"AG_ratio"</span><span class="p">,</span> <span class="s2">"ALP"</span><span class="p">,</span> <span class="s2">"Alb/ALT_ex3"</span><span class="p">,</span> <span class="s2">"TP"</span><span class="p">,</span> <span class="s2">"D_Bil"</span><span class="p">]</span>
<span class="p">]</span>
<span class="n">x_train_lgb</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 8)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=03833e8b-8fe4-4377-869a-c89cd0d50ccb">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># x_train = data_pre00(x_train)</span>
<span class="n">train_oof_lgbm</span><span class="p">,</span> <span class="n">imp_lgbm</span><span class="p">,</span> <span class="n">metrics_lgbm</span> <span class="o">=</span> <span class="n">train_lgb</span><span class="p">(</span>
    <span class="n">x_train_lgb</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">id_train</span><span class="p">,</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>-------------------- 0 --------------------
20240928 11:38:35
(680, 8) (170, 8)
-------training start-------
[LightGBM] [Info] Number of positive: 303, number of negative: 377
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011534 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 1780
[LightGBM] [Info] Number of data points in the train set: 680, number of used features: 8
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445588 -&gt; initscore=-0.218512
[LightGBM] [Info] Start training from score -0.218512
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Training until validation scores don't improve for 100 rounds
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[100]	training's auc: 0.999974	valid_1's auc: 0.936421
[200]	training's auc: 1	valid_1's auc: 0.935579
Early stopping, best iteration is:
[122]	training's auc: 1	valid_1's auc: 0.938947
[auc] tr:1.0000, va:0.9389
-------------------- 1 --------------------
20240928 11:38:36
(680, 8) (170, 8)
-------training start-------
[LightGBM] [Info] Number of positive: 303, number of negative: 377
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000070 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 1785
[LightGBM] [Info] Number of data points in the train set: 680, number of used features: 8
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445588 -&gt; initscore=-0.218512
[LightGBM] [Info] Start training from score -0.218512
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Training until validation scores don't improve for 100 rounds
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[100]	training's auc: 0.999974	valid_1's auc: 0.970105
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[200]	training's auc: 1	valid_1's auc: 0.974316
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[300]	training's auc: 1	valid_1's auc: 0.972351
Early stopping, best iteration is:
[200]	training's auc: 1	valid_1's auc: 0.974316
[auc] tr:1.0000, va:0.9743
-------------------- 2 --------------------
20240928 11:38:36
(680, 8) (170, 8)
-------training start-------
[LightGBM] [Info] Number of positive: 302, number of negative: 378
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000070 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 1785
[LightGBM] [Info] Number of data points in the train set: 680, number of used features: 8
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444118 -&gt; initscore=-0.224467
[LightGBM] [Info] Start training from score -0.224467
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Training until validation scores don't improve for 100 rounds
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[100]	training's auc: 0.999982	valid_1's auc: 0.925252
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Early stopping, best iteration is:
[56]	training's auc: 0.995261	valid_1's auc: 0.931691
[auc] tr:0.9953, va:0.9317
-------------------- 3 --------------------
20240928 11:38:36
(680, 8) (170, 8)
-------training start-------
[LightGBM] [Info] Number of positive: 302, number of negative: 378
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000070 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 1785
[LightGBM] [Info] Number of data points in the train set: 680, number of used features: 8
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444118 -&gt; initscore=-0.224467
[LightGBM] [Info] Start training from score -0.224467
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Training until validation scores don't improve for 100 rounds
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[100]	training's auc: 0.999947	valid_1's auc: 0.925252
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[200]	training's auc: 1	valid_1's auc: 0.929311
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Early stopping, best iteration is:
[152]	training's auc: 1	valid_1's auc: 0.930991
[auc] tr:1.0000, va:0.9310
-------------------- 4 --------------------
20240928 11:38:36
(680, 8) (170, 8)
-------training start-------
[LightGBM] [Info] Number of positive: 302, number of negative: 378
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000078 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 1791
[LightGBM] [Info] Number of data points in the train set: 680, number of used features: 8
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444118 -&gt; initscore=-0.224467
[LightGBM] [Info] Start training from score -0.224467
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Training until validation scores don't improve for 100 rounds
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[100]	training's auc: 0.999956	valid_1's auc: 0.948348
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[200]	training's auc: 1	valid_1's auc: 0.94037
Early stopping, best iteration is:
[109]	training's auc: 1	valid_1's auc: 0.949188
[auc] tr:1.0000, va:0.9492
-------------------- result --------------------
[[0.         1.         0.93894737]
 [1.         1.         0.97431579]
 [2.         0.99526087 0.93169093]
 [3.         1.         0.93099104]
 [4.         1.         0.94918813]]
[cv] tr:0.9991+-0.0019,         va:0.9450+-0.0019
[oof]0.9420
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9e548e97-e31d-4c27-9950-b94814e8d205">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span>
    <span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">,</span> <span class="s2">"pc01"</span><span class="p">,</span> <span class="s2">"AST_GOT"</span><span class="p">,</span> <span class="s2">"AG_ratio"</span><span class="p">,</span> <span class="s2">"ALP"</span><span class="p">,</span> <span class="s2">"Alb/ALT_ex3"</span><span class="p">,</span> <span class="s2">"TP"</span><span class="p">,</span> <span class="s2">"D_Bil"</span><span class="p">]</span>
<span class="p">]</span>
<span class="n">x_train_tf</span><span class="o">.</span><span class="n">shape</span>

<span class="n">std_tf</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_tf</span><span class="p">)</span>
<span class="n">df_std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std_tf</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>


<span class="n">x_train_tf</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>T_Bil</th>
<th>pc01</th>
<th>AST_GOT</th>
<th>AG_ratio</th>
<th>ALP</th>
<th>Alb/ALT_ex3</th>
<th>TP</th>
<th>D_Bil</th>
</tr>
</thead>
<tbody>
<tr>
<th>count</th>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
</tr>
<tr>
<th>mean</th>
<td>1.6074</td>
<td>0.0000</td>
<td>61.0312</td>
<td>1.1445</td>
<td>inf</td>
<td>0.2074</td>
<td>7.0312</td>
<td>0.6064</td>
</tr>
<tr>
<th>std</th>
<td>2.4434</td>
<td>2.4721</td>
<td>inf</td>
<td>0.2273</td>
<td>inf</td>
<td>0.0987</td>
<td>0.8408</td>
<td>1.5684</td>
</tr>
<tr>
<th>min</th>
<td>0.5859</td>
<td>-4.4823</td>
<td>11.2812</td>
<td>0.6270</td>
<td>163.2500</td>
<td>0.0019</td>
<td>4.8594</td>
<td>0.0349</td>
</tr>
<tr>
<th>25%</th>
<td>0.7814</td>
<td>-1.6060</td>
<td>21.2383</td>
<td>1.0049</td>
<td>214.0000</td>
<td>0.1422</td>
<td>6.7305</td>
<td>0.1449</td>
</tr>
<tr>
<th>50%</th>
<td>0.8357</td>
<td>-0.5219</td>
<td>27.0547</td>
<td>1.2051</td>
<td>220.1250</td>
<td>0.2075</td>
<td>6.9141</td>
<td>0.1941</td>
</tr>
<tr>
<th>75%</th>
<td>1.1970</td>
<td>0.9706</td>
<td>56.4688</td>
<td>1.2881</td>
<td>229.5000</td>
<td>0.2560</td>
<td>7.5352</td>
<td>0.3354</td>
</tr>
<tr>
<th>max</th>
<td>23.0156</td>
<td>14.7900</td>
<td>814.5000</td>
<td>1.8213</td>
<td>2108.0000</td>
<td>0.8887</td>
<td>8.7422</td>
<td>17.6875</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=f1490ca4-464c-4024-83df-e42f3f329f6d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">df_std</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>T_Bil</th>
<th>pc01</th>
<th>AST_GOT</th>
<th>AG_ratio</th>
<th>ALP</th>
<th>Alb/ALT_ex3</th>
<th>TP</th>
<th>D_Bil</th>
</tr>
</thead>
<tbody>
<tr>
<th>count</th>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
</tr>
<tr>
<th>mean</th>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
</tr>
<tr>
<th>std</th>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
</tr>
<tr>
<th>min</th>
<td>-0.4182</td>
<td>-1.8143</td>
<td>-0.4519</td>
<td>-2.2793</td>
<td>-0.5442</td>
<td>-2.0823</td>
<td>-2.5861</td>
<td>-0.3647</td>
</tr>
<tr>
<th>25%</th>
<td>-0.3382</td>
<td>-0.6500</td>
<td>-0.3614</td>
<td>-0.6151</td>
<td>-0.2907</td>
<td>-0.6604</td>
<td>-0.3593</td>
<td>-0.2945</td>
</tr>
<tr>
<th>50%</th>
<td>-0.3160</td>
<td>-0.2113</td>
<td>-0.3086</td>
<td>0.2664</td>
<td>-0.2601</td>
<td>0.0014</td>
<td>-0.1408</td>
<td>-0.2632</td>
</tr>
<tr>
<th>75%</th>
<td>-0.1680</td>
<td>0.3929</td>
<td>-0.0413</td>
<td>0.6319</td>
<td>-0.2133</td>
<td>0.4939</td>
<td>0.5984</td>
<td>-0.1730</td>
</tr>
<tr>
<th>max</th>
<td>8.7662</td>
<td>5.9864</td>
<td>6.8459</td>
<td>2.9799</td>
<td>9.1694</td>
<td>6.9078</td>
<td>2.0350</td>
<td>10.8930</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1267bfc7-cd14-4c82-8b56-7296dd2d15e7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span>
    <span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">,</span> <span class="s2">"pc01"</span><span class="p">,</span> <span class="s2">"AST_GOT"</span><span class="p">,</span> <span class="s2">"AG_ratio"</span><span class="p">,</span> <span class="s2">"ALP"</span><span class="p">,</span> <span class="s2">"Alb/ALT_ex3"</span><span class="p">,</span> <span class="s2">"TP"</span><span class="p">,</span> <span class="s2">"D_Bil"</span><span class="p">]</span>
<span class="p">]</span>


<span class="n">std_tf</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_tf</span><span class="p">)</span>
<span class="n">df_std_tf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std_tf</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=5d4b4a32-c5a9-4ab4-b7f7-89ccd9a2514d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">df_std_tf</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>T_Bil</th>
<th>pc01</th>
<th>AST_GOT</th>
<th>AG_ratio</th>
<th>ALP</th>
<th>Alb/ALT_ex3</th>
<th>TP</th>
<th>D_Bil</th>
</tr>
</thead>
<tbody>
<tr>
<th>count</th>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
</tr>
<tr>
<th>mean</th>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
</tr>
<tr>
<th>std</th>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
</tr>
<tr>
<th>min</th>
<td>-0.4182</td>
<td>-1.8143</td>
<td>-0.4519</td>
<td>-2.2793</td>
<td>-0.5442</td>
<td>-2.0823</td>
<td>-2.5861</td>
<td>-0.3647</td>
</tr>
<tr>
<th>25%</th>
<td>-0.3382</td>
<td>-0.6500</td>
<td>-0.3614</td>
<td>-0.6151</td>
<td>-0.2907</td>
<td>-0.6604</td>
<td>-0.3593</td>
<td>-0.2945</td>
</tr>
<tr>
<th>50%</th>
<td>-0.3160</td>
<td>-0.2113</td>
<td>-0.3086</td>
<td>0.2664</td>
<td>-0.2601</td>
<td>0.0014</td>
<td>-0.1408</td>
<td>-0.2632</td>
</tr>
<tr>
<th>75%</th>
<td>-0.1680</td>
<td>0.3929</td>
<td>-0.0413</td>
<td>0.6319</td>
<td>-0.2133</td>
<td>0.4939</td>
<td>0.5984</td>
<td>-0.1730</td>
</tr>
<tr>
<th>max</th>
<td>8.7662</td>
<td>5.9864</td>
<td>6.8459</td>
<td>2.9799</td>
<td>9.1694</td>
<td>6.9078</td>
<td>2.0350</td>
<td>10.8930</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=3afd0b76-7e36-4351-8ccc-99399faf7fae">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">train_oof_tf</span><span class="p">,</span> <span class="n">metrics_tf</span> <span class="o">=</span> <span class="n">train_tf</span><span class="p">(</span>
    <span class="n">df_std_tf</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">id_train</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)                    </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">       Param # </span>

 input_layer (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)         (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">8</span>)                           <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                    (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">1,152</span> 

 batch_normalization              (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">512</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                  (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">16,512</span> 

 batch_normalization_1            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">512</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)              (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                  (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">16,512</span> 

 batch_normalization_2            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">512</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)              (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                  (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">129</span> 

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">35,841</span> (140.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">35,073</span> (137.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">768</span> (3.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>-------------------- 0 --------------------
20240928 11:54:07
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.6365 - roc_auc: 0.7862
Epoch 1: val_loss improved from inf to 0.55835, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6351 - roc_auc: 0.7871 - val_loss: 0.5584 - val_roc_auc: 0.8573 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.5202 - roc_auc: 0.8587
Epoch 2: val_loss improved from 0.55835 to 0.49836, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.5145 - roc_auc: 0.8585 - val_loss: 0.4984 - val_roc_auc: 0.8707 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4879 - roc_auc: 0.8603
Epoch 3: val_loss improved from 0.49836 to 0.45015, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4819 - roc_auc: 0.8627 - val_loss: 0.4501 - val_roc_auc: 0.8589 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">70/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4386 - roc_auc: 0.8881
Epoch 4: val_loss improved from 0.45015 to 0.44711, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4352 - roc_auc: 0.8886 - val_loss: 0.4471 - val_roc_auc: 0.8646 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4241 - roc_auc: 0.8931
Epoch 5: val_loss did not improve from 0.44711
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4214 - roc_auc: 0.8937 - val_loss: 0.4612 - val_roc_auc: 0.8745 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3912 - roc_auc: 0.9125
Epoch 6: val_loss did not improve from 0.44711
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3920 - roc_auc: 0.9111 - val_loss: 0.4957 - val_roc_auc: 0.8620 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3881 - roc_auc: 0.9121
Epoch 7: val_loss did not improve from 0.44711
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3883 - roc_auc: 0.9111 - val_loss: 0.4872 - val_roc_auc: 0.8661 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3813 - roc_auc: 0.9111
Epoch 8: val_loss did not improve from 0.44711
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3808 - roc_auc: 0.9113 - val_loss: 0.4813 - val_roc_auc: 0.8747 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">70/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3873 - roc_auc: 0.9109
Epoch 9: val_loss did not improve from 0.44711

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3885 - roc_auc: 0.9098 - val_loss: 0.4610 - val_roc_auc: 0.8863 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3733 - roc_auc: 0.9179
Epoch 10: val_loss improved from 0.44711 to 0.44184, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3715 - roc_auc: 0.9182 - val_loss: 0.4418 - val_roc_auc: 0.8867 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3110 - roc_auc: 0.9418
Epoch 11: val_loss improved from 0.44184 to 0.43809, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3140 - roc_auc: 0.9401 - val_loss: 0.4381 - val_roc_auc: 0.8885 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3233 - roc_auc: 0.9345
Epoch 12: val_loss improved from 0.43809 to 0.43717, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3251 - roc_auc: 0.9338 - val_loss: 0.4372 - val_roc_auc: 0.8890 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3856 - roc_auc: 0.9098
Epoch 13: val_loss improved from 0.43717 to 0.43674, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3790 - roc_auc: 0.9123 - val_loss: 0.4367 - val_roc_auc: 0.8859 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3573 - roc_auc: 0.9197
Epoch 14: val_loss improved from 0.43674 to 0.43575, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3518 - roc_auc: 0.9222 - val_loss: 0.4358 - val_roc_auc: 0.8888 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3605 - roc_auc: 0.9208
Epoch 15: val_loss did not improve from 0.43575
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3551 - roc_auc: 0.9228 - val_loss: 0.4369 - val_roc_auc: 0.8876 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3510 - roc_auc: 0.9248
Epoch 16: val_loss did not improve from 0.43575
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3498 - roc_auc: 0.9251 - val_loss: 0.4441 - val_roc_auc: 0.8831 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3379 - roc_auc: 0.9304
Epoch 17: val_loss did not improve from 0.43575
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3335 - roc_auc: 0.9317 - val_loss: 0.4437 - val_roc_auc: 0.8851 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3322 - roc_auc: 0.9332
Epoch 18: val_loss did not improve from 0.43575
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3310 - roc_auc: 0.9334 - val_loss: 0.4444 - val_roc_auc: 0.8844 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3304 - roc_auc: 0.9306
Epoch 19: val_loss did not improve from 0.43575

Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3298 - roc_auc: 0.9311 - val_loss: 0.4458 - val_roc_auc: 0.8834 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3534 - roc_auc: 0.9228
Epoch 20: val_loss did not improve from 0.43575
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3504 - roc_auc: 0.9239 - val_loss: 0.4464 - val_roc_auc: 0.8836 - learning_rate: 1.0000e-05
Epoch 21/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3139 - roc_auc: 0.9424
Epoch 21: val_loss did not improve from 0.43575
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3130 - roc_auc: 0.9420 - val_loss: 0.4499 - val_roc_auc: 0.8827 - learning_rate: 1.0000e-05
Epoch 22/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3343 - roc_auc: 0.9326
Epoch 22: val_loss did not improve from 0.43575
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3330 - roc_auc: 0.9325 - val_loss: 0.4492 - val_roc_auc: 0.8818 - learning_rate: 1.0000e-05
Epoch 23/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3120 - roc_auc: 0.9392
Epoch 23: val_loss did not improve from 0.43575
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3106 - roc_auc: 0.9397 - val_loss: 0.4486 - val_roc_auc: 0.8815 - learning_rate: 1.0000e-05
Epoch 24/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3250 - roc_auc: 0.9351
Epoch 24: val_loss did not improve from 0.43575

Epoch 24: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3236 - roc_auc: 0.9353 - val_loss: 0.4478 - val_roc_auc: 0.8825 - learning_rate: 1.0000e-05
Epoch 24: early stopping
Restoring model weights from the end of the best epoch: 14.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9317, va:0.8891
-------------------- 1 --------------------
20240928 11:54:18
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.5965 - roc_auc: 0.7943
Epoch 1: val_loss improved from inf to 0.59733, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.5911 - roc_auc: 0.7995 - val_loss: 0.5973 - val_roc_auc: 0.8815 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4815 - roc_auc: 0.8518
Epoch 2: val_loss improved from 0.59733 to 0.50026, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4813 - roc_auc: 0.8521 - val_loss: 0.5003 - val_roc_auc: 0.9084 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4565 - roc_auc: 0.8699
Epoch 3: val_loss improved from 0.50026 to 0.46231, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4547 - roc_auc: 0.8719 - val_loss: 0.4623 - val_roc_auc: 0.8863 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4248 - roc_auc: 0.8858
Epoch 4: val_loss improved from 0.46231 to 0.37640, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4216 - roc_auc: 0.8875 - val_loss: 0.3764 - val_roc_auc: 0.9285 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3949 - roc_auc: 0.9054
Epoch 5: val_loss improved from 0.37640 to 0.36741, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3952 - roc_auc: 0.9052 - val_loss: 0.3674 - val_roc_auc: 0.9264 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3937 - roc_auc: 0.9031
Epoch 6: val_loss improved from 0.36741 to 0.34623, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3939 - roc_auc: 0.9030 - val_loss: 0.3462 - val_roc_auc: 0.9298 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3752 - roc_auc: 0.9129
Epoch 7: val_loss improved from 0.34623 to 0.33932, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3821 - roc_auc: 0.9097 - val_loss: 0.3393 - val_roc_auc: 0.9306 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3689 - roc_auc: 0.9157
Epoch 8: val_loss did not improve from 0.33932
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3687 - roc_auc: 0.9156 - val_loss: 0.3477 - val_roc_auc: 0.9282 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3767 - roc_auc: 0.9124
Epoch 9: val_loss did not improve from 0.33932
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3765 - roc_auc: 0.9125 - val_loss: 0.3464 - val_roc_auc: 0.9262 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3694 - roc_auc: 0.9137
Epoch 10: val_loss improved from 0.33932 to 0.33623, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3708 - roc_auc: 0.9133 - val_loss: 0.3362 - val_roc_auc: 0.9351 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3490 - roc_auc: 0.9234
Epoch 11: val_loss improved from 0.33623 to 0.33219, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3492 - roc_auc: 0.9231 - val_loss: 0.3322 - val_roc_auc: 0.9346 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3479 - roc_auc: 0.9223
Epoch 12: val_loss improved from 0.33219 to 0.32981, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3470 - roc_auc: 0.9230 - val_loss: 0.3298 - val_roc_auc: 0.9349 - learning_rate: 0.0010
Epoch 13/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3576 - roc_auc: 0.9174
Epoch 13: val_loss improved from 0.32981 to 0.31385, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3575 - roc_auc: 0.9175 - val_loss: 0.3138 - val_roc_auc: 0.9408 - learning_rate: 0.0010
Epoch 14/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3032 - roc_auc: 0.9433
Epoch 14: val_loss did not improve from 0.31385
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3040 - roc_auc: 0.9430 - val_loss: 0.3208 - val_roc_auc: 0.9434 - learning_rate: 0.0010
Epoch 15/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3289 - roc_auc: 0.9329
Epoch 15: val_loss improved from 0.31385 to 0.30514, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3287 - roc_auc: 0.9329 - val_loss: 0.3051 - val_roc_auc: 0.9506 - learning_rate: 0.0010
Epoch 16/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2973 - roc_auc: 0.9495
Epoch 16: val_loss did not improve from 0.30514
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2980 - roc_auc: 0.9487 - val_loss: 0.3135 - val_roc_auc: 0.9439 - learning_rate: 0.0010
Epoch 17/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3117 - roc_auc: 0.9383
Epoch 17: val_loss improved from 0.30514 to 0.29852, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3167 - roc_auc: 0.9363 - val_loss: 0.2985 - val_roc_auc: 0.9488 - learning_rate: 0.0010
Epoch 18/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3471 - roc_auc: 0.9234
Epoch 18: val_loss did not improve from 0.29852
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3471 - roc_auc: 0.9234 - val_loss: 0.3238 - val_roc_auc: 0.9421 - learning_rate: 0.0010
Epoch 19/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3383 - roc_auc: 0.9303
Epoch 19: val_loss improved from 0.29852 to 0.28910, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3321 - roc_auc: 0.9324 - val_loss: 0.2891 - val_roc_auc: 0.9512 - learning_rate: 0.0010
Epoch 20/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3258 - roc_auc: 0.9345
Epoch 20: val_loss improved from 0.28910 to 0.26809, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3254 - roc_auc: 0.9347 - val_loss: 0.2681 - val_roc_auc: 0.9595 - learning_rate: 0.0010
Epoch 21/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3100 - roc_auc: 0.9413
Epoch 21: val_loss did not improve from 0.26809
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3101 - roc_auc: 0.9411 - val_loss: 0.2748 - val_roc_auc: 0.9578 - learning_rate: 0.0010
Epoch 22/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2958 - roc_auc: 0.9447
Epoch 22: val_loss did not improve from 0.26809
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2960 - roc_auc: 0.9446 - val_loss: 0.2903 - val_roc_auc: 0.9524 - learning_rate: 0.0010
Epoch 23/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3145 - roc_auc: 0.9376
Epoch 23: val_loss improved from 0.26809 to 0.26672, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3080 - roc_auc: 0.9402 - val_loss: 0.2667 - val_roc_auc: 0.9601 - learning_rate: 0.0010
Epoch 24/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3070 - roc_auc: 0.9449
Epoch 24: val_loss did not improve from 0.26672
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3038 - roc_auc: 0.9457 - val_loss: 0.2968 - val_roc_auc: 0.9476 - learning_rate: 0.0010
Epoch 25/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2981 - roc_auc: 0.9468
Epoch 25: val_loss did not improve from 0.26672
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2926 - roc_auc: 0.9484 - val_loss: 0.3304 - val_roc_auc: 0.9278 - learning_rate: 0.0010
Epoch 26/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2860 - roc_auc: 0.9491
Epoch 26: val_loss did not improve from 0.26672
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2858 - roc_auc: 0.9492 - val_loss: 0.3436 - val_roc_auc: 0.9215 - learning_rate: 0.0010
Epoch 27/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2871 - roc_auc: 0.9502
Epoch 27: val_loss did not improve from 0.26672
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2824 - roc_auc: 0.9515 - val_loss: 0.3100 - val_roc_auc: 0.9407 - learning_rate: 0.0010
Epoch 28/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3190 - roc_auc: 0.9364
Epoch 28: val_loss did not improve from 0.26672

Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3174 - roc_auc: 0.9370 - val_loss: 0.3026 - val_roc_auc: 0.9398 - learning_rate: 0.0010
Epoch 29/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2502 - roc_auc: 0.9606
Epoch 29: val_loss did not improve from 0.26672
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2502 - roc_auc: 0.9606 - val_loss: 0.2911 - val_roc_auc: 0.9435 - learning_rate: 1.0000e-04
Epoch 30/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2638 - roc_auc: 0.9541
Epoch 30: val_loss did not improve from 0.26672
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2576 - roc_auc: 0.9568 - val_loss: 0.2828 - val_roc_auc: 0.9516 - learning_rate: 1.0000e-04
Epoch 31/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2636 - roc_auc: 0.9581
Epoch 31: val_loss did not improve from 0.26672
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2519 - roc_auc: 0.9616 - val_loss: 0.2794 - val_roc_auc: 0.9528 - learning_rate: 1.0000e-04
Epoch 32/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2437 - roc_auc: 0.9654
Epoch 32: val_loss did not improve from 0.26672
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2405 - roc_auc: 0.9663 - val_loss: 0.2807 - val_roc_auc: 0.9526 - learning_rate: 1.0000e-04
Epoch 33/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2495 - roc_auc: 0.9641
Epoch 33: val_loss did not improve from 0.26672

Epoch 33: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2419 - roc_auc: 0.9661 - val_loss: 0.2773 - val_roc_auc: 0.9529 - learning_rate: 1.0000e-04
Epoch 33: early stopping
Restoring model weights from the end of the best epoch: 23.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9254, va:0.9600
-------------------- 2 --------------------
20240928 11:54:31
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6914 - roc_auc: 0.7501
Epoch 1: val_loss improved from inf to 0.60126, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6886 - roc_auc: 0.7517 - val_loss: 0.6013 - val_roc_auc: 0.8474 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.5366 - roc_auc: 0.8383
Epoch 2: val_loss improved from 0.60126 to 0.53938, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.5312 - roc_auc: 0.8403 - val_loss: 0.5394 - val_roc_auc: 0.8691 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4505 - roc_auc: 0.8815
Epoch 3: val_loss improved from 0.53938 to 0.46556, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4518 - roc_auc: 0.8791 - val_loss: 0.4656 - val_roc_auc: 0.8845 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4674 - roc_auc: 0.8728
Epoch 4: val_loss improved from 0.46556 to 0.42338, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4641 - roc_auc: 0.8736 - val_loss: 0.4234 - val_roc_auc: 0.8957 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4509 - roc_auc: 0.8755
Epoch 5: val_loss improved from 0.42338 to 0.39537, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4496 - roc_auc: 0.8762 - val_loss: 0.3954 - val_roc_auc: 0.9024 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4510 - roc_auc: 0.8707
Epoch 6: val_loss improved from 0.39537 to 0.39437, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4503 - roc_auc: 0.8710 - val_loss: 0.3944 - val_roc_auc: 0.8996 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3939 - roc_auc: 0.9027
Epoch 7: val_loss did not improve from 0.39437
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3941 - roc_auc: 0.9026 - val_loss: 0.4097 - val_roc_auc: 0.8931 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3932 - roc_auc: 0.9035
Epoch 8: val_loss did not improve from 0.39437
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3944 - roc_auc: 0.9030 - val_loss: 0.4018 - val_roc_auc: 0.8947 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4065 - roc_auc: 0.9007
Epoch 9: val_loss did not improve from 0.39437
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4055 - roc_auc: 0.9011 - val_loss: 0.3954 - val_roc_auc: 0.9017 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3852 - roc_auc: 0.9073
Epoch 10: val_loss improved from 0.39437 to 0.39269, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3829 - roc_auc: 0.9082 - val_loss: 0.3927 - val_roc_auc: 0.9051 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3825 - roc_auc: 0.9125
Epoch 11: val_loss did not improve from 0.39269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3823 - roc_auc: 0.9125 - val_loss: 0.4184 - val_roc_auc: 0.8910 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4088 - roc_auc: 0.9022
Epoch 12: val_loss did not improve from 0.39269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4023 - roc_auc: 0.9045 - val_loss: 0.3960 - val_roc_auc: 0.9024 - learning_rate: 0.0010
Epoch 13/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3615 - roc_auc: 0.9178
Epoch 13: val_loss did not improve from 0.39269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3618 - roc_auc: 0.9178 - val_loss: 0.3969 - val_roc_auc: 0.9066 - learning_rate: 0.0010
Epoch 14/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3609 - roc_auc: 0.9186
Epoch 14: val_loss did not improve from 0.39269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3608 - roc_auc: 0.9186 - val_loss: 0.4089 - val_roc_auc: 0.8948 - learning_rate: 0.0010
Epoch 15/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3876 - roc_auc: 0.9116
Epoch 15: val_loss did not improve from 0.39269

Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3799 - roc_auc: 0.9143 - val_loss: 0.4210 - val_roc_auc: 0.8945 - learning_rate: 0.0010
Epoch 16/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3536 - roc_auc: 0.9274
Epoch 16: val_loss did not improve from 0.39269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3469 - roc_auc: 0.9293 - val_loss: 0.4131 - val_roc_auc: 0.8942 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3359 - roc_auc: 0.9317
Epoch 17: val_loss did not improve from 0.39269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3351 - roc_auc: 0.9315 - val_loss: 0.4022 - val_roc_auc: 0.9019 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3325 - roc_auc: 0.9282
Epoch 18: val_loss did not improve from 0.39269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3315 - roc_auc: 0.9292 - val_loss: 0.4024 - val_roc_auc: 0.9021 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3238 - roc_auc: 0.9365
Epoch 19: val_loss did not improve from 0.39269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3237 - roc_auc: 0.9366 - val_loss: 0.4054 - val_roc_auc: 0.9018 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2939 - roc_auc: 0.9483
Epoch 20: val_loss did not improve from 0.39269

Epoch 20: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2939 - roc_auc: 0.9483 - val_loss: 0.4058 - val_roc_auc: 0.9018 - learning_rate: 1.0000e-04
Epoch 20: early stopping
Restoring model weights from the end of the best epoch: 10.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9332, va:0.9055
-------------------- 3 --------------------
20240928 11:54:40
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6024 - roc_auc: 0.8060
Epoch 1: val_loss improved from inf to 0.58685, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6020 - roc_auc: 0.8062 - val_loss: 0.5868 - val_roc_auc: 0.8575 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4855 - roc_auc: 0.8656
Epoch 2: val_loss improved from 0.58685 to 0.51654, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4930 - roc_auc: 0.8610 - val_loss: 0.5165 - val_roc_auc: 0.8799 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4133 - roc_auc: 0.8948
Epoch 3: val_loss improved from 0.51654 to 0.45458, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4176 - roc_auc: 0.8924 - val_loss: 0.4546 - val_roc_auc: 0.8870 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3889 - roc_auc: 0.9058
Epoch 4: val_loss improved from 0.45458 to 0.43814, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3896 - roc_auc: 0.9055 - val_loss: 0.4381 - val_roc_auc: 0.8970 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3784 - roc_auc: 0.9080
Epoch 5: val_loss improved from 0.43814 to 0.43513, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3904 - roc_auc: 0.9027 - val_loss: 0.4351 - val_roc_auc: 0.8869 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3825 - roc_auc: 0.9058
Epoch 6: val_loss improved from 0.43513 to 0.41436, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3832 - roc_auc: 0.9055 - val_loss: 0.4144 - val_roc_auc: 0.8966 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3749 - roc_auc: 0.9090
Epoch 7: val_loss did not improve from 0.41436
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3778 - roc_auc: 0.9076 - val_loss: 0.4432 - val_roc_auc: 0.8893 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3496 - roc_auc: 0.9224
Epoch 8: val_loss did not improve from 0.41436
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3500 - roc_auc: 0.9223 - val_loss: 0.4636 - val_roc_auc: 0.8773 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3694 - roc_auc: 0.9135
Epoch 9: val_loss did not improve from 0.41436
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3730 - roc_auc: 0.9119 - val_loss: 0.4523 - val_roc_auc: 0.8873 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3274 - roc_auc: 0.9331
Epoch 10: val_loss did not improve from 0.41436
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3284 - roc_auc: 0.9327 - val_loss: 0.4344 - val_roc_auc: 0.8950 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3118 - roc_auc: 0.9389
Epoch 11: val_loss did not improve from 0.41436

Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3240 - roc_auc: 0.9339 - val_loss: 0.4407 - val_roc_auc: 0.8946 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3284 - roc_auc: 0.9304
Epoch 12: val_loss did not improve from 0.41436
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3287 - roc_auc: 0.9303 - val_loss: 0.4322 - val_roc_auc: 0.8964 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2737 - roc_auc: 0.9545
Epoch 13: val_loss did not improve from 0.41436
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2875 - roc_auc: 0.9492 - val_loss: 0.4249 - val_roc_auc: 0.8992 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3225 - roc_auc: 0.9337
Epoch 14: val_loss did not improve from 0.41436
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3234 - roc_auc: 0.9334 - val_loss: 0.4303 - val_roc_auc: 0.8984 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3065 - roc_auc: 0.9417
Epoch 15: val_loss did not improve from 0.41436
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3096 - roc_auc: 0.9404 - val_loss: 0.4327 - val_roc_auc: 0.8950 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2894 - roc_auc: 0.9486
Epoch 16: val_loss did not improve from 0.41436

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2904 - roc_auc: 0.9482 - val_loss: 0.4299 - val_roc_auc: 0.8953 - learning_rate: 1.0000e-04
Epoch 16: early stopping
Restoring model weights from the end of the best epoch: 6.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9319, va:0.8968
-------------------- 4 --------------------
20240928 11:54:48
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6913 - roc_auc: 0.7417
Epoch 1: val_loss improved from inf to 0.56132, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6805 - roc_auc: 0.7502 - val_loss: 0.5613 - val_roc_auc: 0.9048 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4937 - roc_auc: 0.8572
Epoch 2: val_loss improved from 0.56132 to 0.48225, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4919 - roc_auc: 0.8578 - val_loss: 0.4823 - val_roc_auc: 0.9143 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4346 - roc_auc: 0.8772
Epoch 3: val_loss improved from 0.48225 to 0.46005, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4364 - roc_auc: 0.8762 - val_loss: 0.4601 - val_roc_auc: 0.9097 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3989 - roc_auc: 0.8983
Epoch 4: val_loss improved from 0.46005 to 0.40915, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4028 - roc_auc: 0.8963 - val_loss: 0.4091 - val_roc_auc: 0.9069 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4010 - roc_auc: 0.8982
Epoch 5: val_loss improved from 0.40915 to 0.40080, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4019 - roc_auc: 0.8977 - val_loss: 0.4008 - val_roc_auc: 0.9041 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3953 - roc_auc: 0.8995
Epoch 6: val_loss improved from 0.40080 to 0.39373, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3959 - roc_auc: 0.8992 - val_loss: 0.3937 - val_roc_auc: 0.9048 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4044 - roc_auc: 0.8934
Epoch 7: val_loss did not improve from 0.39373
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4047 - roc_auc: 0.8934 - val_loss: 0.4085 - val_roc_auc: 0.9049 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3948 - roc_auc: 0.9046
Epoch 8: val_loss improved from 0.39373 to 0.39238, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3936 - roc_auc: 0.9047 - val_loss: 0.3924 - val_roc_auc: 0.9103 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3535 - roc_auc: 0.9173
Epoch 9: val_loss did not improve from 0.39238
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3539 - roc_auc: 0.9171 - val_loss: 0.4046 - val_roc_auc: 0.9028 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3521 - roc_auc: 0.9215
Epoch 10: val_loss did not improve from 0.39238
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3534 - roc_auc: 0.9209 - val_loss: 0.4070 - val_roc_auc: 0.9021 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3616 - roc_auc: 0.9131
Epoch 11: val_loss did not improve from 0.39238
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3631 - roc_auc: 0.9126 - val_loss: 0.4023 - val_roc_auc: 0.9038 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3235 - roc_auc: 0.9311
Epoch 12: val_loss did not improve from 0.39238
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3243 - roc_auc: 0.9308 - val_loss: 0.4035 - val_roc_auc: 0.9063 - learning_rate: 0.0010
Epoch 13/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3449 - roc_auc: 0.9278
Epoch 13: val_loss did not improve from 0.39238

Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3449 - roc_auc: 0.9276 - val_loss: 0.4217 - val_roc_auc: 0.8985 - learning_rate: 0.0010
Epoch 14/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3427 - roc_auc: 0.9253
Epoch 14: val_loss did not improve from 0.39238
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3419 - roc_auc: 0.9257 - val_loss: 0.4052 - val_roc_auc: 0.9021 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3180 - roc_auc: 0.9375
Epoch 15: val_loss did not improve from 0.39238
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3185 - roc_auc: 0.9371 - val_loss: 0.3996 - val_roc_auc: 0.9058 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3091 - roc_auc: 0.9399
Epoch 16: val_loss did not improve from 0.39238
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3095 - roc_auc: 0.9397 - val_loss: 0.3981 - val_roc_auc: 0.9045 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2937 - roc_auc: 0.9453
Epoch 17: val_loss did not improve from 0.39238
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2944 - roc_auc: 0.9451 - val_loss: 0.3959 - val_roc_auc: 0.9043 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3151 - roc_auc: 0.9401
Epoch 18: val_loss did not improve from 0.39238

Epoch 18: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3149 - roc_auc: 0.9401 - val_loss: 0.3969 - val_roc_auc: 0.9056 - learning_rate: 1.0000e-04
Epoch 18: early stopping
Restoring model weights from the end of the best epoch: 8.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step 
[auc] tr:0.9277, va:0.9101
---------- result ----------
[[0.         0.93174357 0.88912281]
 [1.         0.92539678 0.96      ]
 [2.         0.93318792 0.90551512]
 [3.         0.93187393 0.89683651]
 [4.         0.92768667 0.91013438]]
[cv] tr:0.9300+-0.0029,         va:0.9123+-0.0029
[oof]0.9098
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=dd0c3375-0c76-4298-88d4-edb15db900e5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span>
<span class="c1">#[    ["T_Bil", "pc01", "AST_GOT", "AG_ratio", "ALP", "Alb/ALT_ex3", "TP", "D_Bil"]]</span>
<span class="n">x_train_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 30)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=b08e9b71-cab3-4118-9be6-865152424361">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">std_tf</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_tf</span><span class="p">)</span>
<span class="n">df_std_tf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std_tf</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1a7a1e24-cbfa-4ed0-ae74-eeaff8578a43">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">df_std_tf</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Age</th>
<th>Gender</th>
<th>T_Bil</th>
<th>D_Bil</th>
<th>ALP</th>
<th>ALT_GPT</th>
<th>AST_GOT</th>
<th>TP</th>
<th>Alb</th>
<th>AG_ratio</th>
<th>...</th>
<th>ALP/ALT_ex3</th>
<th>TB/Alb_ex3</th>
<th>pc01</th>
<th>pc02</th>
<th>pc03</th>
<th>pc04</th>
<th>pc05</th>
<th>pc06</th>
<th>pc07</th>
<th>pc08</th>
</tr>
</thead>
<tbody>
<tr>
<th>count</th>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>...</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
</tr>
<tr>
<th>mean</th>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>...</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
</tr>
<tr>
<th>std</th>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>...</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
</tr>
<tr>
<th>min</th>
<td>-2.2264</td>
<td>-2.1779</td>
<td>-0.4182</td>
<td>-0.3647</td>
<td>-0.5442</td>
<td>-0.2609</td>
<td>-0.4519</td>
<td>-2.5861</td>
<td>-2.3869</td>
<td>-2.2793</td>
<td>...</td>
<td>-1.3395</td>
<td>-0.4513</td>
<td>-1.8143</td>
<td>-2.9570</td>
<td>-4.5507</td>
<td>-3.5893</td>
<td>-4.0526</td>
<td>-2.5738</td>
<td>-2.5645</td>
<td>-5.5798</td>
</tr>
<tr>
<th>25%</th>
<td>-0.8885</td>
<td>0.4592</td>
<td>-0.3382</td>
<td>-0.2945</td>
<td>-0.2907</td>
<td>-0.1943</td>
<td>-0.3614</td>
<td>-0.3593</td>
<td>-0.6768</td>
<td>-0.6151</td>
<td>...</td>
<td>-0.4513</td>
<td>-0.3485</td>
<td>-0.6500</td>
<td>-0.5127</td>
<td>-0.5759</td>
<td>-0.5831</td>
<td>-0.4516</td>
<td>-0.6939</td>
<td>-0.4110</td>
<td>-0.6809</td>
</tr>
<tr>
<th>50%</th>
<td>0.0845</td>
<td>0.4592</td>
<td>-0.3160</td>
<td>-0.2632</td>
<td>-0.2601</td>
<td>-0.1737</td>
<td>-0.3086</td>
<td>-0.1408</td>
<td>0.1852</td>
<td>0.2664</td>
<td>...</td>
<td>-0.1298</td>
<td>-0.3098</td>
<td>-0.2113</td>
<td>-0.0081</td>
<td>-0.1470</td>
<td>-0.1222</td>
<td>-0.0978</td>
<td>0.0359</td>
<td>-0.0384</td>
<td>0.0674</td>
</tr>
<tr>
<th>75%</th>
<td>0.9359</td>
<td>0.4592</td>
<td>-0.1680</td>
<td>-0.1730</td>
<td>-0.2133</td>
<td>-0.1297</td>
<td>-0.0413</td>
<td>0.5984</td>
<td>0.3492</td>
<td>0.6319</td>
<td>...</td>
<td>0.2086</td>
<td>-0.1508</td>
<td>0.3929</td>
<td>0.3605</td>
<td>0.4822</td>
<td>0.5642</td>
<td>0.4400</td>
<td>0.4648</td>
<td>0.4170</td>
<td>0.8348</td>
</tr>
<tr>
<th>max</th>
<td>1.9089</td>
<td>0.4592</td>
<td>8.7662</td>
<td>10.8930</td>
<td>9.1694</td>
<td>9.4687</td>
<td>6.8459</td>
<td>2.0350</td>
<td>2.6770</td>
<td>2.9799</td>
<td>...</td>
<td>9.3804</td>
<td>8.7931</td>
<td>5.9864</td>
<td>9.8850</td>
<td>6.7522</td>
<td>3.7849</td>
<td>5.9316</td>
<td>4.3850</td>
<td>11.2114</td>
<td>1.8648</td>
</tr>
</tbody>
</table>
<p>8 rows  30 columns</p>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=ccd7077c-1761-4e8d-862c-8db62ed0e33d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">train_oof_tf</span><span class="p">,</span> <span class="n">metrics_tf</span> <span class="o">=</span> <span class="n">train_tf</span><span class="p">(</span>
    <span class="n">df_std_tf</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">id_train</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_8"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)                    </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">       Param # </span>

 input_layer_8 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)       (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">30</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_32 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">15,872</span> 

 batch_normalization_24           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">2,048</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_24 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_33 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                   <span style="color: #00af00; text-decoration-color: #00af00">131,328</span> 

 batch_normalization_25           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">1,024</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_25 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_34 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">32,896</span> 

 batch_normalization_26           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">512</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_26 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_35 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">129</span> 

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">183,809</span> (718.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">182,017</span> (711.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,792</span> (7.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>-------------------- 0 --------------------
20240928 12:13:14
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.6569 - roc_auc: 0.7625
Epoch 1: val_loss improved from inf to 0.51656, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - loss: 0.6491 - roc_auc: 0.7691 - val_loss: 0.5166 - val_roc_auc: 0.8267 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4184 - roc_auc: 0.8955
Epoch 2: val_loss improved from 0.51656 to 0.48955, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 9ms/step - loss: 0.4159 - roc_auc: 0.8966 - val_loss: 0.4896 - val_roc_auc: 0.8260 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3453 - roc_auc: 0.9286
Epoch 3: val_loss improved from 0.48955 to 0.47736, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3446 - roc_auc: 0.9286 - val_loss: 0.4774 - val_roc_auc: 0.8479 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3005 - roc_auc: 0.9456
Epoch 4: val_loss did not improve from 0.47736
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3002 - roc_auc: 0.9457 - val_loss: 0.4974 - val_roc_auc: 0.8430 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2549 - roc_auc: 0.9616
Epoch 5: val_loss did not improve from 0.47736
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2550 - roc_auc: 0.9614 - val_loss: 0.5302 - val_roc_auc: 0.8590 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2535 - roc_auc: 0.9626
Epoch 6: val_loss did not improve from 0.47736
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2528 - roc_auc: 0.9625 - val_loss: 0.5756 - val_roc_auc: 0.8607 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1936 - roc_auc: 0.9770
Epoch 7: val_loss did not improve from 0.47736
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1938 - roc_auc: 0.9773 - val_loss: 0.5964 - val_roc_auc: 0.8577 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1776 - roc_auc: 0.9818
Epoch 8: val_loss did not improve from 0.47736

Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1786 - roc_auc: 0.9815 - val_loss: 0.6425 - val_roc_auc: 0.8476 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1807 - roc_auc: 0.9809
Epoch 9: val_loss improved from 0.47736 to 0.46995, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1794 - roc_auc: 0.9811 - val_loss: 0.4700 - val_roc_auc: 0.8947 - learning_rate: 1.0000e-04
Epoch 10/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1380 - roc_auc: 0.9920
Epoch 10: val_loss improved from 0.46995 to 0.42807, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1386 - roc_auc: 0.9917 - val_loss: 0.4281 - val_roc_auc: 0.9071 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1536 - roc_auc: 0.9897
Epoch 11: val_loss did not improve from 0.42807
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1500 - roc_auc: 0.9900 - val_loss: 0.4330 - val_roc_auc: 0.9048 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1293 - roc_auc: 0.9940
Epoch 12: val_loss improved from 0.42807 to 0.42250, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1278 - roc_auc: 0.9940 - val_loss: 0.4225 - val_roc_auc: 0.9094 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1215 - roc_auc: 0.9938
Epoch 13: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1201 - roc_auc: 0.9939 - val_loss: 0.4278 - val_roc_auc: 0.9088 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1125 - roc_auc: 0.9964
Epoch 14: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1112 - roc_auc: 0.9964 - val_loss: 0.4279 - val_roc_auc: 0.9079 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1021 - roc_auc: 0.9973
Epoch 15: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1016 - roc_auc: 0.9973 - val_loss: 0.4353 - val_roc_auc: 0.9074 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0973 - roc_auc: 0.9979
Epoch 16: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0970 - roc_auc: 0.9979 - val_loss: 0.4354 - val_roc_auc: 0.9081 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1090 - roc_auc: 0.9962
Epoch 17: val_loss did not improve from 0.42250

Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1078 - roc_auc: 0.9963 - val_loss: 0.4338 - val_roc_auc: 0.9058 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0955 - roc_auc: 0.9974
Epoch 18: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0945 - roc_auc: 0.9975 - val_loss: 0.4378 - val_roc_auc: 0.9057 - learning_rate: 1.0000e-05
Epoch 19/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0807 - roc_auc: 0.9988
Epoch 19: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0805 - roc_auc: 0.9988 - val_loss: 0.4428 - val_roc_auc: 0.9055 - learning_rate: 1.0000e-05
Epoch 20/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0832 - roc_auc: 0.9986
Epoch 20: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0833 - roc_auc: 0.9985 - val_loss: 0.4456 - val_roc_auc: 0.9042 - learning_rate: 1.0000e-05
Epoch 21/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0934 - roc_auc: 0.9975
Epoch 21: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0927 - roc_auc: 0.9975 - val_loss: 0.4438 - val_roc_auc: 0.9048 - learning_rate: 1.0000e-05
Epoch 22/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0973 - roc_auc: 0.9973
Epoch 22: val_loss did not improve from 0.42250

Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0961 - roc_auc: 0.9973 - val_loss: 0.4427 - val_roc_auc: 0.9053 - learning_rate: 1.0000e-05
Epoch 22: early stopping
Restoring model weights from the end of the best epoch: 12.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9628, va:0.9086
-------------------- 1 --------------------
20240928 12:13:27
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6302 - roc_auc: 0.7872
Epoch 1: val_loss improved from inf to 0.51631, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6202 - roc_auc: 0.7949 - val_loss: 0.5163 - val_roc_auc: 0.8968 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4506 - roc_auc: 0.8740
Epoch 2: val_loss improved from 0.51631 to 0.45308, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4486 - roc_auc: 0.8760 - val_loss: 0.4531 - val_roc_auc: 0.9158 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3308 - roc_auc: 0.9339
Epoch 3: val_loss improved from 0.45308 to 0.39576, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3312 - roc_auc: 0.9336 - val_loss: 0.3958 - val_roc_auc: 0.9179 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3073 - roc_auc: 0.9430
Epoch 4: val_loss improved from 0.39576 to 0.36833, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.3074 - roc_auc: 0.9429 - val_loss: 0.3683 - val_roc_auc: 0.9249 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2523 - roc_auc: 0.9657
Epoch 5: val_loss did not improve from 0.36833
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2522 - roc_auc: 0.9656 - val_loss: 0.3744 - val_roc_auc: 0.9142 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2273 - roc_auc: 0.9726
Epoch 6: val_loss improved from 0.36833 to 0.35120, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.2281 - roc_auc: 0.9719 - val_loss: 0.3512 - val_roc_auc: 0.9279 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2134 - roc_auc: 0.9758
Epoch 7: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2131 - roc_auc: 0.9756 - val_loss: 0.4577 - val_roc_auc: 0.8938 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1898 - roc_auc: 0.9799
Epoch 8: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1902 - roc_auc: 0.9797 - val_loss: 0.3968 - val_roc_auc: 0.9196 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1841 - roc_auc: 0.9810
Epoch 9: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1831 - roc_auc: 0.9811 - val_loss: 0.5642 - val_roc_auc: 0.8784 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1553 - roc_auc: 0.9877
Epoch 10: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1545 - roc_auc: 0.9878 - val_loss: 0.4862 - val_roc_auc: 0.8998 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1612 - roc_auc: 0.9860
Epoch 11: val_loss did not improve from 0.35120

Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1595 - roc_auc: 0.9862 - val_loss: 0.5397 - val_roc_auc: 0.8907 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1253 - roc_auc: 0.9911
Epoch 12: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1252 - roc_auc: 0.9912 - val_loss: 0.4741 - val_roc_auc: 0.9048 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1208 - roc_auc: 0.9917
Epoch 13: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1181 - roc_auc: 0.9922 - val_loss: 0.4336 - val_roc_auc: 0.9188 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1027 - roc_auc: 0.9954
Epoch 14: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1025 - roc_auc: 0.9954 - val_loss: 0.4445 - val_roc_auc: 0.9173 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1023 - roc_auc: 0.9946
Epoch 15: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1009 - roc_auc: 0.9948 - val_loss: 0.4218 - val_roc_auc: 0.9212 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0915 - roc_auc: 0.9962
Epoch 16: val_loss did not improve from 0.35120

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0887 - roc_auc: 0.9965 - val_loss: 0.4214 - val_roc_auc: 0.9227 - learning_rate: 1.0000e-04
Epoch 16: early stopping
Restoring model weights from the end of the best epoch: 6.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9587, va:0.9291
-------------------- 2 --------------------
20240928 12:13:37
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.7125 - roc_auc: 0.7504
Epoch 1: val_loss improved from inf to 0.49653, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - loss: 0.7029 - roc_auc: 0.7552 - val_loss: 0.4965 - val_roc_auc: 0.9196 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4434 - roc_auc: 0.8823
Epoch 2: val_loss improved from 0.49653 to 0.45180, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.4423 - roc_auc: 0.8827 - val_loss: 0.4518 - val_roc_auc: 0.9094 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3893 - roc_auc: 0.9050
Epoch 3: val_loss improved from 0.45180 to 0.42462, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3890 - roc_auc: 0.9051 - val_loss: 0.4246 - val_roc_auc: 0.9062 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3222 - roc_auc: 0.9360
Epoch 4: val_loss improved from 0.42462 to 0.40963, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3243 - roc_auc: 0.9351 - val_loss: 0.4096 - val_roc_auc: 0.9085 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3104 - roc_auc: 0.9411
Epoch 5: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3071 - roc_auc: 0.9423 - val_loss: 0.4137 - val_roc_auc: 0.9094 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2467 - roc_auc: 0.9646
Epoch 6: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2448 - roc_auc: 0.9653 - val_loss: 0.4554 - val_roc_auc: 0.8966 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2352 - roc_auc: 0.9688
Epoch 7: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2349 - roc_auc: 0.9688 - val_loss: 0.4256 - val_roc_auc: 0.9064 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2119 - roc_auc: 0.9737
Epoch 8: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2116 - roc_auc: 0.9737 - val_loss: 0.5334 - val_roc_auc: 0.8935 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2185 - roc_auc: 0.9730
Epoch 9: val_loss did not improve from 0.40963

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2182 - roc_auc: 0.9730 - val_loss: 0.4645 - val_roc_auc: 0.9048 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1629 - roc_auc: 0.9860
Epoch 10: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1616 - roc_auc: 0.9863 - val_loss: 0.4359 - val_roc_auc: 0.9130 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1557 - roc_auc: 0.9869
Epoch 11: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1536 - roc_auc: 0.9873 - val_loss: 0.4455 - val_roc_auc: 0.9113 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1408 - roc_auc: 0.9903
Epoch 12: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1389 - roc_auc: 0.9905 - val_loss: 0.4524 - val_roc_auc: 0.9106 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1291 - roc_auc: 0.9922
Epoch 13: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1290 - roc_auc: 0.9922 - val_loss: 0.4645 - val_roc_auc: 0.9095 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1424 - roc_auc: 0.9897
Epoch 14: val_loss did not improve from 0.40963

Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1419 - roc_auc: 0.9897 - val_loss: 0.4546 - val_roc_auc: 0.9123 - learning_rate: 1.0000e-04
Epoch 14: early stopping
Restoring model weights from the end of the best epoch: 4.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9434, va:0.9087
-------------------- 3 --------------------
20240928 12:13:46
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6282 - roc_auc: 0.8055
Epoch 1: val_loss improved from inf to 0.51293, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6272 - roc_auc: 0.8059 - val_loss: 0.5129 - val_roc_auc: 0.8732 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4384 - roc_auc: 0.8960
Epoch 2: val_loss improved from 0.51293 to 0.49814, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.4375 - roc_auc: 0.8956 - val_loss: 0.4981 - val_roc_auc: 0.8483 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3439 - roc_auc: 0.9306
Epoch 3: val_loss improved from 0.49814 to 0.42901, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3446 - roc_auc: 0.9303 - val_loss: 0.4290 - val_roc_auc: 0.8969 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2739 - roc_auc: 0.9559
Epoch 4: val_loss did not improve from 0.42901
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2749 - roc_auc: 0.9556 - val_loss: 0.4499 - val_roc_auc: 0.8836 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2482 - roc_auc: 0.9641
Epoch 5: val_loss improved from 0.42901 to 0.42498, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.2497 - roc_auc: 0.9635 - val_loss: 0.4250 - val_roc_auc: 0.9039 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2326 - roc_auc: 0.9694
Epoch 6: val_loss improved from 0.42498 to 0.40484, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2332 - roc_auc: 0.9693 - val_loss: 0.4048 - val_roc_auc: 0.9175 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2066 - roc_auc: 0.9753
Epoch 7: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2075 - roc_auc: 0.9751 - val_loss: 0.4709 - val_roc_auc: 0.9022 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1852 - roc_auc: 0.9804
Epoch 8: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1862 - roc_auc: 0.9802 - val_loss: 0.4736 - val_roc_auc: 0.9020 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1482 - roc_auc: 0.9888
Epoch 9: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1502 - roc_auc: 0.9883 - val_loss: 0.5383 - val_roc_auc: 0.8796 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1672 - roc_auc: 0.9831
Epoch 10: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1680 - roc_auc: 0.9829 - val_loss: 0.5355 - val_roc_auc: 0.8854 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1488 - roc_auc: 0.9845
Epoch 11: val_loss did not improve from 0.40484

Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1506 - roc_auc: 0.9842 - val_loss: 0.5329 - val_roc_auc: 0.8927 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1360 - roc_auc: 0.9892
Epoch 12: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1360 - roc_auc: 0.9892 - val_loss: 0.5107 - val_roc_auc: 0.8982 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1132 - roc_auc: 0.9934
Epoch 13: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 5ms/step - loss: 0.1134 - roc_auc: 0.9934 - val_loss: 0.5093 - val_roc_auc: 0.8961 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0985 - roc_auc: 0.9945
Epoch 14: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.0989 - roc_auc: 0.9945 - val_loss: 0.5040 - val_roc_auc: 0.8969 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0918 - roc_auc: 0.9966
Epoch 15: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 7ms/step - loss: 0.0928 - roc_auc: 0.9964 - val_loss: 0.5006 - val_roc_auc: 0.8983 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.0878 - roc_auc: 0.9970
Epoch 16: val_loss did not improve from 0.40484

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.0891 - roc_auc: 0.9968 - val_loss: 0.5072 - val_roc_auc: 0.9002 - learning_rate: 1.0000e-04
Epoch 16: early stopping
Restoring model weights from the end of the best epoch: 6.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 7ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9480, va:0.9169
-------------------- 4 --------------------
20240928 12:13:56
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.7111 - roc_auc: 0.7492
Epoch 1: val_loss improved from inf to 0.48943, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.7054 - roc_auc: 0.7530 - val_loss: 0.4894 - val_roc_auc: 0.8921 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4875 - roc_auc: 0.8563
Epoch 2: val_loss improved from 0.48943 to 0.42668, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.4837 - roc_auc: 0.8583 - val_loss: 0.4267 - val_roc_auc: 0.9103 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3993 - roc_auc: 0.9010
Epoch 3: val_loss improved from 0.42668 to 0.38645, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3968 - roc_auc: 0.9020 - val_loss: 0.3865 - val_roc_auc: 0.9126 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3123 - roc_auc: 0.9404
Epoch 4: val_loss improved from 0.38645 to 0.37058, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.3129 - roc_auc: 0.9402 - val_loss: 0.3706 - val_roc_auc: 0.9154 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3092 - roc_auc: 0.9393
Epoch 5: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3088 - roc_auc: 0.9395 - val_loss: 0.4364 - val_roc_auc: 0.9040 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2837 - roc_auc: 0.9501
Epoch 6: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2830 - roc_auc: 0.9504 - val_loss: 0.4413 - val_roc_auc: 0.9037 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2285 - roc_auc: 0.9690
Epoch 7: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2292 - roc_auc: 0.9689 - val_loss: 0.4543 - val_roc_auc: 0.9108 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2449 - roc_auc: 0.9662
Epoch 8: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2448 - roc_auc: 0.9659 - val_loss: 0.4485 - val_roc_auc: 0.9114 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2220 - roc_auc: 0.9689
Epoch 9: val_loss did not improve from 0.37058

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2216 - roc_auc: 0.9691 - val_loss: 0.5201 - val_roc_auc: 0.8954 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1619 - roc_auc: 0.9885
Epoch 10: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1613 - roc_auc: 0.9886 - val_loss: 0.4759 - val_roc_auc: 0.9120 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1555 - roc_auc: 0.9888
Epoch 11: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1548 - roc_auc: 0.9889 - val_loss: 0.4483 - val_roc_auc: 0.9149 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1391 - roc_auc: 0.9922
Epoch 12: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1390 - roc_auc: 0.9922 - val_loss: 0.4492 - val_roc_auc: 0.9150 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1447 - roc_auc: 0.9914
Epoch 13: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1437 - roc_auc: 0.9915 - val_loss: 0.4537 - val_roc_auc: 0.9156 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1154 - roc_auc: 0.9954
Epoch 14: val_loss did not improve from 0.37058

Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1156 - roc_auc: 0.9954 - val_loss: 0.4499 - val_roc_auc: 0.9168 - learning_rate: 1.0000e-04
Epoch 14: early stopping
Restoring model weights from the end of the best epoch: 4.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9485, va:0.9153
---------- result ----------
[[0.         0.96279469 0.90863158]
 [1.         0.95867146 0.92912281]
 [2.         0.94342829 0.9087346 ]
 [3.         0.94799222 0.9168533 ]
 [4.         0.9484565  0.91531355]]
[cv] tr:0.9523+-0.0072,         va:0.9157+-0.0072
[oof]0.9121
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=610bf488-2498-4fb7-afac-da6dc4127cdd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># 4</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">data_pre01</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>

<span class="c1"># 8</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">data_pre02</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
<span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span>
<span class="c1">#[    ["T_Bil", "pc01", "AST_GOT", "AG_ratio", "ALP", "Alb/ALT_ex3", "TP", "D_Bil"]]</span>
<span class="n">x_train_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>train.csv
Memory usage of dataframe is 0.07 MB
Memory usage after optimization is: 0.02 MB
Decreased by 70.33%
(850, 11)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Age</th>
<th>Gender</th>
<th>T_Bil</th>
<th>D_Bil</th>
<th>ALP</th>
<th>ALT_GPT</th>
<th>AST_GOT</th>
<th>TP</th>
<th>Alb</th>
<th>AG_ratio</th>
<th>disease</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>59</td>
<td>Male</td>
<td>0.7871</td>
<td>0.1505</td>
<td>220.1250</td>
<td>13.4688</td>
<td>21.7344</td>
<td>6.8164</td>
<td>3.1113</td>
<td>1.0068</td>
<td>0</td>
</tr>
<tr>
<th>1</th>
<td>69</td>
<td>Male</td>
<td>1.0039</td>
<td>0.1957</td>
<td>221.2500</td>
<td>51.0312</td>
<td>64.7500</td>
<td>6.8906</td>
<td>3.0508</td>
<td>0.7515</td>
<td>1</td>
</tr>
<tr>
<th>2</th>
<td>65</td>
<td>Male</td>
<td>0.6572</td>
<td>0.0813</td>
<td>320.7500</td>
<td>12.6250</td>
<td>30.6094</td>
<td>5.9492</td>
<td>2.4883</td>
<td>0.7749</td>
<td>0</td>
</tr>
<tr>
<th>3</th>
<td>65</td>
<td>Male</td>
<td>0.9067</td>
<td>0.2142</td>
<td>369.2500</td>
<td>34.3438</td>
<td>54.5000</td>
<td>6.9688</td>
<td>3.6133</td>
<td>0.9883</td>
<td>1</td>
</tr>
<tr>
<th>4</th>
<td>22</td>
<td>Female</td>
<td>1.7354</td>
<td>0.1978</td>
<td>222.7500</td>
<td>20.5781</td>
<td>170.0000</td>
<td>5.8359</td>
<td>3.0684</td>
<td>1.0264</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>: (850, 15)
()
========================================
: (850, 23)
()
========================================
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 30)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=3bca0e18-6ee7-43de-b892-93c7c86a8fea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">std_tf</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_tf</span><span class="p">)</span>
<span class="n">df_std_tf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std_tf</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">std_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 30)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6bc407b4-3529-4644-84cd-68104b14b32a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">df_std_tf</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Age</th>
<th>Gender</th>
<th>T_Bil</th>
<th>D_Bil</th>
<th>ALP</th>
<th>ALT_GPT</th>
<th>AST_GOT</th>
<th>TP</th>
<th>Alb</th>
<th>AG_ratio</th>
<th>...</th>
<th>ALP/ALT_ex3</th>
<th>TB/Alb_ex3</th>
<th>pc01</th>
<th>pc02</th>
<th>pc03</th>
<th>pc04</th>
<th>pc05</th>
<th>pc06</th>
<th>pc07</th>
<th>pc08</th>
</tr>
</thead>
<tbody>
<tr>
<th>count</th>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>...</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
</tr>
<tr>
<th>mean</th>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>...</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
</tr>
<tr>
<th>std</th>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>...</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
</tr>
<tr>
<th>min</th>
<td>-2.2264</td>
<td>-2.1779</td>
<td>-0.4182</td>
<td>-0.3647</td>
<td>-0.5442</td>
<td>-0.2609</td>
<td>-0.4519</td>
<td>-2.5861</td>
<td>-2.3869</td>
<td>-2.2793</td>
<td>...</td>
<td>-1.3395</td>
<td>-0.4513</td>
<td>-1.8143</td>
<td>-2.9570</td>
<td>-4.5507</td>
<td>-3.5893</td>
<td>-4.0526</td>
<td>-2.5738</td>
<td>-2.5645</td>
<td>-5.5798</td>
</tr>
<tr>
<th>25%</th>
<td>-0.8885</td>
<td>0.4592</td>
<td>-0.3382</td>
<td>-0.2945</td>
<td>-0.2907</td>
<td>-0.1943</td>
<td>-0.3614</td>
<td>-0.3593</td>
<td>-0.6768</td>
<td>-0.6151</td>
<td>...</td>
<td>-0.4513</td>
<td>-0.3485</td>
<td>-0.6500</td>
<td>-0.5127</td>
<td>-0.5759</td>
<td>-0.5831</td>
<td>-0.4516</td>
<td>-0.6939</td>
<td>-0.4110</td>
<td>-0.6809</td>
</tr>
<tr>
<th>50%</th>
<td>0.0845</td>
<td>0.4592</td>
<td>-0.3160</td>
<td>-0.2632</td>
<td>-0.2601</td>
<td>-0.1737</td>
<td>-0.3086</td>
<td>-0.1408</td>
<td>0.1852</td>
<td>0.2664</td>
<td>...</td>
<td>-0.1298</td>
<td>-0.3098</td>
<td>-0.2113</td>
<td>-0.0081</td>
<td>-0.1470</td>
<td>-0.1222</td>
<td>-0.0978</td>
<td>0.0359</td>
<td>-0.0384</td>
<td>0.0674</td>
</tr>
<tr>
<th>75%</th>
<td>0.9359</td>
<td>0.4592</td>
<td>-0.1680</td>
<td>-0.1730</td>
<td>-0.2133</td>
<td>-0.1297</td>
<td>-0.0413</td>
<td>0.5984</td>
<td>0.3492</td>
<td>0.6319</td>
<td>...</td>
<td>0.2086</td>
<td>-0.1508</td>
<td>0.3929</td>
<td>0.3605</td>
<td>0.4822</td>
<td>0.5642</td>
<td>0.4400</td>
<td>0.4648</td>
<td>0.4170</td>
<td>0.8348</td>
</tr>
<tr>
<th>max</th>
<td>1.9089</td>
<td>0.4592</td>
<td>8.7662</td>
<td>10.8930</td>
<td>9.1694</td>
<td>9.4687</td>
<td>6.8459</td>
<td>2.0350</td>
<td>2.6770</td>
<td>2.9799</td>
<td>...</td>
<td>9.3804</td>
<td>8.7931</td>
<td>5.9864</td>
<td>9.8850</td>
<td>6.7522</td>
<td>3.7849</td>
<td>5.9316</td>
<td>4.3850</td>
<td>11.2114</td>
<td>1.8648</td>
</tr>
</tbody>
</table>
<p>8 rows  30 columns</p>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=55d7f345-1081-42dc-aa29-81dd2e4a61c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># category</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data_pre00</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

<span class="c1"># =&gt;PCA</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data_pre03</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>category
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 850 entries, 0 to 849
Data columns (total 30 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   Age           850 non-null    int8   
 1   Gender        850 non-null    uint8  
 2   T_Bil         850 non-null    float16
 3   D_Bil         850 non-null    float16
 4   ALP           850 non-null    float16
 5   ALT_GPT       850 non-null    float16
 6   AST_GOT       850 non-null    float16
 7   TP            850 non-null    float16
 8   Alb           850 non-null    float16
 9   AG_ratio      850 non-null    float16
 10  D/T_ex2       850 non-null    float16
 11  AST/ALT_ex2   850 non-null    float16
 12  TP/AST_ex2    850 non-null    float16
 13  Globulin_ex2  850 non-null    float16
 14  TB/ALT_ex3    850 non-null    float16
 15  TB/AST_ex3    850 non-null    float16
 16  TB/ALP_ex3    850 non-null    float16
 17  Alb/ALT_ex3   850 non-null    float16
 18  TP/ALT_ex3    850 non-null    float16
 19  ALP/AST_ex3   850 non-null    float16
 20  ALP/ALT_ex3   850 non-null    float16
 21  TB/Alb_ex3    850 non-null    float16
 22  pc01          850 non-null    float64
 23  pc02          850 non-null    float64
 24  pc03          850 non-null    float64
 25  pc04          850 non-null    float64
 26  pc05          850 non-null    float64
 27  pc06          850 non-null    float64
 28  pc07          850 non-null    float64
 29  pc08          850 non-null    float64
dtypes: float16(20), float64(8), int8(1), uint8(1)
memory usage: 88.1 KB
[0.23678343 0.39947655 0.50834137 0.60718368 0.69758051 0.77547846
 0.8475803  0.91601986]
: (850, 30)
: (850, 38)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=5d995c5e-5361-4b46-8fdb-3434b876ba21">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">input_num</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">38</span><span class="p">,))</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">input_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"sigmoid"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">input_num</span><span class="p">,</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="s2">"Adam"</span><span class="p">,</span>
        <span class="n">loss</span><span class="o">=</span><span class="s2">"binary_crossentropy"</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">AUC</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"roc_auc"</span><span class="p">)],</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>


<span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"PYTHONHASHSEED"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">session_conf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
        <span class="n">intra_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inter_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span>
        <span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">(),</span> <span class="n">config</span><span class="o">=</span><span class="n">session_conf</span>
    <span class="p">)</span>
    <span class="c1"># tf.compat.v1.keras.backend.set_session(sess)</span>
    <span class="n">K</span><span class="o">.</span><span class="n">set_session</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_tf</span><span class="p">(</span>
    <span class="n">input_x</span><span class="p">,</span>
    <span class="n">input_y</span><span class="p">,</span>
    <span class="n">input_id</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># </span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">))</span>
    <span class="c1"># </span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># validation</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># 1.</span>
    <span class="k">for</span> <span class="n">nfold</span> <span class="ow">in</span> <span class="n">list_nfold</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>

        <span class="n">idx_tr</span><span class="p">,</span> <span class="n">idx_va</span> <span class="o">=</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_va</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_va</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="n">x_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_va</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># </span>
        <span class="n">fname_tf</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">EXP_MODEL</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"model_tf_fold</span><span class="si">{</span><span class="n">nfold</span><span class="si">}</span><span class="s2">.weights.h5"</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">):</span>
            <span class="c1"># if mode_train == 'train':</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"trainning start!"</span><span class="p">)</span>
            <span class="n">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x_tr</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span>
                <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">),</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">ModelCheckpoint</span><span class="p">(</span>
                        <span class="n">filepath</span><span class="o">=</span><span class="n">fname_tf</span><span class="p">,</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">EarlyStopping</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">min_delta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">ReduceLROnPlateau</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">],</span>
                <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"model load."</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">)</span>

        <span class="c1"># valid</span>
        <span class="n">y_tr_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_tr</span><span class="p">)</span>
        <span class="n">y_va_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_va</span><span class="p">)</span>

        <span class="n">metric_tr</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">y_tr_pred</span><span class="p">)</span>
        <span class="n">metric_va</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_va</span><span class="p">,</span> <span class="n">y_va_pred</span><span class="p">)</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">nfold</span><span class="p">,</span> <span class="n">metric_tr</span><span class="p">,</span> <span class="n">metric_va</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[auc] tr:</span><span class="si">{</span><span class="n">metric_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, va:</span><span class="si">{</span><span class="n">metric_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="c1"># oof</span>
        <span class="n">train_oof</span><span class="p">[</span><span class="n">idx_va</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_va_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
    <span class="c1"># metric</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">oof</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"[oof]</span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span><span class="w"> </span><span class="n">train_oof</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># oof</span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">input_id</span><span class="p">,</span>
            <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"pred"</span><span class="p">:</span> <span class="n">train_oof</span><span class="p">}),</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># stdout  stderr </span>
    <span class="n">stdout_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDOUT"</span><span class="p">)</span>
    <span class="n">stderr_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDERR"</span><span class="p">)</span>

    <span class="n">sys_stdout_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span>
    <span class="n">sys_stderr_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span>

    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stdout_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stderr_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># </span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">sys_stdout_backup</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">sys_stderr_backup</span>

    <span class="k">return</span> <span class="n">train_oof</span><span class="p">,</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=5a7433eb-f822-40b6-be8f-2588edda7463">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">std_tf</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_tf</span><span class="p">)</span>
<span class="n">df_std_tf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std_tf</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">std_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 30)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=ad9e31bc-3545-480e-b700-96d66e0c3476">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span>
<span class="c1">#[    ["T_Bil", "pc01", "AST_GOT", "AG_ratio", "ALP", "Alb/ALT_ex3", "TP", "D_Bil"]]</span>
<span class="n">x_train_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 38)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bc968b7f-da44-471b-b5cb-83bcd555bf0c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">std_tf</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_tf</span><span class="p">)</span>
<span class="n">df_std_tf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std_tf</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">std_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 38)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b9755402-3f00-4e61-9477-b676302db7e0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">train_oof_tf</span><span class="p">,</span> <span class="n">metrics_tf</span> <span class="o">=</span> <span class="n">train_tf</span><span class="p">(</span>
    <span class="n">df_std_tf</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">id_train</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_14"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)                    </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">       Param # </span>

 input_layer_14 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">38</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_56 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">19,968</span> 

 batch_normalization_42           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">2,048</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_42 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_57 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                   <span style="color: #00af00; text-decoration-color: #00af00">131,328</span> 

 batch_normalization_43           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">1,024</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_43 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_58 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">32,896</span> 

 batch_normalization_44           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">512</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_44 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_59 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">129</span> 

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">187,905</span> (734.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">186,113</span> (727.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,792</span> (7.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>-------------------- 0 --------------------
20240928 12:23:30
(680, 38) (170, 38)
trainning start!
Epoch 1/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6714 - roc_auc: 0.7737
Epoch 1: val_loss improved from inf to 0.50901, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6691 - roc_auc: 0.7751 - val_loss: 0.5090 - val_roc_auc: 0.8298 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4183 - roc_auc: 0.9010
Epoch 2: val_loss improved from 0.50901 to 0.49579, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.4162 - roc_auc: 0.9014 - val_loss: 0.4958 - val_roc_auc: 0.8274 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3637 - roc_auc: 0.9235
Epoch 3: val_loss improved from 0.49579 to 0.46269, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.3616 - roc_auc: 0.9238 - val_loss: 0.4627 - val_roc_auc: 0.8513 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2922 - roc_auc: 0.9493
Epoch 4: val_loss did not improve from 0.46269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.2921 - roc_auc: 0.9493 - val_loss: 0.4659 - val_roc_auc: 0.8587 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2356 - roc_auc: 0.9673
Epoch 5: val_loss did not improve from 0.46269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2357 - roc_auc: 0.9673 - val_loss: 0.5284 - val_roc_auc: 0.8585 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2441 - roc_auc: 0.9642
Epoch 6: val_loss did not improve from 0.46269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2441 - roc_auc: 0.9642 - val_loss: 0.5371 - val_roc_auc: 0.8600 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2297 - roc_auc: 0.9683
Epoch 7: val_loss did not improve from 0.46269
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2292 - roc_auc: 0.9684 - val_loss: 0.6249 - val_roc_auc: 0.8594 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1843 - roc_auc: 0.9808
Epoch 8: val_loss did not improve from 0.46269

Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1892 - roc_auc: 0.9794 - val_loss: 0.6131 - val_roc_auc: 0.8589 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2297 - roc_auc: 0.9668
Epoch 9: val_loss improved from 0.46269 to 0.45876, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.2285 - roc_auc: 0.9672 - val_loss: 0.4588 - val_roc_auc: 0.8953 - learning_rate: 1.0000e-04
Epoch 10/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1610 - roc_auc: 0.9876
Epoch 10: val_loss improved from 0.45876 to 0.43901, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1607 - roc_auc: 0.9876 - val_loss: 0.4390 - val_roc_auc: 0.9021 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1656 - roc_auc: 0.9877
Epoch 11: val_loss improved from 0.43901 to 0.43458, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1646 - roc_auc: 0.9878 - val_loss: 0.4346 - val_roc_auc: 0.9040 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1382 - roc_auc: 0.9935
Epoch 12: val_loss improved from 0.43458 to 0.43052, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1368 - roc_auc: 0.9936 - val_loss: 0.4305 - val_roc_auc: 0.9079 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1528 - roc_auc: 0.9914
Epoch 13: val_loss improved from 0.43052 to 0.41953, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1506 - roc_auc: 0.9915 - val_loss: 0.4195 - val_roc_auc: 0.9109 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1209 - roc_auc: 0.9954
Epoch 14: val_loss did not improve from 0.41953
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1194 - roc_auc: 0.9955 - val_loss: 0.4290 - val_roc_auc: 0.9098 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1269 - roc_auc: 0.9943
Epoch 15: val_loss did not improve from 0.41953
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1248 - roc_auc: 0.9945 - val_loss: 0.4340 - val_roc_auc: 0.9078 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1051 - roc_auc: 0.9972
Epoch 16: val_loss did not improve from 0.41953
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1050 - roc_auc: 0.9970 - val_loss: 0.4324 - val_roc_auc: 0.9104 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1115 - roc_auc: 0.9964
Epoch 17: val_loss did not improve from 0.41953
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1097 - roc_auc: 0.9965 - val_loss: 0.4442 - val_roc_auc: 0.9058 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1051 - roc_auc: 0.9963
Epoch 18: val_loss did not improve from 0.41953

Epoch 18: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1044 - roc_auc: 0.9964 - val_loss: 0.4468 - val_roc_auc: 0.9053 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0914 - roc_auc: 0.9976
Epoch 19: val_loss did not improve from 0.41953
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0910 - roc_auc: 0.9976 - val_loss: 0.4488 - val_roc_auc: 0.9046 - learning_rate: 1.0000e-05
Epoch 20/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1027 - roc_auc: 0.9966
Epoch 20: val_loss did not improve from 0.41953
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1011 - roc_auc: 0.9968 - val_loss: 0.4489 - val_roc_auc: 0.9049 - learning_rate: 1.0000e-05
Epoch 21/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0996 - roc_auc: 0.9985
Epoch 21: val_loss did not improve from 0.41953
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0982 - roc_auc: 0.9985 - val_loss: 0.4502 - val_roc_auc: 0.9037 - learning_rate: 1.0000e-05
Epoch 22/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0986 - roc_auc: 0.9977
Epoch 22: val_loss did not improve from 0.41953
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0977 - roc_auc: 0.9977 - val_loss: 0.4468 - val_roc_auc: 0.9052 - learning_rate: 1.0000e-05
Epoch 23/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0875 - roc_auc: 0.9993
Epoch 23: val_loss did not improve from 0.41953

Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0862 - roc_auc: 0.9993 - val_loss: 0.4495 - val_roc_auc: 0.9049 - learning_rate: 1.0000e-05
Epoch 23: early stopping
Restoring model weights from the end of the best epoch: 13.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9616, va:0.9109
-------------------- 1 --------------------
20240928 12:23:43
(680, 38) (170, 38)
trainning start!
Epoch 1/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6046 - roc_auc: 0.7973
Epoch 1: val_loss improved from inf to 0.49199, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6008 - roc_auc: 0.8008 - val_loss: 0.4920 - val_roc_auc: 0.8971 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4171 - roc_auc: 0.8919
Epoch 2: val_loss improved from 0.49199 to 0.45866, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.4171 - roc_auc: 0.8920 - val_loss: 0.4587 - val_roc_auc: 0.8978 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3406 - roc_auc: 0.9277
Epoch 3: val_loss did not improve from 0.45866
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3410 - roc_auc: 0.9277 - val_loss: 0.4665 - val_roc_auc: 0.8813 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2986 - roc_auc: 0.9477
Epoch 4: val_loss improved from 0.45866 to 0.37725, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2987 - roc_auc: 0.9473 - val_loss: 0.3772 - val_roc_auc: 0.9176 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2630 - roc_auc: 0.9575
Epoch 5: val_loss did not improve from 0.37725
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2638 - roc_auc: 0.9571 - val_loss: 0.4363 - val_roc_auc: 0.8883 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2423 - roc_auc: 0.9661
Epoch 6: val_loss did not improve from 0.37725
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2417 - roc_auc: 0.9662 - val_loss: 0.4519 - val_roc_auc: 0.8876 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2039 - roc_auc: 0.9777
Epoch 7: val_loss did not improve from 0.37725
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2044 - roc_auc: 0.9773 - val_loss: 0.4167 - val_roc_auc: 0.9126 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2136 - roc_auc: 0.9736
Epoch 8: val_loss improved from 0.37725 to 0.35855, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2118 - roc_auc: 0.9740 - val_loss: 0.3585 - val_roc_auc: 0.9269 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1943 - roc_auc: 0.9788
Epoch 9: val_loss did not improve from 0.35855
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1927 - roc_auc: 0.9790 - val_loss: 0.4430 - val_roc_auc: 0.9054 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1881 - roc_auc: 0.9802
Epoch 10: val_loss did not improve from 0.35855
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1877 - roc_auc: 0.9802 - val_loss: 0.3842 - val_roc_auc: 0.9331 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1860 - roc_auc: 0.9771
Epoch 11: val_loss improved from 0.35855 to 0.32352, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.1837 - roc_auc: 0.9778 - val_loss: 0.3235 - val_roc_auc: 0.9436 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1701 - roc_auc: 0.9822
Epoch 12: val_loss did not improve from 0.32352
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1699 - roc_auc: 0.9822 - val_loss: 0.3605 - val_roc_auc: 0.9404 - learning_rate: 0.0010
Epoch 13/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1322 - roc_auc: 0.9901
Epoch 13: val_loss did not improve from 0.32352
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1315 - roc_auc: 0.9903 - val_loss: 0.4471 - val_roc_auc: 0.9126 - learning_rate: 0.0010
Epoch 14/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0980 - roc_auc: 0.9954
Epoch 14: val_loss did not improve from 0.32352
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0981 - roc_auc: 0.9954 - val_loss: 0.3989 - val_roc_auc: 0.9371 - learning_rate: 0.0010
Epoch 15/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1253 - roc_auc: 0.9906
Epoch 15: val_loss did not improve from 0.32352
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1252 - roc_auc: 0.9906 - val_loss: 0.3870 - val_roc_auc: 0.9308 - learning_rate: 0.0010
Epoch 16/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1041 - roc_auc: 0.9953
Epoch 16: val_loss did not improve from 0.32352

Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1042 - roc_auc: 0.9953 - val_loss: 0.4026 - val_roc_auc: 0.9316 - learning_rate: 0.0010
Epoch 17/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0925 - roc_auc: 0.9965
Epoch 17: val_loss did not improve from 0.32352
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0917 - roc_auc: 0.9964 - val_loss: 0.3855 - val_roc_auc: 0.9324 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0708 - roc_auc: 0.9979
Epoch 18: val_loss did not improve from 0.32352
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0709 - roc_auc: 0.9979 - val_loss: 0.3868 - val_roc_auc: 0.9302 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0522 - roc_auc: 0.9989
Epoch 19: val_loss did not improve from 0.32352
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0521 - roc_auc: 0.9989 - val_loss: 0.3775 - val_roc_auc: 0.9343 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0486 - roc_auc: 0.9998
Epoch 20: val_loss did not improve from 0.32352
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0487 - roc_auc: 0.9998 - val_loss: 0.3691 - val_roc_auc: 0.9350 - learning_rate: 1.0000e-04
Epoch 21/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0535 - roc_auc: 0.9993
Epoch 21: val_loss did not improve from 0.32352

Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0531 - roc_auc: 0.9993 - val_loss: 0.3696 - val_roc_auc: 0.9345 - learning_rate: 1.0000e-04
Epoch 21: early stopping
Restoring model weights from the end of the best epoch: 11.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9622, va:0.9437
-------------------- 2 --------------------
20240928 12:23:55
(680, 38) (170, 38)
trainning start!
Epoch 1/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.6660 - roc_auc: 0.7806
Epoch 1: val_loss improved from inf to 0.49869, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6615 - roc_auc: 0.7829 - val_loss: 0.4987 - val_roc_auc: 0.9092 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4645 - roc_auc: 0.8732
Epoch 2: val_loss improved from 0.49869 to 0.42100, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4637 - roc_auc: 0.8735 - val_loss: 0.4210 - val_roc_auc: 0.9199 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3798 - roc_auc: 0.9113
Epoch 3: val_loss improved from 0.42100 to 0.41730, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3796 - roc_auc: 0.9114 - val_loss: 0.4173 - val_roc_auc: 0.9166 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3594 - roc_auc: 0.9212
Epoch 4: val_loss improved from 0.41730 to 0.41423, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3586 - roc_auc: 0.9215 - val_loss: 0.4142 - val_roc_auc: 0.9075 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2949 - roc_auc: 0.9505
Epoch 5: val_loss did not improve from 0.41423
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2947 - roc_auc: 0.9505 - val_loss: 0.4485 - val_roc_auc: 0.9121 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2665 - roc_auc: 0.9597
Epoch 6: val_loss did not improve from 0.41423
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2655 - roc_auc: 0.9600 - val_loss: 0.4555 - val_roc_auc: 0.9104 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2533 - roc_auc: 0.9614
Epoch 7: val_loss did not improve from 0.41423
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2532 - roc_auc: 0.9614 - val_loss: 0.5655 - val_roc_auc: 0.8886 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2269 - roc_auc: 0.9695
Epoch 8: val_loss did not improve from 0.41423
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2267 - roc_auc: 0.9695 - val_loss: 0.5653 - val_roc_auc: 0.8967 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2125 - roc_auc: 0.9738
Epoch 9: val_loss did not improve from 0.41423

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2125 - roc_auc: 0.9738 - val_loss: 0.5243 - val_roc_auc: 0.9111 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1908 - roc_auc: 0.9778
Epoch 10: val_loss did not improve from 0.41423
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1906 - roc_auc: 0.9779 - val_loss: 0.5101 - val_roc_auc: 0.9050 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1548 - roc_auc: 0.9875
Epoch 11: val_loss did not improve from 0.41423
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1542 - roc_auc: 0.9875 - val_loss: 0.5171 - val_roc_auc: 0.9035 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1407 - roc_auc: 0.9903
Epoch 12: val_loss did not improve from 0.41423
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1404 - roc_auc: 0.9904 - val_loss: 0.5297 - val_roc_auc: 0.9024 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1385 - roc_auc: 0.9908
Epoch 13: val_loss did not improve from 0.41423
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1377 - roc_auc: 0.9909 - val_loss: 0.5328 - val_roc_auc: 0.9035 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1426 - roc_auc: 0.9895
Epoch 14: val_loss did not improve from 0.41423

Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1414 - roc_auc: 0.9897 - val_loss: 0.5377 - val_roc_auc: 0.9027 - learning_rate: 1.0000e-04
Epoch 14: early stopping
Restoring model weights from the end of the best epoch: 4.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9439, va:0.9069
-------------------- 3 --------------------
20240928 12:24:04
(680, 38) (170, 38)
trainning start!
Epoch 1/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.6580 - roc_auc: 0.7945
Epoch 1: val_loss improved from inf to 0.50113, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6528 - roc_auc: 0.7971 - val_loss: 0.5011 - val_roc_auc: 0.8747 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4351 - roc_auc: 0.8961
Epoch 2: val_loss did not improve from 0.50113
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4339 - roc_auc: 0.8956 - val_loss: 0.5230 - val_roc_auc: 0.8172 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3572 - roc_auc: 0.9241
Epoch 3: val_loss improved from 0.50113 to 0.48551, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3573 - roc_auc: 0.9240 - val_loss: 0.4855 - val_roc_auc: 0.8527 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3039 - roc_auc: 0.9436
Epoch 4: val_loss improved from 0.48551 to 0.46733, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3040 - roc_auc: 0.9436 - val_loss: 0.4673 - val_roc_auc: 0.8687 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2562 - roc_auc: 0.9606
Epoch 5: val_loss did not improve from 0.46733
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2566 - roc_auc: 0.9604 - val_loss: 0.5049 - val_roc_auc: 0.8643 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2385 - roc_auc: 0.9659
Epoch 6: val_loss did not improve from 0.46733
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2389 - roc_auc: 0.9658 - val_loss: 0.4723 - val_roc_auc: 0.8908 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2113 - roc_auc: 0.9752
Epoch 7: val_loss did not improve from 0.46733
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2117 - roc_auc: 0.9750 - val_loss: 0.5536 - val_roc_auc: 0.8733 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2274 - roc_auc: 0.9689
Epoch 8: val_loss did not improve from 0.46733
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2296 - roc_auc: 0.9683 - val_loss: 0.5724 - val_roc_auc: 0.8705 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2214 - roc_auc: 0.9717
Epoch 9: val_loss did not improve from 0.46733

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2219 - roc_auc: 0.9715 - val_loss: 0.5060 - val_roc_auc: 0.8833 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1662 - roc_auc: 0.9855
Epoch 10: val_loss did not improve from 0.46733
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1664 - roc_auc: 0.9855 - val_loss: 0.4806 - val_roc_auc: 0.8942 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1296 - roc_auc: 0.9925
Epoch 11: val_loss did not improve from 0.46733
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1300 - roc_auc: 0.9924 - val_loss: 0.4835 - val_roc_auc: 0.8937 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1349 - roc_auc: 0.9925
Epoch 12: val_loss did not improve from 0.46733
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1351 - roc_auc: 0.9925 - val_loss: 0.4921 - val_roc_auc: 0.8914 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1261 - roc_auc: 0.9924
Epoch 13: val_loss did not improve from 0.46733
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1266 - roc_auc: 0.9923 - val_loss: 0.5080 - val_roc_auc: 0.8852 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1107 - roc_auc: 0.9944
Epoch 14: val_loss did not improve from 0.46733

Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1115 - roc_auc: 0.9943 - val_loss: 0.4923 - val_roc_auc: 0.8929 - learning_rate: 1.0000e-04
Epoch 14: early stopping
Restoring model weights from the end of the best epoch: 4.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9353, va:0.8691
-------------------- 4 --------------------
20240928 12:24:12
(680, 38) (170, 38)
trainning start!
Epoch 1/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.6792 - roc_auc: 0.7617
Epoch 1: val_loss improved from inf to 0.46847, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 13ms/step - loss: 0.6701 - roc_auc: 0.7676 - val_loss: 0.4685 - val_roc_auc: 0.9138 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.5028 - roc_auc: 0.8530
Epoch 2: val_loss improved from 0.46847 to 0.41518, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.5007 - roc_auc: 0.8540 - val_loss: 0.4152 - val_roc_auc: 0.9095 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3766 - roc_auc: 0.9104
Epoch 3: val_loss improved from 0.41518 to 0.34882, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.3757 - roc_auc: 0.9110 - val_loss: 0.3488 - val_roc_auc: 0.9274 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3353 - roc_auc: 0.9290
Epoch 4: val_loss did not improve from 0.34882
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3352 - roc_auc: 0.9290 - val_loss: 0.3512 - val_roc_auc: 0.9287 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3239 - roc_auc: 0.9342
Epoch 5: val_loss did not improve from 0.34882
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3229 - roc_auc: 0.9346 - val_loss: 0.3806 - val_roc_auc: 0.9179 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2774 - roc_auc: 0.9533
Epoch 6: val_loss did not improve from 0.34882
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2758 - roc_auc: 0.9537 - val_loss: 0.4074 - val_roc_auc: 0.9169 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2612 - roc_auc: 0.9588
Epoch 7: val_loss did not improve from 0.34882
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2614 - roc_auc: 0.9587 - val_loss: 0.4000 - val_roc_auc: 0.9220 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2427 - roc_auc: 0.9652
Epoch 8: val_loss did not improve from 0.34882

Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2426 - roc_auc: 0.9652 - val_loss: 0.4244 - val_roc_auc: 0.9137 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2002 - roc_auc: 0.9799
Epoch 9: val_loss did not improve from 0.34882
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1986 - roc_auc: 0.9802 - val_loss: 0.3926 - val_roc_auc: 0.9221 - learning_rate: 1.0000e-04
Epoch 10/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1670 - roc_auc: 0.9878
Epoch 10: val_loss did not improve from 0.34882
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1665 - roc_auc: 0.9876 - val_loss: 0.3900 - val_roc_auc: 0.9249 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1560 - roc_auc: 0.9903
Epoch 11: val_loss did not improve from 0.34882
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1559 - roc_auc: 0.9903 - val_loss: 0.3839 - val_roc_auc: 0.9247 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1650 - roc_auc: 0.9860
Epoch 12: val_loss did not improve from 0.34882
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1637 - roc_auc: 0.9864 - val_loss: 0.3828 - val_roc_auc: 0.9258 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1366 - roc_auc: 0.9932
Epoch 13: val_loss did not improve from 0.34882

Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1363 - roc_auc: 0.9933 - val_loss: 0.3850 - val_roc_auc: 0.9253 - learning_rate: 1.0000e-04
Epoch 13: early stopping
Restoring model weights from the end of the best epoch: 3.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9461, va:0.9271
---------- result ----------
[[0.         0.96162163 0.91087719]
 [1.         0.96216439 0.9437193 ]
 [2.         0.94391885 0.90691489]
 [3.         0.93527278 0.86912094]
 [4.         0.94605627 0.92707167]]
[cv] tr:0.9498+-0.0105,         va:0.9115+-0.0105
[oof]0.9097
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c7f28f98-b6ef-40cf-b5bd-d96758fab4d4">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1">#  exp002</span>
<span class="c1"># =================================================</span>
<span class="k">def</span> <span class="nf">data_pre01</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># Gender</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"Gender"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">"Gender"</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">"uint8"</span><span class="p">)</span>

    <span class="c1"># 1:/D/T 3/2</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"D/T_ex2"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"D_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span>

    <span class="c1"># 2:AST/ALTDe Ritis # 6/5</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"AST/ALT_ex2"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AST_GOT"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>

    <span class="c1"># 3:./AST  7/6</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TP/AST_ex2"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"TP"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AST_GOT"</span><span class="p">]</span>

    <span class="c1"># 4:.  1/(8*9)</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"Globulin_ex2"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">"Alb"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AG_ratio"</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">":"</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"()"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f1242a50-74fb-4dc9-aca0-aaf28f90a237">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1">#  exp03</span>
<span class="c1"># =================================================</span>
<span class="k">def</span> <span class="nf">data_pre02</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># 1:/ / ALT   / AST 2/5 2/6</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TB/ALT_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TB/AST_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AST_GOT"</span><span class="p">]</span>

    <span class="c1"># 2: /ALP  2/4</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TB/ALP_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALP"</span><span class="p">]</span>

    <span class="c1"># 3:/ALT 8/5</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"Alb/ALT_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"Alb"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>

    <span class="c1"># 4:/ALT 7/5</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TP/ALT_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"TP"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>

    <span class="c1"># 5:ALP/ASTALP/ALT 4/6 4/5</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"ALP/AST_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALP"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"AST_GOT"</span><span class="p">]</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"ALP/ALT_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALP"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"ALT_GPT"</span><span class="p">]</span>

    <span class="c1"># 6: /  2/8</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">"TB/Alb_ex3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s2">"Alb"</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">":"</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"()"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=ea8a8716-41f0-44ed-9b57-ab90bbfc9009">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">data_pre03</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"T_Bil"</span><span class="p">,</span> <span class="s2">"D_Bil"</span><span class="p">,</span> <span class="s2">"ALP"</span><span class="p">,</span> <span class="s2">"ALT_GPT"</span><span class="p">,</span> <span class="s2">"AST_GOT"</span><span class="p">]:</span>
        <span class="n">df2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1">#  pca,UMAP </span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span>
    <span class="n">df_std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

    <span class="n">n_components</span> <span class="o">=</span> <span class="mi">8</span>

    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_std</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_components</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_components</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"components"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"components"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"cumulative explained variance"</span><span class="p">)</span>

    <span class="n">X_pc</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_std</span><span class="p">)</span>

    <span class="n">df_pc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X_pc</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">df_pc</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"pc01"</span><span class="p">,</span> <span class="s2">"pc02"</span><span class="p">,</span> <span class="s2">"pc03"</span><span class="p">,</span> <span class="s2">"pc04"</span><span class="p">,</span> <span class="s2">"pc05"</span><span class="p">,</span> <span class="s2">"pc06"</span><span class="p">,</span> <span class="s2">"pc07"</span><span class="p">,</span> <span class="s2">"pc08"</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">":"</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">df_pc</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">":"</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=e3faa96f-be2e-46e9-9aa8-311be5f92b3a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># Load Data</span>
<span class="c1"># =================================================</span>
<span class="c1"># train</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>train.csv
Memory usage of dataframe is 0.07 MB
Memory usage after optimization is: 0.02 MB
Decreased by 70.33%
(850, 11)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Age</th>
<th>Gender</th>
<th>T_Bil</th>
<th>D_Bil</th>
<th>ALP</th>
<th>ALT_GPT</th>
<th>AST_GOT</th>
<th>TP</th>
<th>Alb</th>
<th>AG_ratio</th>
<th>disease</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>59</td>
<td>Male</td>
<td>0.7871</td>
<td>0.1505</td>
<td>220.1250</td>
<td>13.4688</td>
<td>21.7344</td>
<td>6.8164</td>
<td>3.1113</td>
<td>1.0068</td>
<td>0</td>
</tr>
<tr>
<th>1</th>
<td>69</td>
<td>Male</td>
<td>1.0039</td>
<td>0.1957</td>
<td>221.2500</td>
<td>51.0312</td>
<td>64.7500</td>
<td>6.8906</td>
<td>3.0508</td>
<td>0.7515</td>
<td>1</td>
</tr>
<tr>
<th>2</th>
<td>65</td>
<td>Male</td>
<td>0.6572</td>
<td>0.0813</td>
<td>320.7500</td>
<td>12.6250</td>
<td>30.6094</td>
<td>5.9492</td>
<td>2.4883</td>
<td>0.7749</td>
<td>0</td>
</tr>
<tr>
<th>3</th>
<td>65</td>
<td>Male</td>
<td>0.9067</td>
<td>0.2142</td>
<td>369.2500</td>
<td>34.3438</td>
<td>54.5000</td>
<td>6.9688</td>
<td>3.6133</td>
<td>0.9883</td>
<td>1</td>
</tr>
<tr>
<th>4</th>
<td>22</td>
<td>Female</td>
<td>1.7354</td>
<td>0.1978</td>
<td>222.7500</td>
<td>20.5781</td>
<td>170.0000</td>
<td>5.8359</td>
<td>3.0684</td>
<td>1.0264</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=887df406-ed31-4714-9305-568d203ed833">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># ==========================================================</span>
<span class="c1"># 4</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">data_pre01</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>

<span class="c1"># 8</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">data_pre02</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>: (850, 15)
()
========================================
: (850, 23)
()
========================================
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=099bff23-d4d3-461e-a452-a606b3367d43">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># =================================================</span>
<span class="n">set_file</span> <span class="o">=</span> <span class="n">df_train</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">set_file</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="n">target_columns</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">set_file</span><span class="p">[</span><span class="n">target_columns</span><span class="p">]</span>
<span class="n">id_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">set_file</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">id_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>(850, 22) (850,) (850, 1)
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c841aac7-2dca-4b4e-a982-190c148f9511">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span>
<span class="c1">#[    ["T_Bil", "pc01", "AST_GOT", "AG_ratio", "ALP", "Alb/ALT_ex3", "TP", "D_Bil"]]</span>
<span class="n">x_train_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 22)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=124acb05-bea7-4a0e-9ca3-2eb1c86b629d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># category</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data_pre00</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

<span class="c1"># =&gt;PCA</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data_pre03</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>category
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 850 entries, 0 to 849
Data columns (total 22 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   Age           850 non-null    int8   
 1   Gender        850 non-null    uint8  
 2   T_Bil         850 non-null    float16
 3   D_Bil         850 non-null    float16
 4   ALP           850 non-null    float16
 5   ALT_GPT       850 non-null    float16
 6   AST_GOT       850 non-null    float16
 7   TP            850 non-null    float16
 8   Alb           850 non-null    float16
 9   AG_ratio      850 non-null    float16
 10  D/T_ex2       850 non-null    float16
 11  AST/ALT_ex2   850 non-null    float16
 12  TP/AST_ex2    850 non-null    float16
 13  Globulin_ex2  850 non-null    float16
 14  TB/ALT_ex3    850 non-null    float16
 15  TB/AST_ex3    850 non-null    float16
 16  TB/ALP_ex3    850 non-null    float16
 17  Alb/ALT_ex3   850 non-null    float16
 18  TP/ALT_ex3    850 non-null    float16
 19  ALP/AST_ex3   850 non-null    float16
 20  ALP/ALT_ex3   850 non-null    float16
 21  TB/Alb_ex3    850 non-null    float16
dtypes: float16(20), int8(1), uint8(1)
memory usage: 35.0 KB
[0.27742435 0.45382265 0.55682828 0.64615944 0.72397803 0.78474861
 0.83761075 0.88547489]
: (850, 22)
: (850, 30)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c8c62f0f-47e7-4549-a6d1-31bc4a6ca81a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span>
<span class="c1">#[    ["T_Bil", "pc01", "AST_GOT", "AG_ratio", "ALP", "Alb/ALT_ex3", "TP", "D_Bil"]]</span>
<span class="n">x_train_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 30)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=f47a510b-315f-44f4-b452-6e51b3a0a5de">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">std_tf</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_tf</span><span class="p">)</span>
<span class="n">df_std_tf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std_tf</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">std_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 30)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=3612edb8-8620-45af-a780-77609ec5cfb7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">df_std_tf</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Age</th>
<th>Gender</th>
<th>T_Bil</th>
<th>D_Bil</th>
<th>ALP</th>
<th>ALT_GPT</th>
<th>AST_GOT</th>
<th>TP</th>
<th>Alb</th>
<th>AG_ratio</th>
<th>...</th>
<th>ALP/ALT_ex3</th>
<th>TB/Alb_ex3</th>
<th>pc01</th>
<th>pc02</th>
<th>pc03</th>
<th>pc04</th>
<th>pc05</th>
<th>pc06</th>
<th>pc07</th>
<th>pc08</th>
</tr>
</thead>
<tbody>
<tr>
<th>count</th>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>...</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
</tr>
<tr>
<th>mean</th>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>...</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
</tr>
<tr>
<th>std</th>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>...</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
</tr>
<tr>
<th>min</th>
<td>-2.2264</td>
<td>-2.1779</td>
<td>-0.4182</td>
<td>-0.3647</td>
<td>-0.5442</td>
<td>-0.2609</td>
<td>-0.4519</td>
<td>-2.5861</td>
<td>-2.3869</td>
<td>-2.2793</td>
<td>...</td>
<td>-1.3395</td>
<td>-0.4513</td>
<td>-1.8143</td>
<td>-2.9570</td>
<td>-4.5507</td>
<td>-3.5893</td>
<td>-4.0526</td>
<td>-2.5738</td>
<td>-2.5645</td>
<td>-5.5798</td>
</tr>
<tr>
<th>25%</th>
<td>-0.8885</td>
<td>0.4592</td>
<td>-0.3382</td>
<td>-0.2945</td>
<td>-0.2907</td>
<td>-0.1943</td>
<td>-0.3614</td>
<td>-0.3593</td>
<td>-0.6768</td>
<td>-0.6151</td>
<td>...</td>
<td>-0.4513</td>
<td>-0.3485</td>
<td>-0.6500</td>
<td>-0.5127</td>
<td>-0.5759</td>
<td>-0.5831</td>
<td>-0.4516</td>
<td>-0.6939</td>
<td>-0.4110</td>
<td>-0.6809</td>
</tr>
<tr>
<th>50%</th>
<td>0.0845</td>
<td>0.4592</td>
<td>-0.3160</td>
<td>-0.2632</td>
<td>-0.2601</td>
<td>-0.1737</td>
<td>-0.3086</td>
<td>-0.1408</td>
<td>0.1852</td>
<td>0.2664</td>
<td>...</td>
<td>-0.1298</td>
<td>-0.3098</td>
<td>-0.2113</td>
<td>-0.0081</td>
<td>-0.1470</td>
<td>-0.1222</td>
<td>-0.0978</td>
<td>0.0359</td>
<td>-0.0384</td>
<td>0.0674</td>
</tr>
<tr>
<th>75%</th>
<td>0.9359</td>
<td>0.4592</td>
<td>-0.1680</td>
<td>-0.1730</td>
<td>-0.2133</td>
<td>-0.1297</td>
<td>-0.0413</td>
<td>0.5984</td>
<td>0.3492</td>
<td>0.6319</td>
<td>...</td>
<td>0.2086</td>
<td>-0.1508</td>
<td>0.3929</td>
<td>0.3605</td>
<td>0.4822</td>
<td>0.5642</td>
<td>0.4400</td>
<td>0.4648</td>
<td>0.4170</td>
<td>0.8348</td>
</tr>
<tr>
<th>max</th>
<td>1.9089</td>
<td>0.4592</td>
<td>8.7662</td>
<td>10.8930</td>
<td>9.1694</td>
<td>9.4687</td>
<td>6.8459</td>
<td>2.0350</td>
<td>2.6770</td>
<td>2.9799</td>
<td>...</td>
<td>9.3804</td>
<td>8.7931</td>
<td>5.9864</td>
<td>9.8850</td>
<td>6.7522</td>
<td>3.7849</td>
<td>5.9316</td>
<td>4.3850</td>
<td>11.2114</td>
<td>1.8648</td>
</tr>
</tbody>
</table>
<p>8 rows  30 columns</p>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=85f4d214-774b-4e4f-8644-f049da6f252d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">input_num</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,))</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">input_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"sigmoid"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">input_num</span><span class="p">,</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="s2">"Adam"</span><span class="p">,</span>
        <span class="n">loss</span><span class="o">=</span><span class="s2">"binary_crossentropy"</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">AUC</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"roc_auc"</span><span class="p">)],</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>


<span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"PYTHONHASHSEED"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">session_conf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
        <span class="n">intra_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inter_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span>
        <span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">(),</span> <span class="n">config</span><span class="o">=</span><span class="n">session_conf</span>
    <span class="p">)</span>
    <span class="c1"># tf.compat.v1.keras.backend.set_session(sess)</span>
    <span class="n">K</span><span class="o">.</span><span class="n">set_session</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_tf</span><span class="p">(</span>
    <span class="n">input_x</span><span class="p">,</span>
    <span class="n">input_y</span><span class="p">,</span>
    <span class="n">input_id</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># </span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">))</span>
    <span class="c1"># </span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># validation</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># 1.</span>
    <span class="k">for</span> <span class="n">nfold</span> <span class="ow">in</span> <span class="n">list_nfold</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>

        <span class="n">idx_tr</span><span class="p">,</span> <span class="n">idx_va</span> <span class="o">=</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_va</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_va</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="n">x_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_va</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># </span>
        <span class="n">fname_tf</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">EXP_MODEL</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"model_tf_fold</span><span class="si">{</span><span class="n">nfold</span><span class="si">}</span><span class="s2">.weights.h5"</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">):</span>
            <span class="c1"># if mode_train == 'train':</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"trainning start!"</span><span class="p">)</span>
            <span class="n">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x_tr</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span>
                <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">),</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">ModelCheckpoint</span><span class="p">(</span>
                        <span class="n">filepath</span><span class="o">=</span><span class="n">fname_tf</span><span class="p">,</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">EarlyStopping</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">min_delta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">ReduceLROnPlateau</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">],</span>
                <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"model load."</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">)</span>

        <span class="c1"># valid</span>
        <span class="n">y_tr_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_tr</span><span class="p">)</span>
        <span class="n">y_va_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_va</span><span class="p">)</span>

        <span class="n">metric_tr</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">y_tr_pred</span><span class="p">)</span>
        <span class="n">metric_va</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_va</span><span class="p">,</span> <span class="n">y_va_pred</span><span class="p">)</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">nfold</span><span class="p">,</span> <span class="n">metric_tr</span><span class="p">,</span> <span class="n">metric_va</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[auc] tr:</span><span class="si">{</span><span class="n">metric_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, va:</span><span class="si">{</span><span class="n">metric_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="c1"># oof</span>
        <span class="n">train_oof</span><span class="p">[</span><span class="n">idx_va</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_va_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
    <span class="c1"># metric</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">oof</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"[oof]</span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span><span class="w"> </span><span class="n">train_oof</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># oof</span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">input_id</span><span class="p">,</span>
            <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"pred"</span><span class="p">:</span> <span class="n">train_oof</span><span class="p">}),</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># stdout  stderr </span>
    <span class="n">stdout_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDOUT"</span><span class="p">)</span>
    <span class="n">stderr_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDERR"</span><span class="p">)</span>

    <span class="n">sys_stdout_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span>
    <span class="n">sys_stderr_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span>

    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stdout_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stderr_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># </span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">sys_stdout_backup</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">sys_stderr_backup</span>

    <span class="k">return</span> <span class="n">train_oof</span><span class="p">,</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=406062c1-9c5c-4c45-bdc1-a48eca2bd414">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">train_oof_tf</span><span class="p">,</span> <span class="n">metrics_tf</span> <span class="o">=</span> <span class="n">train_tf</span><span class="p">(</span>
    <span class="n">df_std_tf</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">id_train</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_22"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)                    </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">       Param # </span>

 input_layer_22 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">30</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_88 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">15,872</span> 

 batch_normalization_66           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">2,048</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_66 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_89 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                   <span style="color: #00af00; text-decoration-color: #00af00">131,328</span> 

 batch_normalization_67           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">1,024</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_67 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_90 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">32,896</span> 

 batch_normalization_68           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">512</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_68 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_91 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">129</span> 

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">183,809</span> (718.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">182,017</span> (711.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,792</span> (7.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>-------------------- 0 --------------------
20240928 12:32:05
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6515 - roc_auc: 0.7670
Epoch 1: val_loss improved from inf to 0.51656, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6491 - roc_auc: 0.7691 - val_loss: 0.5166 - val_roc_auc: 0.8267 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4161 - roc_auc: 0.8965
Epoch 2: val_loss improved from 0.51656 to 0.48955, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.4159 - roc_auc: 0.8966 - val_loss: 0.4896 - val_roc_auc: 0.8260 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3453 - roc_auc: 0.9286
Epoch 3: val_loss improved from 0.48955 to 0.47736, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3446 - roc_auc: 0.9286 - val_loss: 0.4774 - val_roc_auc: 0.8479 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3002 - roc_auc: 0.9459
Epoch 4: val_loss did not improve from 0.47736
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3002 - roc_auc: 0.9457 - val_loss: 0.4974 - val_roc_auc: 0.8430 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2548 - roc_auc: 0.9616
Epoch 5: val_loss did not improve from 0.47736
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2550 - roc_auc: 0.9614 - val_loss: 0.5302 - val_roc_auc: 0.8590 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2535 - roc_auc: 0.9626
Epoch 6: val_loss did not improve from 0.47736
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2528 - roc_auc: 0.9625 - val_loss: 0.5756 - val_roc_auc: 0.8607 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1934 - roc_auc: 0.9771
Epoch 7: val_loss did not improve from 0.47736
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1938 - roc_auc: 0.9773 - val_loss: 0.5964 - val_roc_auc: 0.8577 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1769 - roc_auc: 0.9820
Epoch 8: val_loss did not improve from 0.47736

Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1786 - roc_auc: 0.9815 - val_loss: 0.6425 - val_roc_auc: 0.8476 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1798 - roc_auc: 0.9810
Epoch 9: val_loss improved from 0.47736 to 0.46995, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1794 - roc_auc: 0.9811 - val_loss: 0.4700 - val_roc_auc: 0.8947 - learning_rate: 1.0000e-04
Epoch 10/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1385 - roc_auc: 0.9918
Epoch 10: val_loss improved from 0.46995 to 0.42807, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1386 - roc_auc: 0.9917 - val_loss: 0.4281 - val_roc_auc: 0.9071 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1532 - roc_auc: 0.9897
Epoch 11: val_loss did not improve from 0.42807
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1500 - roc_auc: 0.9900 - val_loss: 0.4330 - val_roc_auc: 0.9048 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1291 - roc_auc: 0.9940
Epoch 12: val_loss improved from 0.42807 to 0.42250, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1278 - roc_auc: 0.9940 - val_loss: 0.4225 - val_roc_auc: 0.9094 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1215 - roc_auc: 0.9938
Epoch 13: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1201 - roc_auc: 0.9939 - val_loss: 0.4278 - val_roc_auc: 0.9088 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1128 - roc_auc: 0.9963
Epoch 14: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1112 - roc_auc: 0.9964 - val_loss: 0.4279 - val_roc_auc: 0.9079 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1027 - roc_auc: 0.9974
Epoch 15: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1016 - roc_auc: 0.9973 - val_loss: 0.4353 - val_roc_auc: 0.9074 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0973 - roc_auc: 0.9979
Epoch 16: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0970 - roc_auc: 0.9979 - val_loss: 0.4354 - val_roc_auc: 0.9081 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1085 - roc_auc: 0.9963
Epoch 17: val_loss did not improve from 0.42250

Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1078 - roc_auc: 0.9963 - val_loss: 0.4338 - val_roc_auc: 0.9058 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0957 - roc_auc: 0.9974
Epoch 18: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0945 - roc_auc: 0.9975 - val_loss: 0.4378 - val_roc_auc: 0.9057 - learning_rate: 1.0000e-05
Epoch 19/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0807 - roc_auc: 0.9988
Epoch 19: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0805 - roc_auc: 0.9988 - val_loss: 0.4428 - val_roc_auc: 0.9055 - learning_rate: 1.0000e-05
Epoch 20/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0831 - roc_auc: 0.9986
Epoch 20: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0833 - roc_auc: 0.9985 - val_loss: 0.4456 - val_roc_auc: 0.9042 - learning_rate: 1.0000e-05
Epoch 21/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0939 - roc_auc: 0.9976
Epoch 21: val_loss did not improve from 0.42250
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0927 - roc_auc: 0.9975 - val_loss: 0.4438 - val_roc_auc: 0.9048 - learning_rate: 1.0000e-05
Epoch 22/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0962 - roc_auc: 0.9973
Epoch 22: val_loss did not improve from 0.42250

Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0961 - roc_auc: 0.9973 - val_loss: 0.4427 - val_roc_auc: 0.9053 - learning_rate: 1.0000e-05
Epoch 22: early stopping
Restoring model weights from the end of the best epoch: 12.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9628, va:0.9086
-------------------- 1 --------------------
20240928 12:32:17
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.6218 - roc_auc: 0.7937
Epoch 1: val_loss improved from inf to 0.51631, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6202 - roc_auc: 0.7949 - val_loss: 0.5163 - val_roc_auc: 0.8968 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4498 - roc_auc: 0.8749
Epoch 2: val_loss improved from 0.51631 to 0.45308, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.4486 - roc_auc: 0.8760 - val_loss: 0.4531 - val_roc_auc: 0.9158 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3303 - roc_auc: 0.9342
Epoch 3: val_loss improved from 0.45308 to 0.39576, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3312 - roc_auc: 0.9336 - val_loss: 0.3958 - val_roc_auc: 0.9179 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3073 - roc_auc: 0.9430
Epoch 4: val_loss improved from 0.39576 to 0.36833, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3074 - roc_auc: 0.9429 - val_loss: 0.3683 - val_roc_auc: 0.9249 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2523 - roc_auc: 0.9657
Epoch 5: val_loss did not improve from 0.36833
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2522 - roc_auc: 0.9656 - val_loss: 0.3744 - val_roc_auc: 0.9142 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2268 - roc_auc: 0.9729
Epoch 6: val_loss improved from 0.36833 to 0.35120, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2281 - roc_auc: 0.9719 - val_loss: 0.3512 - val_roc_auc: 0.9279 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2131 - roc_auc: 0.9756
Epoch 7: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2131 - roc_auc: 0.9756 - val_loss: 0.4577 - val_roc_auc: 0.8938 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1896 - roc_auc: 0.9800
Epoch 8: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1902 - roc_auc: 0.9797 - val_loss: 0.3968 - val_roc_auc: 0.9196 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1833 - roc_auc: 0.9811
Epoch 9: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1831 - roc_auc: 0.9811 - val_loss: 0.5642 - val_roc_auc: 0.8784 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1548 - roc_auc: 0.9878
Epoch 10: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1545 - roc_auc: 0.9878 - val_loss: 0.4862 - val_roc_auc: 0.8998 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1613 - roc_auc: 0.9860
Epoch 11: val_loss did not improve from 0.35120

Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1595 - roc_auc: 0.9862 - val_loss: 0.5397 - val_roc_auc: 0.8907 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1259 - roc_auc: 0.9910
Epoch 12: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1252 - roc_auc: 0.9912 - val_loss: 0.4741 - val_roc_auc: 0.9048 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1201 - roc_auc: 0.9918
Epoch 13: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1181 - roc_auc: 0.9922 - val_loss: 0.4336 - val_roc_auc: 0.9188 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1032 - roc_auc: 0.9953
Epoch 14: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1025 - roc_auc: 0.9954 - val_loss: 0.4445 - val_roc_auc: 0.9173 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1026 - roc_auc: 0.9946
Epoch 15: val_loss did not improve from 0.35120
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1009 - roc_auc: 0.9948 - val_loss: 0.4218 - val_roc_auc: 0.9212 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0896 - roc_auc: 0.9964
Epoch 16: val_loss did not improve from 0.35120

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0887 - roc_auc: 0.9965 - val_loss: 0.4214 - val_roc_auc: 0.9227 - learning_rate: 1.0000e-04
Epoch 16: early stopping
Restoring model weights from the end of the best epoch: 6.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9587, va:0.9291
-------------------- 2 --------------------
20240928 12:32:26
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.7089 - roc_auc: 0.7522
Epoch 1: val_loss improved from inf to 0.49653, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.7029 - roc_auc: 0.7552 - val_loss: 0.4965 - val_roc_auc: 0.9196 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4432 - roc_auc: 0.8824
Epoch 2: val_loss improved from 0.49653 to 0.45180, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4423 - roc_auc: 0.8827 - val_loss: 0.4518 - val_roc_auc: 0.9094 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3912 - roc_auc: 0.9040
Epoch 3: val_loss improved from 0.45180 to 0.42462, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3890 - roc_auc: 0.9051 - val_loss: 0.4246 - val_roc_auc: 0.9062 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3242 - roc_auc: 0.9351
Epoch 4: val_loss improved from 0.42462 to 0.40963, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3243 - roc_auc: 0.9351 - val_loss: 0.4096 - val_roc_auc: 0.9085 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3076 - roc_auc: 0.9421
Epoch 5: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3071 - roc_auc: 0.9423 - val_loss: 0.4137 - val_roc_auc: 0.9094 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2465 - roc_auc: 0.9646
Epoch 6: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2448 - roc_auc: 0.9653 - val_loss: 0.4554 - val_roc_auc: 0.8966 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2369 - roc_auc: 0.9684
Epoch 7: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2349 - roc_auc: 0.9688 - val_loss: 0.4256 - val_roc_auc: 0.9064 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2122 - roc_auc: 0.9736
Epoch 8: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2116 - roc_auc: 0.9737 - val_loss: 0.5334 - val_roc_auc: 0.8935 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2207 - roc_auc: 0.9725
Epoch 9: val_loss did not improve from 0.40963

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2182 - roc_auc: 0.9730 - val_loss: 0.4645 - val_roc_auc: 0.9048 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1637 - roc_auc: 0.9858
Epoch 10: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1616 - roc_auc: 0.9863 - val_loss: 0.4359 - val_roc_auc: 0.9130 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1548 - roc_auc: 0.9871
Epoch 11: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1536 - roc_auc: 0.9873 - val_loss: 0.4455 - val_roc_auc: 0.9113 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1393 - roc_auc: 0.9904
Epoch 12: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1389 - roc_auc: 0.9905 - val_loss: 0.4524 - val_roc_auc: 0.9106 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1290 - roc_auc: 0.9922
Epoch 13: val_loss did not improve from 0.40963
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1290 - roc_auc: 0.9922 - val_loss: 0.4645 - val_roc_auc: 0.9095 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1449 - roc_auc: 0.9893
Epoch 14: val_loss did not improve from 0.40963

Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1419 - roc_auc: 0.9897 - val_loss: 0.4546 - val_roc_auc: 0.9123 - learning_rate: 1.0000e-04
Epoch 14: early stopping
Restoring model weights from the end of the best epoch: 4.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9434, va:0.9087
-------------------- 3 --------------------
20240928 12:32:35
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.6359 - roc_auc: 0.8016
Epoch 1: val_loss improved from inf to 0.51293, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6272 - roc_auc: 0.8059 - val_loss: 0.5129 - val_roc_auc: 0.8732 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4384 - roc_auc: 0.8960
Epoch 2: val_loss improved from 0.51293 to 0.49814, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.4375 - roc_auc: 0.8956 - val_loss: 0.4981 - val_roc_auc: 0.8483 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3433 - roc_auc: 0.9309
Epoch 3: val_loss improved from 0.49814 to 0.42901, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3446 - roc_auc: 0.9303 - val_loss: 0.4290 - val_roc_auc: 0.8969 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2739 - roc_auc: 0.9559
Epoch 4: val_loss did not improve from 0.42901
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2749 - roc_auc: 0.9556 - val_loss: 0.4499 - val_roc_auc: 0.8836 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2482 - roc_auc: 0.9641
Epoch 5: val_loss improved from 0.42901 to 0.42498, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2497 - roc_auc: 0.9635 - val_loss: 0.4250 - val_roc_auc: 0.9039 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2323 - roc_auc: 0.9696
Epoch 6: val_loss improved from 0.42498 to 0.40484, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.2332 - roc_auc: 0.9693 - val_loss: 0.4048 - val_roc_auc: 0.9175 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2066 - roc_auc: 0.9753
Epoch 7: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2075 - roc_auc: 0.9751 - val_loss: 0.4709 - val_roc_auc: 0.9022 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1858 - roc_auc: 0.9802
Epoch 8: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1862 - roc_auc: 0.9802 - val_loss: 0.4736 - val_roc_auc: 0.9020 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1485 - roc_auc: 0.9887
Epoch 9: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1502 - roc_auc: 0.9883 - val_loss: 0.5383 - val_roc_auc: 0.8796 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1673 - roc_auc: 0.9830
Epoch 10: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1680 - roc_auc: 0.9829 - val_loss: 0.5355 - val_roc_auc: 0.8854 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1491 - roc_auc: 0.9844
Epoch 11: val_loss did not improve from 0.40484

Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1506 - roc_auc: 0.9842 - val_loss: 0.5329 - val_roc_auc: 0.8927 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1359 - roc_auc: 0.9892
Epoch 12: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1360 - roc_auc: 0.9892 - val_loss: 0.5107 - val_roc_auc: 0.8982 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1132 - roc_auc: 0.9934
Epoch 13: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1134 - roc_auc: 0.9934 - val_loss: 0.5093 - val_roc_auc: 0.8961 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0982 - roc_auc: 0.9946
Epoch 14: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0989 - roc_auc: 0.9945 - val_loss: 0.5040 - val_roc_auc: 0.8969 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0922 - roc_auc: 0.9965
Epoch 15: val_loss did not improve from 0.40484
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0928 - roc_auc: 0.9964 - val_loss: 0.5006 - val_roc_auc: 0.8983 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0880 - roc_auc: 0.9969
Epoch 16: val_loss did not improve from 0.40484

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0891 - roc_auc: 0.9968 - val_loss: 0.5072 - val_roc_auc: 0.9002 - learning_rate: 1.0000e-04
Epoch 16: early stopping
Restoring model weights from the end of the best epoch: 6.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9480, va:0.9169
-------------------- 4 --------------------
20240928 12:32:45
(680, 30) (170, 30)
trainning start!
Epoch 1/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.7063 - roc_auc: 0.7524
Epoch 1: val_loss improved from inf to 0.48943, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - loss: 0.7054 - roc_auc: 0.7530 - val_loss: 0.4894 - val_roc_auc: 0.8921 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4858 - roc_auc: 0.8572
Epoch 2: val_loss improved from 0.48943 to 0.42668, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.4837 - roc_auc: 0.8583 - val_loss: 0.4267 - val_roc_auc: 0.9103 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3986 - roc_auc: 0.9013
Epoch 3: val_loss improved from 0.42668 to 0.38645, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.3968 - roc_auc: 0.9020 - val_loss: 0.3865 - val_roc_auc: 0.9126 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3123 - roc_auc: 0.9405
Epoch 4: val_loss improved from 0.38645 to 0.37058, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.3129 - roc_auc: 0.9402 - val_loss: 0.3706 - val_roc_auc: 0.9154 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3104 - roc_auc: 0.9388
Epoch 5: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3088 - roc_auc: 0.9395 - val_loss: 0.4364 - val_roc_auc: 0.9040 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2850 - roc_auc: 0.9497
Epoch 6: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2830 - roc_auc: 0.9504 - val_loss: 0.4413 - val_roc_auc: 0.9037 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2288 - roc_auc: 0.9690
Epoch 7: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2292 - roc_auc: 0.9689 - val_loss: 0.4543 - val_roc_auc: 0.9108 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2449 - roc_auc: 0.9661
Epoch 8: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2448 - roc_auc: 0.9659 - val_loss: 0.4485 - val_roc_auc: 0.9114 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2232 - roc_auc: 0.9684
Epoch 9: val_loss did not improve from 0.37058

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2216 - roc_auc: 0.9691 - val_loss: 0.5201 - val_roc_auc: 0.8954 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1618 - roc_auc: 0.9885
Epoch 10: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1613 - roc_auc: 0.9886 - val_loss: 0.4759 - val_roc_auc: 0.9120 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1555 - roc_auc: 0.9888
Epoch 11: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1548 - roc_auc: 0.9889 - val_loss: 0.4483 - val_roc_auc: 0.9149 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1393 - roc_auc: 0.9922
Epoch 12: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1390 - roc_auc: 0.9922 - val_loss: 0.4492 - val_roc_auc: 0.9150 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1454 - roc_auc: 0.9913
Epoch 13: val_loss did not improve from 0.37058
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1437 - roc_auc: 0.9915 - val_loss: 0.4537 - val_roc_auc: 0.9156 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1150 - roc_auc: 0.9956
Epoch 14: val_loss did not improve from 0.37058

Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1156 - roc_auc: 0.9954 - val_loss: 0.4499 - val_roc_auc: 0.9168 - learning_rate: 1.0000e-04
Epoch 14: early stopping
Restoring model weights from the end of the best epoch: 4.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9485, va:0.9153
---------- result ----------
[[0.         0.96279469 0.90863158]
 [1.         0.95867146 0.92912281]
 [2.         0.94342829 0.9087346 ]
 [3.         0.94799222 0.9168533 ]
 [4.         0.9484565  0.91531355]]
[cv] tr:0.9523+-0.0072,         va:0.9157+-0.0072
[oof]0.9121
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=7f0db183-341e-4b5f-8411-53d90c9c2ffc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># Load Data</span>
<span class="c1"># =================================================</span>
<span class="c1"># train</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>train.csv
Memory usage of dataframe is 0.07 MB
Memory usage after optimization is: 0.02 MB
Decreased by 70.33%
(850, 11)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Age</th>
<th>Gender</th>
<th>T_Bil</th>
<th>D_Bil</th>
<th>ALP</th>
<th>ALT_GPT</th>
<th>AST_GOT</th>
<th>TP</th>
<th>Alb</th>
<th>AG_ratio</th>
<th>disease</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>59</td>
<td>Male</td>
<td>0.7871</td>
<td>0.1505</td>
<td>220.1250</td>
<td>13.4688</td>
<td>21.7344</td>
<td>6.8164</td>
<td>3.1113</td>
<td>1.0068</td>
<td>0</td>
</tr>
<tr>
<th>1</th>
<td>69</td>
<td>Male</td>
<td>1.0039</td>
<td>0.1957</td>
<td>221.2500</td>
<td>51.0312</td>
<td>64.7500</td>
<td>6.8906</td>
<td>3.0508</td>
<td>0.7515</td>
<td>1</td>
</tr>
<tr>
<th>2</th>
<td>65</td>
<td>Male</td>
<td>0.6572</td>
<td>0.0813</td>
<td>320.7500</td>
<td>12.6250</td>
<td>30.6094</td>
<td>5.9492</td>
<td>2.4883</td>
<td>0.7749</td>
<td>0</td>
</tr>
<tr>
<th>3</th>
<td>65</td>
<td>Male</td>
<td>0.9067</td>
<td>0.2142</td>
<td>369.2500</td>
<td>34.3438</td>
<td>54.5000</td>
<td>6.9688</td>
<td>3.6133</td>
<td>0.9883</td>
<td>1</td>
</tr>
<tr>
<th>4</th>
<td>22</td>
<td>Female</td>
<td>1.7354</td>
<td>0.1978</td>
<td>222.7500</td>
<td>20.5781</td>
<td>170.0000</td>
<td>5.8359</td>
<td>3.0684</td>
<td>1.0264</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=4000f658-1d17-4ec3-b826-2f7a032ff2e5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">df_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">df_train</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>(850, 11)</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 850 entries, 0 to 849
Data columns (total 11 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   Age       850 non-null    int8   
 1   Gender    850 non-null    object 
 2   T_Bil     850 non-null    float16
 3   D_Bil     850 non-null    float16
 4   ALP       850 non-null    float16
 5   ALT_GPT   850 non-null    float16
 6   AST_GOT   850 non-null    float16
 7   TP        850 non-null    float16
 8   Alb       850 non-null    float16
 9   AG_ratio  850 non-null    float16
 10  disease   850 non-null    int8   
dtypes: float16(8), int8(2), object(1)
memory usage: 21.7+ KB
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=f73a7fae-5299-411d-928b-4174d82d5276">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># ==========================================================</span>
<span class="c1"># 4</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">data_pre01</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>

<span class="c1"># 8</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">data_pre02</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>: (850, 15)
()
========================================
: (850, 23)
()
========================================
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=66c0c5a9-ccf2-42b2-b2fb-d0631efb3b17">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># </span>
<span class="c1"># =================================================</span>
<span class="n">set_file</span> <span class="o">=</span> <span class="n">df_train</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">set_file</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="n">target_columns</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">set_file</span><span class="p">[</span><span class="n">target_columns</span><span class="p">]</span>
<span class="n">id_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">set_file</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">id_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>(850, 22) (850,) (850, 1)
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=f95cdd42-33a4-4a5c-985c-510a13bfa6ea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># category</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data_pre00</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

<span class="c1"># =&gt;PCA</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data_pre03</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>category
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 850 entries, 0 to 849
Data columns (total 22 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   Age           850 non-null    int8   
 1   Gender        850 non-null    uint8  
 2   T_Bil         850 non-null    float16
 3   D_Bil         850 non-null    float16
 4   ALP           850 non-null    float16
 5   ALT_GPT       850 non-null    float16
 6   AST_GOT       850 non-null    float16
 7   TP            850 non-null    float16
 8   Alb           850 non-null    float16
 9   AG_ratio      850 non-null    float16
 10  D/T_ex2       850 non-null    float16
 11  AST/ALT_ex2   850 non-null    float16
 12  TP/AST_ex2    850 non-null    float16
 13  Globulin_ex2  850 non-null    float16
 14  TB/ALT_ex3    850 non-null    float16
 15  TB/AST_ex3    850 non-null    float16
 16  TB/ALP_ex3    850 non-null    float16
 17  Alb/ALT_ex3   850 non-null    float16
 18  TP/ALT_ex3    850 non-null    float16
 19  ALP/AST_ex3   850 non-null    float16
 20  ALP/ALT_ex3   850 non-null    float16
 21  TB/Alb_ex3    850 non-null    float16
dtypes: float16(20), int8(1), uint8(1)
memory usage: 35.0 KB
[0.27742435 0.45382265 0.55682828 0.64615944 0.72397803 0.78474861
 0.83761075 0.88547489]
: (850, 22)
: (850, 30)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0f1fb3e3-8904-429e-acdb-7fe51b637b8e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 850 entries, 0 to 849
Data columns (total 30 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   Age           850 non-null    int8   
 1   Gender        850 non-null    uint8  
 2   T_Bil         850 non-null    float16
 3   D_Bil         850 non-null    float16
 4   ALP           850 non-null    float16
 5   ALT_GPT       850 non-null    float16
 6   AST_GOT       850 non-null    float16
 7   TP            850 non-null    float16
 8   Alb           850 non-null    float16
 9   AG_ratio      850 non-null    float16
 10  D/T_ex2       850 non-null    float16
 11  AST/ALT_ex2   850 non-null    float16
 12  TP/AST_ex2    850 non-null    float16
 13  Globulin_ex2  850 non-null    float16
 14  TB/ALT_ex3    850 non-null    float16
 15  TB/AST_ex3    850 non-null    float16
 16  TB/ALP_ex3    850 non-null    float16
 17  Alb/ALT_ex3   850 non-null    float16
 18  TP/ALT_ex3    850 non-null    float16
 19  ALP/AST_ex3   850 non-null    float16
 20  ALP/ALT_ex3   850 non-null    float16
 21  TB/Alb_ex3    850 non-null    float16
 22  pc01          850 non-null    float64
 23  pc02          850 non-null    float64
 24  pc03          850 non-null    float64
 25  pc04          850 non-null    float64
 26  pc05          850 non-null    float64
 27  pc06          850 non-null    float64
 28  pc07          850 non-null    float64
 29  pc08          850 non-null    float64
dtypes: float16(20), float64(8), int8(1), uint8(1)
memory usage: 88.1 KB
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=26f5cade-ceeb-4c25-a75b-f5f0d9118a24">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># x_train_tf = x_train</span>
<span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">"pc03"</span><span class="p">,</span><span class="s2">"pc04"</span><span class="p">,</span><span class="s2">"pc05"</span><span class="p">,</span><span class="s2">"pc06"</span><span class="p">,</span><span class="s2">"pc07"</span><span class="p">,</span><span class="s2">"pc08"</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#[    ["T_Bil", "pc01", "AST_GOT", "AG_ratio", "ALP", "Alb/ALT_ex3", "TP", "D_Bil"]]</span>
<span class="n">x_train_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 24)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1ccb565b-652d-4d1a-aea9-f0eb0de426e6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">input_num</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">24</span><span class="p">,))</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">input_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"sigmoid"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">input_num</span><span class="p">,</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="s2">"Adam"</span><span class="p">,</span>
        <span class="n">loss</span><span class="o">=</span><span class="s2">"binary_crossentropy"</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">AUC</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"roc_auc"</span><span class="p">)],</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>


<span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"PYTHONHASHSEED"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">session_conf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
        <span class="n">intra_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inter_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span>
        <span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">(),</span> <span class="n">config</span><span class="o">=</span><span class="n">session_conf</span>
    <span class="p">)</span>
    <span class="c1"># tf.compat.v1.keras.backend.set_session(sess)</span>
    <span class="n">K</span><span class="o">.</span><span class="n">set_session</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_tf</span><span class="p">(</span>
    <span class="n">input_x</span><span class="p">,</span>
    <span class="n">input_y</span><span class="p">,</span>
    <span class="n">input_id</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># </span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">))</span>
    <span class="c1"># </span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># validation</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># 1.</span>
    <span class="k">for</span> <span class="n">nfold</span> <span class="ow">in</span> <span class="n">list_nfold</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>

        <span class="n">idx_tr</span><span class="p">,</span> <span class="n">idx_va</span> <span class="o">=</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_va</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_va</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="n">x_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_va</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># </span>
        <span class="n">fname_tf</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">EXP_MODEL</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"model_tf_fold</span><span class="si">{</span><span class="n">nfold</span><span class="si">}</span><span class="s2">.weights.h5"</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">):</span>
            <span class="c1"># if mode_train == 'train':</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"trainning start!"</span><span class="p">)</span>
            <span class="n">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x_tr</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span>
                <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">),</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">ModelCheckpoint</span><span class="p">(</span>
                        <span class="n">filepath</span><span class="o">=</span><span class="n">fname_tf</span><span class="p">,</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">EarlyStopping</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">min_delta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">ReduceLROnPlateau</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">],</span>
                <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"model load."</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">)</span>

        <span class="c1"># valid</span>
        <span class="n">y_tr_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_tr</span><span class="p">)</span>
        <span class="n">y_va_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_va</span><span class="p">)</span>

        <span class="n">metric_tr</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">y_tr_pred</span><span class="p">)</span>
        <span class="n">metric_va</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_va</span><span class="p">,</span> <span class="n">y_va_pred</span><span class="p">)</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">nfold</span><span class="p">,</span> <span class="n">metric_tr</span><span class="p">,</span> <span class="n">metric_va</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[auc] tr:</span><span class="si">{</span><span class="n">metric_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, va:</span><span class="si">{</span><span class="n">metric_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="c1"># oof</span>
        <span class="n">train_oof</span><span class="p">[</span><span class="n">idx_va</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_va_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
    <span class="c1"># metric</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">oof</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"[oof]</span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span><span class="w"> </span><span class="n">train_oof</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># oof</span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">input_id</span><span class="p">,</span>
            <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"pred"</span><span class="p">:</span> <span class="n">train_oof</span><span class="p">}),</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># stdout  stderr </span>
    <span class="n">stdout_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDOUT"</span><span class="p">)</span>
    <span class="n">stderr_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDERR"</span><span class="p">)</span>

    <span class="n">sys_stdout_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span>
    <span class="n">sys_stderr_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span>

    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stdout_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stderr_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># </span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">sys_stdout_backup</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">sys_stderr_backup</span>

    <span class="k">return</span> <span class="n">train_oof</span><span class="p">,</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=88f1cee8-9d83-4205-b991-b4ce2d10a706">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">std_tf</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_tf</span><span class="p">)</span>
<span class="n">df_std_tf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std_tf</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">std_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 24)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=635ab359-9fe7-459f-917c-f43e75ff8179">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">train_oof_tf</span><span class="p">,</span> <span class="n">metrics_tf</span> <span class="o">=</span> <span class="n">train_tf</span><span class="p">(</span>
    <span class="n">df_std_tf</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">id_train</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_30"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)                    </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">       Param # </span>

 input_layer_30 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_120 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">384</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">9,600</span> 

 batch_normalization_90           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">384</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">1,536</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_90 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">384</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_121 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">192</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">73,920</span> 

 batch_normalization_91           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">192</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">768</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_91 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">192</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_122 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">96</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">18,528</span> 

 batch_normalization_92           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">96</span>)                        <span style="color: #00af00; text-decoration-color: #00af00">384</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_92 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">96</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_123 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">97</span> 

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">104,833</span> (409.50 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">103,489</span> (404.25 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,344</span> (5.25 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>-------------------- 0 --------------------
20240928 12:40:57
(680, 24) (170, 24)
trainning start!
Epoch 1/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.5967 - roc_auc: 0.7972
Epoch 1: val_loss improved from inf to 0.55088, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.5943 - roc_auc: 0.7988 - val_loss: 0.5509 - val_roc_auc: 0.8298 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4575 - roc_auc: 0.8794
Epoch 2: val_loss improved from 0.55088 to 0.49709, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4544 - roc_auc: 0.8803 - val_loss: 0.4971 - val_roc_auc: 0.8529 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3653 - roc_auc: 0.9195
Epoch 3: val_loss improved from 0.49709 to 0.44307, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3651 - roc_auc: 0.9195 - val_loss: 0.4431 - val_roc_auc: 0.8694 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3344 - roc_auc: 0.9319
Epoch 4: val_loss did not improve from 0.44307
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3338 - roc_auc: 0.9321 - val_loss: 0.4745 - val_roc_auc: 0.8435 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2858 - roc_auc: 0.9523
Epoch 5: val_loss did not improve from 0.44307
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2848 - roc_auc: 0.9526 - val_loss: 0.4801 - val_roc_auc: 0.8631 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2778 - roc_auc: 0.9537
Epoch 6: val_loss did not improve from 0.44307
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2778 - roc_auc: 0.9536 - val_loss: 0.4672 - val_roc_auc: 0.8840 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2366 - roc_auc: 0.9670
Epoch 7: val_loss did not improve from 0.44307
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2367 - roc_auc: 0.9669 - val_loss: 0.5134 - val_roc_auc: 0.8701 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2320 - roc_auc: 0.9682
Epoch 8: val_loss did not improve from 0.44307

Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2312 - roc_auc: 0.9685 - val_loss: 0.5849 - val_roc_auc: 0.8715 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2173 - roc_auc: 0.9721
Epoch 9: val_loss did not improve from 0.44307
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2160 - roc_auc: 0.9720 - val_loss: 0.4801 - val_roc_auc: 0.8843 - learning_rate: 1.0000e-04
Epoch 10/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2106 - roc_auc: 0.9744
Epoch 10: val_loss did not improve from 0.44307
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2077 - roc_auc: 0.9750 - val_loss: 0.4487 - val_roc_auc: 0.8918 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1779 - roc_auc: 0.9832
Epoch 11: val_loss did not improve from 0.44307
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1767 - roc_auc: 0.9834 - val_loss: 0.4494 - val_roc_auc: 0.8918 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1491 - roc_auc: 0.9907
Epoch 12: val_loss did not improve from 0.44307
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1484 - roc_auc: 0.9907 - val_loss: 0.4507 - val_roc_auc: 0.8919 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1721 - roc_auc: 0.9856
Epoch 13: val_loss did not improve from 0.44307

Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1708 - roc_auc: 0.9858 - val_loss: 0.4455 - val_roc_auc: 0.8961 - learning_rate: 1.0000e-04
Epoch 13: early stopping
Restoring model weights from the end of the best epoch: 3.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9218, va:0.8705
-------------------- 1 --------------------
20240928 12:41:05
(680, 24) (170, 24)
trainning start!
Epoch 1/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6633 - roc_auc: 0.7495
Epoch 1: val_loss improved from inf to 0.57789, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6465 - roc_auc: 0.7624 - val_loss: 0.5779 - val_roc_auc: 0.8606 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4066 - roc_auc: 0.8963
Epoch 2: val_loss improved from 0.57789 to 0.51399, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4072 - roc_auc: 0.8965 - val_loss: 0.5140 - val_roc_auc: 0.8737 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4082 - roc_auc: 0.8952
Epoch 3: val_loss improved from 0.51399 to 0.40497, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4025 - roc_auc: 0.8982 - val_loss: 0.4050 - val_roc_auc: 0.9197 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3419 - roc_auc: 0.9247
Epoch 4: val_loss improved from 0.40497 to 0.35982, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.3416 - roc_auc: 0.9250 - val_loss: 0.3598 - val_roc_auc: 0.9344 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2821 - roc_auc: 0.9531
Epoch 5: val_loss improved from 0.35982 to 0.29650, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.2821 - roc_auc: 0.9531 - val_loss: 0.2965 - val_roc_auc: 0.9513 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2539 - roc_auc: 0.9635
Epoch 6: val_loss did not improve from 0.29650
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2541 - roc_auc: 0.9631 - val_loss: 0.3140 - val_roc_auc: 0.9421 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2361 - roc_auc: 0.9686
Epoch 7: val_loss did not improve from 0.29650
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2359 - roc_auc: 0.9686 - val_loss: 0.3307 - val_roc_auc: 0.9381 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2349 - roc_auc: 0.9670
Epoch 8: val_loss did not improve from 0.29650
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2333 - roc_auc: 0.9674 - val_loss: 0.3677 - val_roc_auc: 0.9213 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2427 - roc_auc: 0.9654
Epoch 9: val_loss did not improve from 0.29650
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2407 - roc_auc: 0.9659 - val_loss: 0.3387 - val_roc_auc: 0.9291 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1844 - roc_auc: 0.9828
Epoch 10: val_loss did not improve from 0.29650

Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1840 - roc_auc: 0.9828 - val_loss: 0.3269 - val_roc_auc: 0.9375 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1984 - roc_auc: 0.9774
Epoch 11: val_loss did not improve from 0.29650
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1950 - roc_auc: 0.9781 - val_loss: 0.3191 - val_roc_auc: 0.9410 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1534 - roc_auc: 0.9883
Epoch 12: val_loss did not improve from 0.29650
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1533 - roc_auc: 0.9884 - val_loss: 0.3199 - val_roc_auc: 0.9417 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1482 - roc_auc: 0.9898
Epoch 13: val_loss did not improve from 0.29650
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1454 - roc_auc: 0.9902 - val_loss: 0.3278 - val_roc_auc: 0.9394 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1238 - roc_auc: 0.9941
Epoch 14: val_loss did not improve from 0.29650
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1236 - roc_auc: 0.9941 - val_loss: 0.3302 - val_roc_auc: 0.9375 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1227 - roc_auc: 0.9935
Epoch 15: val_loss did not improve from 0.29650

Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1221 - roc_auc: 0.9936 - val_loss: 0.3374 - val_roc_auc: 0.9350 - learning_rate: 1.0000e-04
Epoch 15: early stopping
Restoring model weights from the end of the best epoch: 5.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9507, va:0.9510
-------------------- 2 --------------------
20240928 12:41:14
(680, 24) (170, 24)
trainning start!
Epoch 1/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6284 - roc_auc: 0.7574
Epoch 1: val_loss improved from inf to 0.54885, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6264 - roc_auc: 0.7591 - val_loss: 0.5488 - val_roc_auc: 0.9036 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4321 - roc_auc: 0.8848
Epoch 2: val_loss improved from 0.54885 to 0.44923, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4292 - roc_auc: 0.8862 - val_loss: 0.4492 - val_roc_auc: 0.9282 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3959 - roc_auc: 0.9032
Epoch 3: val_loss improved from 0.44923 to 0.41011, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3956 - roc_auc: 0.9034 - val_loss: 0.4101 - val_roc_auc: 0.9179 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3548 - roc_auc: 0.9222
Epoch 4: val_loss improved from 0.41011 to 0.37945, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3540 - roc_auc: 0.9225 - val_loss: 0.3794 - val_roc_auc: 0.9116 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2850 - roc_auc: 0.9508
Epoch 5: val_loss did not improve from 0.37945
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2853 - roc_auc: 0.9507 - val_loss: 0.3859 - val_roc_auc: 0.9123 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3041 - roc_auc: 0.9427
Epoch 6: val_loss did not improve from 0.37945
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3034 - roc_auc: 0.9430 - val_loss: 0.4293 - val_roc_auc: 0.9078 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2714 - roc_auc: 0.9544
Epoch 7: val_loss did not improve from 0.37945
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2707 - roc_auc: 0.9546 - val_loss: 0.4049 - val_roc_auc: 0.9165 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2195 - roc_auc: 0.9734
Epoch 8: val_loss did not improve from 0.37945
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2194 - roc_auc: 0.9734 - val_loss: 0.4296 - val_roc_auc: 0.9075 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2056 - roc_auc: 0.9752
Epoch 9: val_loss did not improve from 0.37945

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2057 - roc_auc: 0.9752 - val_loss: 0.5124 - val_roc_auc: 0.8980 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1712 - roc_auc: 0.9847
Epoch 10: val_loss did not improve from 0.37945
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1709 - roc_auc: 0.9848 - val_loss: 0.4857 - val_roc_auc: 0.9037 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1601 - roc_auc: 0.9862
Epoch 11: val_loss did not improve from 0.37945
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1599 - roc_auc: 0.9863 - val_loss: 0.4776 - val_roc_auc: 0.9085 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1662 - roc_auc: 0.9855
Epoch 12: val_loss did not improve from 0.37945
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1660 - roc_auc: 0.9856 - val_loss: 0.4682 - val_roc_auc: 0.9092 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1674 - roc_auc: 0.9842
Epoch 13: val_loss did not improve from 0.37945
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1667 - roc_auc: 0.9844 - val_loss: 0.4763 - val_roc_auc: 0.9099 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1445 - roc_auc: 0.9893
Epoch 14: val_loss did not improve from 0.37945

Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1445 - roc_auc: 0.9893 - val_loss: 0.4674 - val_roc_auc: 0.9104 - learning_rate: 1.0000e-04
Epoch 14: early stopping
Restoring model weights from the end of the best epoch: 4.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9451, va:0.9118
-------------------- 3 --------------------
20240928 12:41:22
(680, 24) (170, 24)
trainning start!
Epoch 1/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.5890 - roc_auc: 0.8046
Epoch 1: val_loss improved from inf to 0.58652, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.5881 - roc_auc: 0.8056 - val_loss: 0.5865 - val_roc_auc: 0.8660 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3802 - roc_auc: 0.9099
Epoch 2: val_loss improved from 0.58652 to 0.47332, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3822 - roc_auc: 0.9090 - val_loss: 0.4733 - val_roc_auc: 0.9061 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3470 - roc_auc: 0.9270
Epoch 3: val_loss improved from 0.47332 to 0.43005, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3516 - roc_auc: 0.9249 - val_loss: 0.4300 - val_roc_auc: 0.8993 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2957 - roc_auc: 0.9477
Epoch 4: val_loss improved from 0.43005 to 0.39662, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2965 - roc_auc: 0.9474 - val_loss: 0.3966 - val_roc_auc: 0.9085 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2821 - roc_auc: 0.9539
Epoch 5: val_loss improved from 0.39662 to 0.38687, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2893 - roc_auc: 0.9513 - val_loss: 0.3869 - val_roc_auc: 0.9075 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2544 - roc_auc: 0.9609
Epoch 6: val_loss did not improve from 0.38687
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2572 - roc_auc: 0.9602 - val_loss: 0.4146 - val_roc_auc: 0.8969 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2210 - roc_auc: 0.9722
Epoch 7: val_loss did not improve from 0.38687
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2217 - roc_auc: 0.9720 - val_loss: 0.4067 - val_roc_auc: 0.9026 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2205 - roc_auc: 0.9717
Epoch 8: val_loss did not improve from 0.38687
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2225 - roc_auc: 0.9712 - val_loss: 0.4487 - val_roc_auc: 0.8956 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">70/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1780 - roc_auc: 0.9827
Epoch 9: val_loss did not improve from 0.38687
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1820 - roc_auc: 0.9817 - val_loss: 0.4458 - val_roc_auc: 0.8947 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1714 - roc_auc: 0.9831
Epoch 10: val_loss did not improve from 0.38687

Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1768 - roc_auc: 0.9818 - val_loss: 0.4634 - val_roc_auc: 0.8956 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1619 - roc_auc: 0.9847
Epoch 11: val_loss did not improve from 0.38687
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1639 - roc_auc: 0.9843 - val_loss: 0.4406 - val_roc_auc: 0.8998 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1536 - roc_auc: 0.9876
Epoch 12: val_loss did not improve from 0.38687
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1544 - roc_auc: 0.9872 - val_loss: 0.4444 - val_roc_auc: 0.8975 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1500 - roc_auc: 0.9873
Epoch 13: val_loss did not improve from 0.38687
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1509 - roc_auc: 0.9871 - val_loss: 0.4319 - val_roc_auc: 0.9002 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1326 - roc_auc: 0.9902
Epoch 14: val_loss did not improve from 0.38687
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1328 - roc_auc: 0.9901 - val_loss: 0.4446 - val_roc_auc: 0.9011 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1035 - roc_auc: 0.9953
Epoch 15: val_loss did not improve from 0.38687

Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1067 - roc_auc: 0.9948 - val_loss: 0.4364 - val_roc_auc: 0.9035 - learning_rate: 1.0000e-04
Epoch 15: early stopping
Restoring model weights from the end of the best epoch: 5.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9522, va:0.9071
-------------------- 4 --------------------
20240928 12:41:30
(680, 24) (170, 24)
trainning start!
Epoch 1/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6256 - roc_auc: 0.7751
Epoch 1: val_loss improved from inf to 0.49756, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6214 - roc_auc: 0.7782 - val_loss: 0.4976 - val_roc_auc: 0.9097 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4437 - roc_auc: 0.8781
Epoch 2: val_loss improved from 0.49756 to 0.44200, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4433 - roc_auc: 0.8783 - val_loss: 0.4420 - val_roc_auc: 0.9101 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4020 - roc_auc: 0.9005
Epoch 3: val_loss improved from 0.44200 to 0.38938, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4014 - roc_auc: 0.9007 - val_loss: 0.3894 - val_roc_auc: 0.9137 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3544 - roc_auc: 0.9210
Epoch 4: val_loss improved from 0.38938 to 0.37329, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3550 - roc_auc: 0.9208 - val_loss: 0.3733 - val_roc_auc: 0.9078 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2768 - roc_auc: 0.9548
Epoch 5: val_loss improved from 0.37329 to 0.36083, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2775 - roc_auc: 0.9545 - val_loss: 0.3608 - val_roc_auc: 0.9211 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3056 - roc_auc: 0.9442
Epoch 6: val_loss did not improve from 0.36083
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3056 - roc_auc: 0.9442 - val_loss: 0.3833 - val_roc_auc: 0.9169 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2838 - roc_auc: 0.9534
Epoch 7: val_loss did not improve from 0.36083
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2796 - roc_auc: 0.9543 - val_loss: 0.4289 - val_roc_auc: 0.9033 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2717 - roc_auc: 0.9584
Epoch 8: val_loss did not improve from 0.36083
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2667 - roc_auc: 0.9595 - val_loss: 0.3960 - val_roc_auc: 0.9162 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2149 - roc_auc: 0.9704
Epoch 9: val_loss did not improve from 0.36083
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2153 - roc_auc: 0.9707 - val_loss: 0.4119 - val_roc_auc: 0.9173 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2041 - roc_auc: 0.9762
Epoch 10: val_loss did not improve from 0.36083

Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2038 - roc_auc: 0.9763 - val_loss: 0.4868 - val_roc_auc: 0.9017 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1897 - roc_auc: 0.9815
Epoch 11: val_loss did not improve from 0.36083
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1894 - roc_auc: 0.9815 - val_loss: 0.4498 - val_roc_auc: 0.9050 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1704 - roc_auc: 0.9858
Epoch 12: val_loss did not improve from 0.36083
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1687 - roc_auc: 0.9862 - val_loss: 0.4404 - val_roc_auc: 0.9090 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1464 - roc_auc: 0.9907
Epoch 13: val_loss did not improve from 0.36083
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1462 - roc_auc: 0.9906 - val_loss: 0.4365 - val_roc_auc: 0.9092 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1321 - roc_auc: 0.9942
Epoch 14: val_loss did not improve from 0.36083
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1326 - roc_auc: 0.9940 - val_loss: 0.4327 - val_roc_auc: 0.9117 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1375 - roc_auc: 0.9921
Epoch 15: val_loss did not improve from 0.36083

Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1374 - roc_auc: 0.9921 - val_loss: 0.4230 - val_roc_auc: 0.9142 - learning_rate: 1.0000e-04
Epoch 15: early stopping
Restoring model weights from the end of the best epoch: 5.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9568, va:0.9215
---------- result ----------
[[0.         0.92182507 0.87045614]
 [1.         0.9507139  0.95101754]
 [2.         0.94513648 0.91181411]
 [3.         0.95219699 0.90705487]
 [4.         0.95684852 0.92147256]]
[cv] tr:0.9453+-0.0123,         va:0.9124+-0.0123
[oof]0.9134
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=02838fd8-069d-4ccc-a89e-1f422b346339">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">input_num</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,))</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">input_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"sigmoid"</span><span class="p">)(</span><span class="n">x_num</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">input_num</span><span class="p">,</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="s2">"Adam"</span><span class="p">,</span>
        <span class="n">loss</span><span class="o">=</span><span class="s2">"binary_crossentropy"</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">AUC</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"roc_auc"</span><span class="p">)],</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>


<span class="c1"># tf</span>
<span class="k">def</span> <span class="nf">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"PYTHONHASHSEED"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">session_conf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
        <span class="n">intra_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inter_op_parallelism_threads</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span>
        <span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">(),</span> <span class="n">config</span><span class="o">=</span><span class="n">session_conf</span>
    <span class="p">)</span>
    <span class="c1"># tf.compat.v1.keras.backend.set_session(sess)</span>
    <span class="n">K</span><span class="o">.</span><span class="n">set_session</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_tf</span><span class="p">(</span>
    <span class="n">input_x</span><span class="p">,</span>
    <span class="n">input_y</span><span class="p">,</span>
    <span class="n">input_id</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># </span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">))</span>
    <span class="c1"># </span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># validation</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># 1.</span>
    <span class="k">for</span> <span class="n">nfold</span> <span class="ow">in</span> <span class="n">list_nfold</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>

        <span class="n">idx_tr</span><span class="p">,</span> <span class="n">idx_va</span> <span class="o">=</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_x</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_va</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">input_y</span><span class="p">[</span><span class="n">idx_va</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="n">x_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_va</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># </span>
        <span class="n">fname_tf</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">EXP_MODEL</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"model_tf_fold</span><span class="si">{</span><span class="n">nfold</span><span class="si">}</span><span class="s2">.weights.h5"</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">):</span>
            <span class="c1"># if mode_train == 'train':</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"trainning start!"</span><span class="p">)</span>
            <span class="n">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x_tr</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span>
                <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">),</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">ModelCheckpoint</span><span class="p">(</span>
                        <span class="n">filepath</span><span class="o">=</span><span class="n">fname_tf</span><span class="p">,</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">EarlyStopping</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">min_delta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">ReduceLROnPlateau</span><span class="p">(</span>
                        <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_loss"</span><span class="p">,</span>
                        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
                        <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">],</span>
                <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"model load."</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">fname_tf</span><span class="p">)</span>

        <span class="c1"># valid</span>
        <span class="n">y_tr_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_tr</span><span class="p">)</span>
        <span class="n">y_va_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_va</span><span class="p">)</span>

        <span class="n">metric_tr</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">y_tr_pred</span><span class="p">)</span>
        <span class="n">metric_va</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_va</span><span class="p">,</span> <span class="n">y_va_pred</span><span class="p">)</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">nfold</span><span class="p">,</span> <span class="n">metric_tr</span><span class="p">,</span> <span class="n">metric_va</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[auc] tr:</span><span class="si">{</span><span class="n">metric_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, va:</span><span class="si">{</span><span class="n">metric_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="c1"># oof</span>
        <span class="n">train_oof</span><span class="p">[</span><span class="n">idx_va</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_va_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
    <span class="c1"># metric</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">oof</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"[oof]</span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span><span class="w"> </span><span class="n">train_oof</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># oof</span>
    <span class="n">train_oof</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">input_id</span><span class="p">,</span>
            <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"pred"</span><span class="p">:</span> <span class="n">train_oof</span><span class="p">}),</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># stdout  stderr </span>
    <span class="n">stdout_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDOUT"</span><span class="p">)</span>
    <span class="n">stderr_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"STDERR"</span><span class="p">)</span>

    <span class="n">sys_stdout_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span>
    <span class="n">sys_stderr_backup</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span>

    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stdout_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">StreamToLogger</span><span class="p">(</span><span class="n">stderr_logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">"result"</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[cv] tr:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">        va:</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">+-</span><span class="si">{</span><span class="n">metrics</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">oof</span><span class="p">)</span>

    <span class="c1"># </span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">sys_stdout_backup</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">sys_stderr_backup</span>

    <span class="k">return</span> <span class="n">train_oof</span><span class="p">,</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=64ed69d3-4287-4a76-a84b-5a15bcc9573c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># x_train_tf = x_train</span>
<span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[[</span><span class="s2">"pc01"</span><span class="p">,</span><span class="s2">"pc02"</span><span class="p">,</span><span class="s2">"pc03"</span><span class="p">,</span><span class="s2">"pc04"</span><span class="p">,</span><span class="s2">"pc05"</span><span class="p">,</span><span class="s2">"pc06"</span><span class="p">,</span><span class="s2">"pc07"</span><span class="p">,</span><span class="s2">"pc08"</span><span class="p">]]</span>

<span class="c1">#[    ["T_Bil", "pc01", "AST_GOT", "AG_ratio", "ALP", "Alb/ALT_ex3", "TP", "D_Bil"]]</span>
<span class="n">x_train_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 8)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=10ad0603-3c15-4bc3-b704-f6c0329a377f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">df_std_tf</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Age</th>
<th>Gender</th>
<th>T_Bil</th>
<th>D_Bil</th>
<th>ALP</th>
<th>ALT_GPT</th>
<th>AST_GOT</th>
<th>TP</th>
<th>Alb</th>
<th>AG_ratio</th>
<th>...</th>
<th>TB/ALT_ex3</th>
<th>TB/AST_ex3</th>
<th>TB/ALP_ex3</th>
<th>Alb/ALT_ex3</th>
<th>TP/ALT_ex3</th>
<th>ALP/AST_ex3</th>
<th>ALP/ALT_ex3</th>
<th>TB/Alb_ex3</th>
<th>pc01</th>
<th>pc02</th>
</tr>
</thead>
<tbody>
<tr>
<th>count</th>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>...</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
</tr>
<tr>
<th>mean</th>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>...</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
</tr>
<tr>
<th>std</th>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>...</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
</tr>
<tr>
<th>min</th>
<td>-2.2264</td>
<td>-2.1779</td>
<td>-0.4182</td>
<td>-0.3647</td>
<td>-0.5442</td>
<td>-0.2609</td>
<td>-0.4519</td>
<td>-2.5861</td>
<td>-2.3869</td>
<td>-2.2793</td>
<td>...</td>
<td>-0.5534</td>
<td>-0.6688</td>
<td>-0.6125</td>
<td>-2.0823</td>
<td>-2.1019</td>
<td>-1.6281</td>
<td>-1.3395</td>
<td>-0.4513</td>
<td>-1.8143</td>
<td>-2.9570</td>
</tr>
<tr>
<th>25%</th>
<td>-0.8885</td>
<td>0.4592</td>
<td>-0.3382</td>
<td>-0.2945</td>
<td>-0.2907</td>
<td>-0.1943</td>
<td>-0.3614</td>
<td>-0.3593</td>
<td>-0.6768</td>
<td>-0.6151</td>
<td>...</td>
<td>-0.2763</td>
<td>-0.3542</td>
<td>-0.2998</td>
<td>-0.6604</td>
<td>-0.6297</td>
<td>-0.7787</td>
<td>-0.4513</td>
<td>-0.3485</td>
<td>-0.6500</td>
<td>-0.5127</td>
</tr>
<tr>
<th>50%</th>
<td>0.0845</td>
<td>0.4592</td>
<td>-0.3160</td>
<td>-0.2632</td>
<td>-0.2601</td>
<td>-0.1737</td>
<td>-0.3086</td>
<td>-0.1408</td>
<td>0.1852</td>
<td>0.2664</td>
<td>...</td>
<td>-0.1887</td>
<td>-0.1047</td>
<td>-0.2679</td>
<td>0.0014</td>
<td>-0.0092</td>
<td>0.0531</td>
<td>-0.1298</td>
<td>-0.3098</td>
<td>-0.2113</td>
<td>-0.0081</td>
</tr>
<tr>
<th>75%</th>
<td>0.9359</td>
<td>0.4592</td>
<td>-0.1680</td>
<td>-0.1730</td>
<td>-0.2133</td>
<td>-0.1297</td>
<td>-0.0413</td>
<td>0.5984</td>
<td>0.3492</td>
<td>0.6319</td>
<td>...</td>
<td>-0.0771</td>
<td>0.0012</td>
<td>-0.1900</td>
<td>0.4939</td>
<td>0.5168</td>
<td>0.4293</td>
<td>0.2086</td>
<td>-0.1508</td>
<td>0.3929</td>
<td>0.3605</td>
</tr>
<tr>
<th>max</th>
<td>1.9089</td>
<td>0.4592</td>
<td>8.7662</td>
<td>10.8930</td>
<td>9.1694</td>
<td>9.4687</td>
<td>6.8459</td>
<td>2.0350</td>
<td>2.6770</td>
<td>2.9799</td>
<td>...</td>
<td>14.8083</td>
<td>16.5953</td>
<td>10.5536</td>
<td>6.9078</td>
<td>7.0256</td>
<td>6.3045</td>
<td>9.3804</td>
<td>8.7931</td>
<td>5.9864</td>
<td>9.8850</td>
</tr>
</tbody>
</table>
<p>8 rows  24 columns</p>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=a365a126-b1ff-4e5d-90a1-f405d5776e78">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">train_oof_tf</span><span class="p">,</span> <span class="n">metrics_tf</span> <span class="o">=</span> <span class="n">train_tf</span><span class="p">(</span>
    <span class="n">x_train_tf</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">id_train</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_38"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)                    </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">       Param # </span>

 input_layer_38 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">8</span>)                           <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_152 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">2,304</span> 

 batch_normalization_114          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">1,024</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_114 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_153 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">32,896</span> 

 batch_normalization_115          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">512</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_115 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_154 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                      <span style="color: #00af00; text-decoration-color: #00af00">8,256</span> 

 batch_normalization_116          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                        <span style="color: #00af00; text-decoration-color: #00af00">256</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_116 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_155 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">65</span> 

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">45,313</span> (177.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">44,417</span> (173.50 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">896</span> (3.50 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>-------------------- 0 --------------------
20240928 12:51:51
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.6304 - roc_auc: 0.7573
Epoch 1: val_loss improved from inf to 0.51597, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6175 - roc_auc: 0.7670 - val_loss: 0.5160 - val_roc_auc: 0.8477 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4909 - roc_auc: 0.8676
Epoch 2: val_loss improved from 0.51597 to 0.47824, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4860 - roc_auc: 0.8676 - val_loss: 0.4782 - val_roc_auc: 0.8599 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4340 - roc_auc: 0.8894
Epoch 3: val_loss improved from 0.47824 to 0.46223, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4327 - roc_auc: 0.8897 - val_loss: 0.4622 - val_roc_auc: 0.8565 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3958 - roc_auc: 0.9033
Epoch 4: val_loss improved from 0.46223 to 0.45798, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3949 - roc_auc: 0.9036 - val_loss: 0.4580 - val_roc_auc: 0.8620 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3465 - roc_auc: 0.9266
Epoch 5: val_loss did not improve from 0.45798
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3489 - roc_auc: 0.9251 - val_loss: 0.4637 - val_roc_auc: 0.8663 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3588 - roc_auc: 0.9228
Epoch 6: val_loss did not improve from 0.45798
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3567 - roc_auc: 0.9230 - val_loss: 0.4647 - val_roc_auc: 0.8665 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3006 - roc_auc: 0.9456
Epoch 7: val_loss did not improve from 0.45798
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3045 - roc_auc: 0.9437 - val_loss: 0.4786 - val_roc_auc: 0.8671 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3391 - roc_auc: 0.9320
Epoch 8: val_loss did not improve from 0.45798
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3377 - roc_auc: 0.9323 - val_loss: 0.4669 - val_roc_auc: 0.8738 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3071 - roc_auc: 0.9423
Epoch 9: val_loss did not improve from 0.45798

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3046 - roc_auc: 0.9433 - val_loss: 0.4587 - val_roc_auc: 0.8792 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2853 - roc_auc: 0.9509
Epoch 10: val_loss improved from 0.45798 to 0.43295, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2837 - roc_auc: 0.9513 - val_loss: 0.4330 - val_roc_auc: 0.8820 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2508 - roc_auc: 0.9638
Epoch 11: val_loss improved from 0.43295 to 0.42508, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2504 - roc_auc: 0.9638 - val_loss: 0.4251 - val_roc_auc: 0.8836 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2735 - roc_auc: 0.9569
Epoch 12: val_loss improved from 0.42508 to 0.42452, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2726 - roc_auc: 0.9570 - val_loss: 0.4245 - val_roc_auc: 0.8846 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2406 - roc_auc: 0.9692
Epoch 13: val_loss did not improve from 0.42452
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2405 - roc_auc: 0.9692 - val_loss: 0.4274 - val_roc_auc: 0.8836 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">62/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2398 - roc_auc: 0.9694
Epoch 14: val_loss did not improve from 0.42452
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2384 - roc_auc: 0.9695 - val_loss: 0.4271 - val_roc_auc: 0.8862 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2267 - roc_auc: 0.9745
Epoch 15: val_loss did not improve from 0.42452
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2286 - roc_auc: 0.9730 - val_loss: 0.4260 - val_roc_auc: 0.8857 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2495 - roc_auc: 0.9644
Epoch 16: val_loss improved from 0.42452 to 0.42313, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2496 - roc_auc: 0.9642 - val_loss: 0.4231 - val_roc_auc: 0.8856 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2384 - roc_auc: 0.9700
Epoch 17: val_loss did not improve from 0.42313
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2371 - roc_auc: 0.9699 - val_loss: 0.4291 - val_roc_auc: 0.8836 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2354 - roc_auc: 0.9696
Epoch 18: val_loss did not improve from 0.42313
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2341 - roc_auc: 0.9698 - val_loss: 0.4286 - val_roc_auc: 0.8844 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2111 - roc_auc: 0.9787
Epoch 19: val_loss did not improve from 0.42313
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2111 - roc_auc: 0.9787 - val_loss: 0.4243 - val_roc_auc: 0.8865 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2237 - roc_auc: 0.9727
Epoch 20: val_loss improved from 0.42313 to 0.42073, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2225 - roc_auc: 0.9727 - val_loss: 0.4207 - val_roc_auc: 0.8876 - learning_rate: 1.0000e-04
Epoch 21/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2217 - roc_auc: 0.9768
Epoch 21: val_loss did not improve from 0.42073
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2214 - roc_auc: 0.9763 - val_loss: 0.4229 - val_roc_auc: 0.8875 - learning_rate: 1.0000e-04
Epoch 22/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2283 - roc_auc: 0.9708
Epoch 22: val_loss did not improve from 0.42073
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2265 - roc_auc: 0.9712 - val_loss: 0.4221 - val_roc_auc: 0.8891 - learning_rate: 1.0000e-04
Epoch 23/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2132 - roc_auc: 0.9748
Epoch 23: val_loss improved from 0.42073 to 0.41787, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2135 - roc_auc: 0.9745 - val_loss: 0.4179 - val_roc_auc: 0.8874 - learning_rate: 1.0000e-04
Epoch 24/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1901 - roc_auc: 0.9815
Epoch 24: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1948 - roc_auc: 0.9801 - val_loss: 0.4224 - val_roc_auc: 0.8889 - learning_rate: 1.0000e-04
Epoch 25/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1917 - roc_auc: 0.9830
Epoch 25: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1916 - roc_auc: 0.9829 - val_loss: 0.4221 - val_roc_auc: 0.8907 - learning_rate: 1.0000e-04
Epoch 26/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2017 - roc_auc: 0.9776
Epoch 26: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2019 - roc_auc: 0.9774 - val_loss: 0.4204 - val_roc_auc: 0.8903 - learning_rate: 1.0000e-04
Epoch 27/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2096 - roc_auc: 0.9768
Epoch 27: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2093 - roc_auc: 0.9768 - val_loss: 0.4205 - val_roc_auc: 0.8904 - learning_rate: 1.0000e-04
Epoch 28/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2032 - roc_auc: 0.9784
Epoch 28: val_loss did not improve from 0.41787

Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2018 - roc_auc: 0.9785 - val_loss: 0.4233 - val_roc_auc: 0.8900 - learning_rate: 1.0000e-04
Epoch 29/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1860 - roc_auc: 0.9848
Epoch 29: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1860 - roc_auc: 0.9844 - val_loss: 0.4242 - val_roc_auc: 0.8898 - learning_rate: 1.0000e-05
Epoch 30/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1921 - roc_auc: 0.9855
Epoch 30: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1918 - roc_auc: 0.9842 - val_loss: 0.4254 - val_roc_auc: 0.8898 - learning_rate: 1.0000e-05
Epoch 31/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1899 - roc_auc: 0.9816
Epoch 31: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1903 - roc_auc: 0.9814 - val_loss: 0.4271 - val_roc_auc: 0.8901 - learning_rate: 1.0000e-05
Epoch 32/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1842 - roc_auc: 0.9834
Epoch 32: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1861 - roc_auc: 0.9824 - val_loss: 0.4262 - val_roc_auc: 0.8898 - learning_rate: 1.0000e-05
Epoch 33/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1911 - roc_auc: 0.9812
Epoch 33: val_loss did not improve from 0.41787

Epoch 33: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1933 - roc_auc: 0.9804 - val_loss: 0.4261 - val_roc_auc: 0.8898 - learning_rate: 1.0000e-05
Epoch 33: early stopping
Restoring model weights from the end of the best epoch: 23.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9508, va:0.8879
-------------------- 1 --------------------
20240928 12:52:05
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.5614 - roc_auc: 0.8011
Epoch 1: val_loss improved from inf to 0.47624, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.5592 - roc_auc: 0.8039 - val_loss: 0.4762 - val_roc_auc: 0.9424 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4242 - roc_auc: 0.8844
Epoch 2: val_loss improved from 0.47624 to 0.42351, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4262 - roc_auc: 0.8842 - val_loss: 0.4235 - val_roc_auc: 0.9383 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4113 - roc_auc: 0.8934
Epoch 3: val_loss improved from 0.42351 to 0.39494, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4115 - roc_auc: 0.8934 - val_loss: 0.3949 - val_roc_auc: 0.9331 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3845 - roc_auc: 0.9073
Epoch 4: val_loss improved from 0.39494 to 0.37424, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3853 - roc_auc: 0.9070 - val_loss: 0.3742 - val_roc_auc: 0.9208 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3701 - roc_auc: 0.9113
Epoch 5: val_loss improved from 0.37424 to 0.34710, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3679 - roc_auc: 0.9126 - val_loss: 0.3471 - val_roc_auc: 0.9284 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3375 - roc_auc: 0.9298
Epoch 6: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3374 - roc_auc: 0.9299 - val_loss: 0.3842 - val_roc_auc: 0.9072 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3362 - roc_auc: 0.9318
Epoch 7: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3372 - roc_auc: 0.9311 - val_loss: 0.3682 - val_roc_auc: 0.9163 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3041 - roc_auc: 0.9449
Epoch 8: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3045 - roc_auc: 0.9446 - val_loss: 0.3661 - val_roc_auc: 0.9186 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3141 - roc_auc: 0.9407
Epoch 9: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3125 - roc_auc: 0.9411 - val_loss: 0.3977 - val_roc_auc: 0.9070 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2901 - roc_auc: 0.9503
Epoch 10: val_loss did not improve from 0.34710

Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2893 - roc_auc: 0.9504 - val_loss: 0.3658 - val_roc_auc: 0.9215 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2728 - roc_auc: 0.9571
Epoch 11: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2708 - roc_auc: 0.9576 - val_loss: 0.3559 - val_roc_auc: 0.9248 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2754 - roc_auc: 0.9537
Epoch 12: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2728 - roc_auc: 0.9547 - val_loss: 0.3570 - val_roc_auc: 0.9243 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2437 - roc_auc: 0.9666
Epoch 13: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2435 - roc_auc: 0.9666 - val_loss: 0.3498 - val_roc_auc: 0.9265 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2267 - roc_auc: 0.9712
Epoch 14: val_loss improved from 0.34710 to 0.34624, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2266 - roc_auc: 0.9713 - val_loss: 0.3462 - val_roc_auc: 0.9287 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2230 - roc_auc: 0.9747
Epoch 15: val_loss did not improve from 0.34624
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2231 - roc_auc: 0.9745 - val_loss: 0.3514 - val_roc_auc: 0.9256 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2271 - roc_auc: 0.9713
Epoch 16: val_loss did not improve from 0.34624
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2268 - roc_auc: 0.9714 - val_loss: 0.3506 - val_roc_auc: 0.9264 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2251 - roc_auc: 0.9745
Epoch 17: val_loss did not improve from 0.34624
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2243 - roc_auc: 0.9744 - val_loss: 0.3497 - val_roc_auc: 0.9276 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2155 - roc_auc: 0.9767
Epoch 18: val_loss improved from 0.34624 to 0.34076, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2158 - roc_auc: 0.9766 - val_loss: 0.3408 - val_roc_auc: 0.9305 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2120 - roc_auc: 0.9767
Epoch 19: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2116 - roc_auc: 0.9766 - val_loss: 0.3428 - val_roc_auc: 0.9286 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2138 - roc_auc: 0.9770
Epoch 20: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2139 - roc_auc: 0.9766 - val_loss: 0.3495 - val_roc_auc: 0.9274 - learning_rate: 1.0000e-04
Epoch 21/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2319 - roc_auc: 0.9704
Epoch 21: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2284 - roc_auc: 0.9714 - val_loss: 0.3427 - val_roc_auc: 0.9308 - learning_rate: 1.0000e-04
Epoch 22/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1853 - roc_auc: 0.9844
Epoch 22: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1859 - roc_auc: 0.9841 - val_loss: 0.3514 - val_roc_auc: 0.9277 - learning_rate: 1.0000e-04
Epoch 23/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1891 - roc_auc: 0.9817
Epoch 23: val_loss did not improve from 0.34076

Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1892 - roc_auc: 0.9816 - val_loss: 0.3424 - val_roc_auc: 0.9314 - learning_rate: 1.0000e-04
Epoch 24/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1918 - roc_auc: 0.9817
Epoch 24: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1921 - roc_auc: 0.9816 - val_loss: 0.3442 - val_roc_auc: 0.9310 - learning_rate: 1.0000e-05
Epoch 25/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1853 - roc_auc: 0.9845
Epoch 25: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1844 - roc_auc: 0.9844 - val_loss: 0.3445 - val_roc_auc: 0.9309 - learning_rate: 1.0000e-05
Epoch 26/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2164 - roc_auc: 0.9715
Epoch 26: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2158 - roc_auc: 0.9719 - val_loss: 0.3438 - val_roc_auc: 0.9305 - learning_rate: 1.0000e-05
Epoch 27/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1891 - roc_auc: 0.9845
Epoch 27: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1904 - roc_auc: 0.9839 - val_loss: 0.3460 - val_roc_auc: 0.9302 - learning_rate: 1.0000e-05
Epoch 28/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2059 - roc_auc: 0.9777
Epoch 28: val_loss did not improve from 0.34076

Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2061 - roc_auc: 0.9773 - val_loss: 0.3463 - val_roc_auc: 0.9295 - learning_rate: 1.0000e-05
Epoch 28: early stopping
Restoring model weights from the end of the best epoch: 18.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9500, va:0.9302
-------------------- 2 --------------------
20240928 12:52:19
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.6381 - roc_auc: 0.7450
Epoch 1: val_loss improved from inf to 0.53014, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6306 - roc_auc: 0.7514 - val_loss: 0.5301 - val_roc_auc: 0.9148 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4825 - roc_auc: 0.8572
Epoch 2: val_loss improved from 0.53014 to 0.46486, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4793 - roc_auc: 0.8588 - val_loss: 0.4649 - val_roc_auc: 0.9140 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4360 - roc_auc: 0.8836
Epoch 3: val_loss improved from 0.46486 to 0.42648, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4351 - roc_auc: 0.8839 - val_loss: 0.4265 - val_roc_auc: 0.9229 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4063 - roc_auc: 0.8963
Epoch 4: val_loss improved from 0.42648 to 0.40602, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4044 - roc_auc: 0.8972 - val_loss: 0.4060 - val_roc_auc: 0.9173 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3886 - roc_auc: 0.9092
Epoch 5: val_loss improved from 0.40602 to 0.39127, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3879 - roc_auc: 0.9092 - val_loss: 0.3913 - val_roc_auc: 0.9206 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3623 - roc_auc: 0.9208
Epoch 6: val_loss did not improve from 0.39127
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3627 - roc_auc: 0.9206 - val_loss: 0.4116 - val_roc_auc: 0.9150 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3574 - roc_auc: 0.9229
Epoch 7: val_loss improved from 0.39127 to 0.38583, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3565 - roc_auc: 0.9231 - val_loss: 0.3858 - val_roc_auc: 0.9267 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3422 - roc_auc: 0.9299
Epoch 8: val_loss did not improve from 0.38583
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3416 - roc_auc: 0.9301 - val_loss: 0.4072 - val_roc_auc: 0.9164 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2938 - roc_auc: 0.9501
Epoch 9: val_loss did not improve from 0.38583
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2934 - roc_auc: 0.9501 - val_loss: 0.4035 - val_roc_auc: 0.9169 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">70/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2783 - roc_auc: 0.9532
Epoch 10: val_loss did not improve from 0.38583
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2821 - roc_auc: 0.9517 - val_loss: 0.4069 - val_roc_auc: 0.9193 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2964 - roc_auc: 0.9481
Epoch 11: val_loss did not improve from 0.38583
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2955 - roc_auc: 0.9482 - val_loss: 0.3898 - val_roc_auc: 0.9205 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2739 - roc_auc: 0.9552
Epoch 12: val_loss improved from 0.38583 to 0.38346, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2728 - roc_auc: 0.9556 - val_loss: 0.3835 - val_roc_auc: 0.9270 - learning_rate: 0.0010
Epoch 13/10000
<span class="ansi-bold">70/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2963 - roc_auc: 0.9479
Epoch 13: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2920 - roc_auc: 0.9494 - val_loss: 0.4068 - val_roc_auc: 0.9164 - learning_rate: 0.0010
Epoch 14/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2465 - roc_auc: 0.9648
Epoch 14: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2469 - roc_auc: 0.9646 - val_loss: 0.3885 - val_roc_auc: 0.9300 - learning_rate: 0.0010
Epoch 15/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2666 - roc_auc: 0.9607
Epoch 15: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2655 - roc_auc: 0.9608 - val_loss: 0.4470 - val_roc_auc: 0.9169 - learning_rate: 0.0010
Epoch 16/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2065 - roc_auc: 0.9763
Epoch 16: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2089 - roc_auc: 0.9755 - val_loss: 0.4424 - val_roc_auc: 0.9213 - learning_rate: 0.0010
Epoch 17/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1949 - roc_auc: 0.9775
Epoch 17: val_loss did not improve from 0.38346

Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1975 - roc_auc: 0.9768 - val_loss: 0.4290 - val_roc_auc: 0.9236 - learning_rate: 0.0010
Epoch 18/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1736 - roc_auc: 0.9863
Epoch 18: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1748 - roc_auc: 0.9860 - val_loss: 0.4306 - val_roc_auc: 0.9253 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2117 - roc_auc: 0.9756
Epoch 19: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2093 - roc_auc: 0.9762 - val_loss: 0.4362 - val_roc_auc: 0.9208 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1741 - roc_auc: 0.9859
Epoch 20: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1733 - roc_auc: 0.9861 - val_loss: 0.4364 - val_roc_auc: 0.9246 - learning_rate: 1.0000e-04
Epoch 21/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1805 - roc_auc: 0.9832
Epoch 21: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1804 - roc_auc: 0.9833 - val_loss: 0.4418 - val_roc_auc: 0.9223 - learning_rate: 1.0000e-04
Epoch 22/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1566 - roc_auc: 0.9883
Epoch 22: val_loss did not improve from 0.38346

Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1566 - roc_auc: 0.9884 - val_loss: 0.4424 - val_roc_auc: 0.9235 - learning_rate: 1.0000e-04
Epoch 22: early stopping
Restoring model weights from the end of the best epoch: 12.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9405, va:0.9269
-------------------- 3 --------------------
20240928 12:52:29
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.6539 - roc_auc: 0.7554
Epoch 1: val_loss improved from inf to 0.51066, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.6489 - roc_auc: 0.7588 - val_loss: 0.5107 - val_roc_auc: 0.9158 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4150 - roc_auc: 0.8981
Epoch 2: val_loss improved from 0.51066 to 0.44970, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4176 - roc_auc: 0.8967 - val_loss: 0.4497 - val_roc_auc: 0.9128 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3801 - roc_auc: 0.9113
Epoch 3: val_loss improved from 0.44970 to 0.41891, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3841 - roc_auc: 0.9092 - val_loss: 0.4189 - val_roc_auc: 0.9141 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3615 - roc_auc: 0.9187
Epoch 4: val_loss improved from 0.41891 to 0.39361, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3627 - roc_auc: 0.9181 - val_loss: 0.3936 - val_roc_auc: 0.9123 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3414 - roc_auc: 0.9290
Epoch 5: val_loss improved from 0.39361 to 0.37704, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3429 - roc_auc: 0.9283 - val_loss: 0.3770 - val_roc_auc: 0.9167 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3144 - roc_auc: 0.9421
Epoch 6: val_loss improved from 0.37704 to 0.36579, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3165 - roc_auc: 0.9411 - val_loss: 0.3658 - val_roc_auc: 0.9205 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2918 - roc_auc: 0.9487
Epoch 7: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2950 - roc_auc: 0.9474 - val_loss: 0.3793 - val_roc_auc: 0.9103 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2902 - roc_auc: 0.9509
Epoch 8: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2910 - roc_auc: 0.9505 - val_loss: 0.3986 - val_roc_auc: 0.9076 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3044 - roc_auc: 0.9436
Epoch 9: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3065 - roc_auc: 0.9429 - val_loss: 0.3951 - val_roc_auc: 0.9106 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2890 - roc_auc: 0.9496
Epoch 10: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2907 - roc_auc: 0.9492 - val_loss: 0.4230 - val_roc_auc: 0.9038 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2601 - roc_auc: 0.9606
Epoch 11: val_loss did not improve from 0.36579

Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2615 - roc_auc: 0.9601 - val_loss: 0.4134 - val_roc_auc: 0.9080 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2326 - roc_auc: 0.9695
Epoch 12: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2356 - roc_auc: 0.9685 - val_loss: 0.4006 - val_roc_auc: 0.9119 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2247 - roc_auc: 0.9724
Epoch 13: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2271 - roc_auc: 0.9717 - val_loss: 0.3968 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2220 - roc_auc: 0.9729
Epoch 14: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2238 - roc_auc: 0.9723 - val_loss: 0.3939 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2154 - roc_auc: 0.9737
Epoch 15: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2169 - roc_auc: 0.9734 - val_loss: 0.3946 - val_roc_auc: 0.9141 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2152 - roc_auc: 0.9730
Epoch 16: val_loss did not improve from 0.36579

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2166 - roc_auc: 0.9726 - val_loss: 0.3955 - val_roc_auc: 0.9130 - learning_rate: 1.0000e-04
Epoch 16: early stopping
Restoring model weights from the end of the best epoch: 6.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9436, va:0.9206
-------------------- 4 --------------------
20240928 12:52:37
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.5668 - roc_auc: 0.8051
Epoch 1: val_loss improved from inf to 0.50199, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 8ms/step - loss: 0.5618 - roc_auc: 0.8088 - val_loss: 0.5020 - val_roc_auc: 0.9066 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4723 - roc_auc: 0.8642
Epoch 2: val_loss improved from 0.50199 to 0.47330, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4707 - roc_auc: 0.8648 - val_loss: 0.4733 - val_roc_auc: 0.8816 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4107 - roc_auc: 0.8950
Epoch 3: val_loss improved from 0.47330 to 0.43743, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4111 - roc_auc: 0.8945 - val_loss: 0.4374 - val_roc_auc: 0.8891 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3651 - roc_auc: 0.9163
Epoch 4: val_loss improved from 0.43743 to 0.39902, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3665 - roc_auc: 0.9154 - val_loss: 0.3990 - val_roc_auc: 0.9012 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3702 - roc_auc: 0.9144
Epoch 5: val_loss did not improve from 0.39902
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 4ms/step - loss: 0.3705 - roc_auc: 0.9142 - val_loss: 0.4005 - val_roc_auc: 0.9025 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3608 - roc_auc: 0.9199
Epoch 6: val_loss improved from 0.39902 to 0.38825, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3615 - roc_auc: 0.9194 - val_loss: 0.3882 - val_roc_auc: 0.9067 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3418 - roc_auc: 0.9263
Epoch 7: val_loss did not improve from 0.38825
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3417 - roc_auc: 0.9264 - val_loss: 0.3936 - val_roc_auc: 0.9086 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3285 - roc_auc: 0.9314
Epoch 8: val_loss did not improve from 0.38825
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3277 - roc_auc: 0.9318 - val_loss: 0.4068 - val_roc_auc: 0.9019 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3194 - roc_auc: 0.9403
Epoch 9: val_loss improved from 0.38825 to 0.38055, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3181 - roc_auc: 0.9402 - val_loss: 0.3805 - val_roc_auc: 0.9136 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2882 - roc_auc: 0.9504
Epoch 10: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2872 - roc_auc: 0.9507 - val_loss: 0.3975 - val_roc_auc: 0.9123 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2712 - roc_auc: 0.9558
Epoch 11: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2718 - roc_auc: 0.9555 - val_loss: 0.4149 - val_roc_auc: 0.9057 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2851 - roc_auc: 0.9531
Epoch 12: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2868 - roc_auc: 0.9523 - val_loss: 0.4054 - val_roc_auc: 0.9085 - learning_rate: 0.0010
Epoch 13/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2686 - roc_auc: 0.9573
Epoch 13: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2679 - roc_auc: 0.9575 - val_loss: 0.3900 - val_roc_auc: 0.9131 - learning_rate: 0.0010
Epoch 14/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2541 - roc_auc: 0.9636
Epoch 14: val_loss did not improve from 0.38055

Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2528 - roc_auc: 0.9638 - val_loss: 0.4159 - val_roc_auc: 0.9087 - learning_rate: 0.0010
Epoch 15/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2410 - roc_auc: 0.9668
Epoch 15: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2390 - roc_auc: 0.9673 - val_loss: 0.4008 - val_roc_auc: 0.9108 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2243 - roc_auc: 0.9689
Epoch 16: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2230 - roc_auc: 0.9697 - val_loss: 0.3957 - val_roc_auc: 0.9114 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2093 - roc_auc: 0.9762
Epoch 17: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2079 - roc_auc: 0.9767 - val_loss: 0.3864 - val_roc_auc: 0.9137 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2051 - roc_auc: 0.9759
Epoch 18: val_loss improved from 0.38055 to 0.37843, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2045 - roc_auc: 0.9760 - val_loss: 0.3784 - val_roc_auc: 0.9162 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1742 - roc_auc: 0.9891
Epoch 19: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1745 - roc_auc: 0.9889 - val_loss: 0.3805 - val_roc_auc: 0.9144 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2022 - roc_auc: 0.9793
Epoch 20: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2018 - roc_auc: 0.9793 - val_loss: 0.3786 - val_roc_auc: 0.9162 - learning_rate: 1.0000e-04
Epoch 21/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1792 - roc_auc: 0.9868
Epoch 21: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1795 - roc_auc: 0.9864 - val_loss: 0.3893 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-04
Epoch 22/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1977 - roc_auc: 0.9781
Epoch 22: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1971 - roc_auc: 0.9783 - val_loss: 0.3891 - val_roc_auc: 0.9138 - learning_rate: 1.0000e-04
Epoch 23/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1851 - roc_auc: 0.9842
Epoch 23: val_loss did not improve from 0.37843

Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1843 - roc_auc: 0.9842 - val_loss: 0.3887 - val_roc_auc: 0.9144 - learning_rate: 1.0000e-04
Epoch 24/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1906 - roc_auc: 0.9828
Epoch 24: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1898 - roc_auc: 0.9827 - val_loss: 0.3889 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-05
Epoch 25/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1976 - roc_auc: 0.9796
Epoch 25: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1972 - roc_auc: 0.9796 - val_loss: 0.3893 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-05
Epoch 26/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1761 - roc_auc: 0.9861
Epoch 26: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1764 - roc_auc: 0.9860 - val_loss: 0.3897 - val_roc_auc: 0.9134 - learning_rate: 1.0000e-05
Epoch 27/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1773 - roc_auc: 0.9834
Epoch 27: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1774 - roc_auc: 0.9834 - val_loss: 0.3897 - val_roc_auc: 0.9138 - learning_rate: 1.0000e-05
Epoch 28/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1891 - roc_auc: 0.9809
Epoch 28: val_loss did not improve from 0.37843

Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1870 - roc_auc: 0.9814 - val_loss: 0.3880 - val_roc_auc: 0.9138 - learning_rate: 1.0000e-05
Epoch 28: early stopping
Restoring model weights from the end of the best epoch: 18.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9505, va:0.9164
---------- result ----------
[[0.         0.95076643 0.88785965]
 [1.         0.94999606 0.93024561]
 [2.         0.94049371 0.92693169]
 [3.         0.94362977 0.9206327 ]
 [4.         0.9504888  0.91643337]]
[cv] tr:0.9471+-0.0042,         va:0.9164+-0.0042
[oof]0.9166
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2ff1dc00-bdf4-4b78-b7d9-8feec15de026">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">std_tf</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_tf</span><span class="p">)</span>
<span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">std_tf</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">x_train_tf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(850, 8)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=aaffd7d0-6b99-4dc9-810a-15fe9847f144">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">x_train_tf</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>pc01</th>
<th>pc02</th>
<th>pc03</th>
<th>pc04</th>
<th>pc05</th>
<th>pc06</th>
<th>pc07</th>
<th>pc08</th>
</tr>
</thead>
<tbody>
<tr>
<th>count</th>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
<td>850.0000</td>
</tr>
<tr>
<th>mean</th>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
<td>0.0000</td>
<td>-0.0000</td>
<td>-0.0000</td>
</tr>
<tr>
<th>std</th>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
<td>1.0006</td>
</tr>
<tr>
<th>min</th>
<td>-1.8143</td>
<td>-2.9570</td>
<td>-4.5507</td>
<td>-3.5893</td>
<td>-4.0526</td>
<td>-2.5738</td>
<td>-2.5645</td>
<td>-5.5798</td>
</tr>
<tr>
<th>25%</th>
<td>-0.6500</td>
<td>-0.5127</td>
<td>-0.5759</td>
<td>-0.5831</td>
<td>-0.4516</td>
<td>-0.6939</td>
<td>-0.4110</td>
<td>-0.6809</td>
</tr>
<tr>
<th>50%</th>
<td>-0.2113</td>
<td>-0.0081</td>
<td>-0.1470</td>
<td>-0.1222</td>
<td>-0.0978</td>
<td>0.0359</td>
<td>-0.0384</td>
<td>0.0674</td>
</tr>
<tr>
<th>75%</th>
<td>0.3929</td>
<td>0.3605</td>
<td>0.4822</td>
<td>0.5642</td>
<td>0.4400</td>
<td>0.4648</td>
<td>0.4170</td>
<td>0.8348</td>
</tr>
<tr>
<th>max</th>
<td>5.9864</td>
<td>9.8850</td>
<td>6.7522</td>
<td>3.7849</td>
<td>5.9316</td>
<td>4.3850</td>
<td>11.2114</td>
<td>1.8648</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c0f11336-5879-4719-9a47-bfe488a0f14e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># x_train_tf = x_train</span>
<span class="n">x_train_tf</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[[</span><span class="s2">"pc01"</span><span class="p">,</span> <span class="s2">"pc02"</span><span class="p">,</span> <span class="s2">"pc03"</span><span class="p">,</span> <span class="s2">"pc04"</span><span class="p">,</span> <span class="s2">"pc05"</span><span class="p">,</span> <span class="s2">"pc06"</span><span class="p">,</span> <span class="s2">"pc07"</span><span class="p">,</span> <span class="s2">"pc08"</span><span class="p">]]</span>

<span class="c1"># [    ["T_Bil", "pc01", "AST_GOT", "AG_ratio", "ALP", "Alb/ALT_ex3", "TP", "D_Bil"]]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1f33dfa5-8062-4c1f-94b1-b828252fd6f5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="c1"># tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">train_oof_tf</span><span class="p">,</span> <span class="n">metrics_tf</span> <span class="o">=</span> <span class="n">train_tf</span><span class="p">(</span>
    <span class="n">x_train_tf</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">id_train</span><span class="p">,</span>
    <span class="n">list_nfold</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_50"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)                    </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">       Param # </span>

 input_layer_50 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">8</span>)                           <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_200 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">2,304</span> 

 batch_normalization_150          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                     <span style="color: #00af00; text-decoration-color: #00af00">1,024</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_150 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_201 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">32,896</span> 

 batch_normalization_151          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">512</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_151 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_202 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                      <span style="color: #00af00; text-decoration-color: #00af00">8,256</span> 

 batch_normalization_152          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                        <span style="color: #00af00; text-decoration-color: #00af00">256</span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)                                                   

 dropout_152 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 dense_203 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">65</span> 

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">45,313</span> (177.00 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">44,417</span> (173.50 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">896</span> (3.50 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>-------------------- 0 --------------------
20240928 12:57:00
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.6212 - roc_auc: 0.7642
Epoch 1: val_loss improved from inf to 0.51597, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.6175 - roc_auc: 0.7670 - val_loss: 0.5160 - val_roc_auc: 0.8477 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4894 - roc_auc: 0.8668
Epoch 2: val_loss improved from 0.51597 to 0.47824, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4860 - roc_auc: 0.8676 - val_loss: 0.4782 - val_roc_auc: 0.8599 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4341 - roc_auc: 0.8895
Epoch 3: val_loss improved from 0.47824 to 0.46223, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4327 - roc_auc: 0.8897 - val_loss: 0.4622 - val_roc_auc: 0.8565 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3955 - roc_auc: 0.9033
Epoch 4: val_loss improved from 0.46223 to 0.45798, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 6ms/step - loss: 0.3949 - roc_auc: 0.9036 - val_loss: 0.4580 - val_roc_auc: 0.8620 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3475 - roc_auc: 0.9261
Epoch 5: val_loss did not improve from 0.45798
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3489 - roc_auc: 0.9251 - val_loss: 0.4637 - val_roc_auc: 0.8663 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">70/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3587 - roc_auc: 0.9227
Epoch 6: val_loss did not improve from 0.45798
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3567 - roc_auc: 0.9230 - val_loss: 0.4647 - val_roc_auc: 0.8665 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3041 - roc_auc: 0.9439
Epoch 7: val_loss did not improve from 0.45798
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3045 - roc_auc: 0.9437 - val_loss: 0.4786 - val_roc_auc: 0.8671 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3393 - roc_auc: 0.9324
Epoch 8: val_loss did not improve from 0.45798
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3377 - roc_auc: 0.9323 - val_loss: 0.4669 - val_roc_auc: 0.8738 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3087 - roc_auc: 0.9418
Epoch 9: val_loss did not improve from 0.45798

Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3046 - roc_auc: 0.9433 - val_loss: 0.4587 - val_roc_auc: 0.8792 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2853 - roc_auc: 0.9509
Epoch 10: val_loss improved from 0.45798 to 0.43295, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2837 - roc_auc: 0.9513 - val_loss: 0.4330 - val_roc_auc: 0.8820 - learning_rate: 1.0000e-04
Epoch 11/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2509 - roc_auc: 0.9638
Epoch 11: val_loss improved from 0.43295 to 0.42508, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2504 - roc_auc: 0.9638 - val_loss: 0.4251 - val_roc_auc: 0.8836 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2737 - roc_auc: 0.9569
Epoch 12: val_loss improved from 0.42508 to 0.42452, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2726 - roc_auc: 0.9570 - val_loss: 0.4245 - val_roc_auc: 0.8846 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2405 - roc_auc: 0.9692
Epoch 13: val_loss did not improve from 0.42452
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2405 - roc_auc: 0.9692 - val_loss: 0.4274 - val_roc_auc: 0.8836 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2398 - roc_auc: 0.9692
Epoch 14: val_loss did not improve from 0.42452
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2384 - roc_auc: 0.9695 - val_loss: 0.4271 - val_roc_auc: 0.8862 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2276 - roc_auc: 0.9739
Epoch 15: val_loss did not improve from 0.42452
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2286 - roc_auc: 0.9730 - val_loss: 0.4260 - val_roc_auc: 0.8857 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2466 - roc_auc: 0.9660
Epoch 16: val_loss improved from 0.42452 to 0.42313, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2496 - roc_auc: 0.9642 - val_loss: 0.4231 - val_roc_auc: 0.8856 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2386 - roc_auc: 0.9698
Epoch 17: val_loss did not improve from 0.42313
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2371 - roc_auc: 0.9699 - val_loss: 0.4291 - val_roc_auc: 0.8836 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2354 - roc_auc: 0.9697
Epoch 18: val_loss did not improve from 0.42313
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2341 - roc_auc: 0.9698 - val_loss: 0.4286 - val_roc_auc: 0.8844 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2112 - roc_auc: 0.9789
Epoch 19: val_loss did not improve from 0.42313
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2111 - roc_auc: 0.9787 - val_loss: 0.4243 - val_roc_auc: 0.8865 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2233 - roc_auc: 0.9730
Epoch 20: val_loss improved from 0.42313 to 0.42073, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2225 - roc_auc: 0.9727 - val_loss: 0.4207 - val_roc_auc: 0.8876 - learning_rate: 1.0000e-04
Epoch 21/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2216 - roc_auc: 0.9769
Epoch 21: val_loss did not improve from 0.42073
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2214 - roc_auc: 0.9763 - val_loss: 0.4229 - val_roc_auc: 0.8875 - learning_rate: 1.0000e-04
Epoch 22/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2287 - roc_auc: 0.9708
Epoch 22: val_loss did not improve from 0.42073
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2265 - roc_auc: 0.9712 - val_loss: 0.4221 - val_roc_auc: 0.8891 - learning_rate: 1.0000e-04
Epoch 23/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2112 - roc_auc: 0.9755
Epoch 23: val_loss improved from 0.42073 to 0.41787, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold0.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2135 - roc_auc: 0.9745 - val_loss: 0.4179 - val_roc_auc: 0.8874 - learning_rate: 1.0000e-04
Epoch 24/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1912 - roc_auc: 0.9812
Epoch 24: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1948 - roc_auc: 0.9801 - val_loss: 0.4224 - val_roc_auc: 0.8889 - learning_rate: 1.0000e-04
Epoch 25/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1916 - roc_auc: 0.9829
Epoch 25: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1916 - roc_auc: 0.9829 - val_loss: 0.4221 - val_roc_auc: 0.8907 - learning_rate: 1.0000e-04
Epoch 26/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2020 - roc_auc: 0.9774
Epoch 26: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2019 - roc_auc: 0.9774 - val_loss: 0.4204 - val_roc_auc: 0.8903 - learning_rate: 1.0000e-04
Epoch 27/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2101 - roc_auc: 0.9767
Epoch 27: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2093 - roc_auc: 0.9768 - val_loss: 0.4205 - val_roc_auc: 0.8904 - learning_rate: 1.0000e-04
Epoch 28/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2032 - roc_auc: 0.9784
Epoch 28: val_loss did not improve from 0.41787

Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2018 - roc_auc: 0.9785 - val_loss: 0.4233 - val_roc_auc: 0.8900 - learning_rate: 1.0000e-04
Epoch 29/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1861 - roc_auc: 0.9844
Epoch 29: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1860 - roc_auc: 0.9844 - val_loss: 0.4242 - val_roc_auc: 0.8898 - learning_rate: 1.0000e-05
Epoch 30/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1927 - roc_auc: 0.9846
Epoch 30: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1918 - roc_auc: 0.9842 - val_loss: 0.4254 - val_roc_auc: 0.8898 - learning_rate: 1.0000e-05
Epoch 31/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1899 - roc_auc: 0.9816
Epoch 31: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1903 - roc_auc: 0.9814 - val_loss: 0.4271 - val_roc_auc: 0.8901 - learning_rate: 1.0000e-05
Epoch 32/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1859 - roc_auc: 0.9826
Epoch 32: val_loss did not improve from 0.41787
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1861 - roc_auc: 0.9824 - val_loss: 0.4262 - val_roc_auc: 0.8898 - learning_rate: 1.0000e-05
Epoch 33/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1933 - roc_auc: 0.9804
Epoch 33: val_loss did not improve from 0.41787

Epoch 33: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1933 - roc_auc: 0.9804 - val_loss: 0.4261 - val_roc_auc: 0.8898 - learning_rate: 1.0000e-05
Epoch 33: early stopping
Restoring model weights from the end of the best epoch: 23.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9508, va:0.8879
-------------------- 1 --------------------
20240928 12:57:13
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.5602 - roc_auc: 0.8026
Epoch 1: val_loss improved from inf to 0.47624, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.5592 - roc_auc: 0.8039 - val_loss: 0.4762 - val_roc_auc: 0.9424 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4257 - roc_auc: 0.8843
Epoch 2: val_loss improved from 0.47624 to 0.42351, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4262 - roc_auc: 0.8842 - val_loss: 0.4235 - val_roc_auc: 0.9383 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4117 - roc_auc: 0.8925
Epoch 3: val_loss improved from 0.42351 to 0.39494, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4115 - roc_auc: 0.8934 - val_loss: 0.3949 - val_roc_auc: 0.9331 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3842 - roc_auc: 0.9074
Epoch 4: val_loss improved from 0.39494 to 0.37424, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3853 - roc_auc: 0.9070 - val_loss: 0.3742 - val_roc_auc: 0.9208 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3687 - roc_auc: 0.9121
Epoch 5: val_loss improved from 0.37424 to 0.34710, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3679 - roc_auc: 0.9126 - val_loss: 0.3471 - val_roc_auc: 0.9284 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3379 - roc_auc: 0.9297
Epoch 6: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3374 - roc_auc: 0.9299 - val_loss: 0.3842 - val_roc_auc: 0.9072 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3365 - roc_auc: 0.9318
Epoch 7: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3372 - roc_auc: 0.9311 - val_loss: 0.3682 - val_roc_auc: 0.9163 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3039 - roc_auc: 0.9451
Epoch 8: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3045 - roc_auc: 0.9446 - val_loss: 0.3661 - val_roc_auc: 0.9186 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3135 - roc_auc: 0.9408
Epoch 9: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3125 - roc_auc: 0.9411 - val_loss: 0.3977 - val_roc_auc: 0.9070 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2893 - roc_auc: 0.9504
Epoch 10: val_loss did not improve from 0.34710

Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2893 - roc_auc: 0.9504 - val_loss: 0.3658 - val_roc_auc: 0.9215 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2709 - roc_auc: 0.9576
Epoch 11: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2708 - roc_auc: 0.9576 - val_loss: 0.3559 - val_roc_auc: 0.9248 - learning_rate: 1.0000e-04
Epoch 12/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2750 - roc_auc: 0.9539
Epoch 12: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2728 - roc_auc: 0.9547 - val_loss: 0.3570 - val_roc_auc: 0.9243 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2446 - roc_auc: 0.9664
Epoch 13: val_loss did not improve from 0.34710
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2435 - roc_auc: 0.9666 - val_loss: 0.3498 - val_roc_auc: 0.9265 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2266 - roc_auc: 0.9712
Epoch 14: val_loss improved from 0.34710 to 0.34624, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2266 - roc_auc: 0.9713 - val_loss: 0.3462 - val_roc_auc: 0.9287 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2231 - roc_auc: 0.9745
Epoch 15: val_loss did not improve from 0.34624
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2231 - roc_auc: 0.9745 - val_loss: 0.3514 - val_roc_auc: 0.9256 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">70/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2285 - roc_auc: 0.9710
Epoch 16: val_loss did not improve from 0.34624
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2268 - roc_auc: 0.9714 - val_loss: 0.3506 - val_roc_auc: 0.9264 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2260 - roc_auc: 0.9744
Epoch 17: val_loss did not improve from 0.34624
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2243 - roc_auc: 0.9744 - val_loss: 0.3497 - val_roc_auc: 0.9276 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2155 - roc_auc: 0.9767
Epoch 18: val_loss improved from 0.34624 to 0.34076, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold1.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2158 - roc_auc: 0.9766 - val_loss: 0.3408 - val_roc_auc: 0.9305 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2122 - roc_auc: 0.9767
Epoch 19: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2116 - roc_auc: 0.9766 - val_loss: 0.3428 - val_roc_auc: 0.9286 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2139 - roc_auc: 0.9768
Epoch 20: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2139 - roc_auc: 0.9766 - val_loss: 0.3495 - val_roc_auc: 0.9274 - learning_rate: 1.0000e-04
Epoch 21/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2291 - roc_auc: 0.9712
Epoch 21: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2284 - roc_auc: 0.9714 - val_loss: 0.3427 - val_roc_auc: 0.9308 - learning_rate: 1.0000e-04
Epoch 22/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1853 - roc_auc: 0.9844
Epoch 22: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1859 - roc_auc: 0.9841 - val_loss: 0.3514 - val_roc_auc: 0.9277 - learning_rate: 1.0000e-04
Epoch 23/10000
<span class="ansi-bold">72/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1889 - roc_auc: 0.9818
Epoch 23: val_loss did not improve from 0.34076

Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1892 - roc_auc: 0.9816 - val_loss: 0.3424 - val_roc_auc: 0.9314 - learning_rate: 1.0000e-04
Epoch 24/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1918 - roc_auc: 0.9817
Epoch 24: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1921 - roc_auc: 0.9816 - val_loss: 0.3442 - val_roc_auc: 0.9310 - learning_rate: 1.0000e-05
Epoch 25/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1853 - roc_auc: 0.9844
Epoch 25: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1844 - roc_auc: 0.9844 - val_loss: 0.3445 - val_roc_auc: 0.9309 - learning_rate: 1.0000e-05
Epoch 26/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2162 - roc_auc: 0.9716
Epoch 26: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2158 - roc_auc: 0.9719 - val_loss: 0.3438 - val_roc_auc: 0.9305 - learning_rate: 1.0000e-05
Epoch 27/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1897 - roc_auc: 0.9843
Epoch 27: val_loss did not improve from 0.34076
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1904 - roc_auc: 0.9839 - val_loss: 0.3460 - val_roc_auc: 0.9302 - learning_rate: 1.0000e-05
Epoch 28/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2059 - roc_auc: 0.9774
Epoch 28: val_loss did not improve from 0.34076

Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2061 - roc_auc: 0.9773 - val_loss: 0.3463 - val_roc_auc: 0.9295 - learning_rate: 1.0000e-05
Epoch 28: early stopping
Restoring model weights from the end of the best epoch: 18.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9500, va:0.9302
-------------------- 2 --------------------
20240928 12:57:26
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.6325 - roc_auc: 0.7499
Epoch 1: val_loss improved from inf to 0.53014, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - loss: 0.6306 - roc_auc: 0.7514 - val_loss: 0.5301 - val_roc_auc: 0.9148 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4825 - roc_auc: 0.8572
Epoch 2: val_loss improved from 0.53014 to 0.46486, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4793 - roc_auc: 0.8588 - val_loss: 0.4649 - val_roc_auc: 0.9140 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">68/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4379 - roc_auc: 0.8831
Epoch 3: val_loss improved from 0.46486 to 0.42648, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4351 - roc_auc: 0.8839 - val_loss: 0.4265 - val_roc_auc: 0.9229 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4085 - roc_auc: 0.8952
Epoch 4: val_loss improved from 0.42648 to 0.40602, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4044 - roc_auc: 0.8972 - val_loss: 0.4060 - val_roc_auc: 0.9173 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3880 - roc_auc: 0.9092
Epoch 5: val_loss improved from 0.40602 to 0.39127, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3879 - roc_auc: 0.9092 - val_loss: 0.3913 - val_roc_auc: 0.9206 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3627 - roc_auc: 0.9206
Epoch 6: val_loss did not improve from 0.39127
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3627 - roc_auc: 0.9206 - val_loss: 0.4116 - val_roc_auc: 0.9150 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3592 - roc_auc: 0.9222
Epoch 7: val_loss improved from 0.39127 to 0.38583, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3565 - roc_auc: 0.9231 - val_loss: 0.3858 - val_roc_auc: 0.9267 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3421 - roc_auc: 0.9299
Epoch 8: val_loss did not improve from 0.38583
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3416 - roc_auc: 0.9301 - val_loss: 0.4072 - val_roc_auc: 0.9164 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2940 - roc_auc: 0.9501
Epoch 9: val_loss did not improve from 0.38583
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2934 - roc_auc: 0.9501 - val_loss: 0.4035 - val_roc_auc: 0.9169 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2765 - roc_auc: 0.9538
Epoch 10: val_loss did not improve from 0.38583
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2821 - roc_auc: 0.9517 - val_loss: 0.4069 - val_roc_auc: 0.9193 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2976 - roc_auc: 0.9479
Epoch 11: val_loss did not improve from 0.38583
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2955 - roc_auc: 0.9482 - val_loss: 0.3898 - val_roc_auc: 0.9205 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2732 - roc_auc: 0.9555
Epoch 12: val_loss improved from 0.38583 to 0.38346, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold2.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2728 - roc_auc: 0.9556 - val_loss: 0.3835 - val_roc_auc: 0.9270 - learning_rate: 0.0010
Epoch 13/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2944 - roc_auc: 0.9486
Epoch 13: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2920 - roc_auc: 0.9494 - val_loss: 0.4068 - val_roc_auc: 0.9164 - learning_rate: 0.0010
Epoch 14/10000
<span class="ansi-bold">64/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2464 - roc_auc: 0.9649
Epoch 14: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2469 - roc_auc: 0.9646 - val_loss: 0.3885 - val_roc_auc: 0.9300 - learning_rate: 0.0010
Epoch 15/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2661 - roc_auc: 0.9608
Epoch 15: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2655 - roc_auc: 0.9608 - val_loss: 0.4470 - val_roc_auc: 0.9169 - learning_rate: 0.0010
Epoch 16/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2063 - roc_auc: 0.9764
Epoch 16: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2089 - roc_auc: 0.9755 - val_loss: 0.4424 - val_roc_auc: 0.9213 - learning_rate: 0.0010
Epoch 17/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1953 - roc_auc: 0.9774
Epoch 17: val_loss did not improve from 0.38346

Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1975 - roc_auc: 0.9768 - val_loss: 0.4290 - val_roc_auc: 0.9236 - learning_rate: 0.0010
Epoch 18/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1724 - roc_auc: 0.9865
Epoch 18: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1748 - roc_auc: 0.9860 - val_loss: 0.4306 - val_roc_auc: 0.9253 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2102 - roc_auc: 0.9759
Epoch 19: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2093 - roc_auc: 0.9762 - val_loss: 0.4362 - val_roc_auc: 0.9208 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.1734 - roc_auc: 0.9861
Epoch 20: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1733 - roc_auc: 0.9861 - val_loss: 0.4364 - val_roc_auc: 0.9246 - learning_rate: 1.0000e-04
Epoch 21/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1809 - roc_auc: 0.9831
Epoch 21: val_loss did not improve from 0.38346
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1804 - roc_auc: 0.9833 - val_loss: 0.4418 - val_roc_auc: 0.9223 - learning_rate: 1.0000e-04
Epoch 22/10000
<span class="ansi-bold">80/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1566 - roc_auc: 0.9884
Epoch 22: val_loss did not improve from 0.38346

Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1566 - roc_auc: 0.9884 - val_loss: 0.4424 - val_roc_auc: 0.9235 - learning_rate: 1.0000e-04
Epoch 22: early stopping
Restoring model weights from the end of the best epoch: 12.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9405, va:0.9269
-------------------- 3 --------------------
20240928 12:57:36
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.6539 - roc_auc: 0.7554
Epoch 1: val_loss improved from inf to 0.51066, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 7ms/step - loss: 0.6489 - roc_auc: 0.7588 - val_loss: 0.5107 - val_roc_auc: 0.9158 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.4174 - roc_auc: 0.8968
Epoch 2: val_loss improved from 0.51066 to 0.44970, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4176 - roc_auc: 0.8967 - val_loss: 0.4497 - val_roc_auc: 0.9128 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3838 - roc_auc: 0.9093
Epoch 3: val_loss improved from 0.44970 to 0.41891, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3841 - roc_auc: 0.9092 - val_loss: 0.4189 - val_roc_auc: 0.9141 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3603 - roc_auc: 0.9192
Epoch 4: val_loss improved from 0.41891 to 0.39361, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3627 - roc_auc: 0.9181 - val_loss: 0.3936 - val_roc_auc: 0.9123 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">69/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.3388 - roc_auc: 0.9303
Epoch 5: val_loss improved from 0.39361 to 0.37704, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3429 - roc_auc: 0.9283 - val_loss: 0.3770 - val_roc_auc: 0.9167 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3144 - roc_auc: 0.9421
Epoch 6: val_loss improved from 0.37704 to 0.36579, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold3.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3165 - roc_auc: 0.9411 - val_loss: 0.3658 - val_roc_auc: 0.9205 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">65/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2878 - roc_auc: 0.9504
Epoch 7: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2950 - roc_auc: 0.9474 - val_loss: 0.3793 - val_roc_auc: 0.9103 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">66/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2872 - roc_auc: 0.9522
Epoch 8: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2910 - roc_auc: 0.9505 - val_loss: 0.3986 - val_roc_auc: 0.9076 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">81/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3057 - roc_auc: 0.9431
Epoch 9: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3065 - roc_auc: 0.9429 - val_loss: 0.3951 - val_roc_auc: 0.9106 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">63/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2851 - roc_auc: 0.9508
Epoch 10: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2907 - roc_auc: 0.9492 - val_loss: 0.4230 - val_roc_auc: 0.9038 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2610 - roc_auc: 0.9603
Epoch 11: val_loss did not improve from 0.36579

Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2615 - roc_auc: 0.9601 - val_loss: 0.4134 - val_roc_auc: 0.9080 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2353 - roc_auc: 0.9686
Epoch 12: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2356 - roc_auc: 0.9685 - val_loss: 0.4006 - val_roc_auc: 0.9119 - learning_rate: 1.0000e-04
Epoch 13/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2210 - roc_auc: 0.9736
Epoch 13: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2271 - roc_auc: 0.9717 - val_loss: 0.3968 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-04
Epoch 14/10000
<span class="ansi-bold">67/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2192 - roc_auc: 0.9739
Epoch 14: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2238 - roc_auc: 0.9723 - val_loss: 0.3939 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-04
Epoch 15/10000
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2169 - roc_auc: 0.9734
Epoch 15: val_loss did not improve from 0.36579
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2169 - roc_auc: 0.9734 - val_loss: 0.3946 - val_roc_auc: 0.9141 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.2147 - roc_auc: 0.9731
Epoch 16: val_loss did not improve from 0.36579

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2166 - roc_auc: 0.9726 - val_loss: 0.3955 - val_roc_auc: 0.9130 - learning_rate: 1.0000e-04
Epoch 16: early stopping
Restoring model weights from the end of the best epoch: 6.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 7ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9436, va:0.9206
-------------------- 4 --------------------
20240928 12:57:44
(680, 8) (170, 8)
trainning start!
Epoch 1/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.5664 - roc_auc: 0.8053
Epoch 1: val_loss improved from inf to 0.50199, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - loss: 0.5618 - roc_auc: 0.8088 - val_loss: 0.5020 - val_roc_auc: 0.9066 - learning_rate: 0.0010
Epoch 2/10000
<span class="ansi-bold">82/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4713 - roc_auc: 0.8646
Epoch 2: val_loss improved from 0.50199 to 0.47330, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.4707 - roc_auc: 0.8648 - val_loss: 0.4733 - val_roc_auc: 0.8816 - learning_rate: 0.0010
Epoch 3/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.4107 - roc_auc: 0.8950
Epoch 3: val_loss improved from 0.47330 to 0.43743, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.4111 - roc_auc: 0.8945 - val_loss: 0.4374 - val_roc_auc: 0.8891 - learning_rate: 0.0010
Epoch 4/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3655 - roc_auc: 0.9161
Epoch 4: val_loss improved from 0.43743 to 0.39902, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3665 - roc_auc: 0.9154 - val_loss: 0.3990 - val_roc_auc: 0.9012 - learning_rate: 0.0010
Epoch 5/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3698 - roc_auc: 0.9147
Epoch 5: val_loss did not improve from 0.39902
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3705 - roc_auc: 0.9142 - val_loss: 0.4005 - val_roc_auc: 0.9025 - learning_rate: 0.0010
Epoch 6/10000
<span class="ansi-bold">71/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3600 - roc_auc: 0.9204
Epoch 6: val_loss improved from 0.39902 to 0.38825, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3615 - roc_auc: 0.9194 - val_loss: 0.3882 - val_roc_auc: 0.9067 - learning_rate: 0.0010
Epoch 7/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3419 - roc_auc: 0.9263
Epoch 7: val_loss did not improve from 0.38825
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3417 - roc_auc: 0.9264 - val_loss: 0.3936 - val_roc_auc: 0.9086 - learning_rate: 0.0010
Epoch 8/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3285 - roc_auc: 0.9313
Epoch 8: val_loss did not improve from 0.38825
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3277 - roc_auc: 0.9318 - val_loss: 0.4068 - val_roc_auc: 0.9019 - learning_rate: 0.0010
Epoch 9/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.3194 - roc_auc: 0.9403
Epoch 9: val_loss improved from 0.38825 to 0.38055, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.3181 - roc_auc: 0.9402 - val_loss: 0.3805 - val_roc_auc: 0.9136 - learning_rate: 0.0010
Epoch 10/10000
<span class="ansi-bold">74/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2882 - roc_auc: 0.9504
Epoch 10: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2872 - roc_auc: 0.9507 - val_loss: 0.3975 - val_roc_auc: 0.9123 - learning_rate: 0.0010
Epoch 11/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2713 - roc_auc: 0.9557
Epoch 11: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2718 - roc_auc: 0.9555 - val_loss: 0.4149 - val_roc_auc: 0.9057 - learning_rate: 0.0010
Epoch 12/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2848 - roc_auc: 0.9533
Epoch 12: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2868 - roc_auc: 0.9523 - val_loss: 0.4054 - val_roc_auc: 0.9085 - learning_rate: 0.0010
Epoch 13/10000
<span class="ansi-bold">73/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2687 - roc_auc: 0.9573
Epoch 13: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2679 - roc_auc: 0.9575 - val_loss: 0.3900 - val_roc_auc: 0.9131 - learning_rate: 0.0010
Epoch 14/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2549 - roc_auc: 0.9634
Epoch 14: val_loss did not improve from 0.38055

Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2528 - roc_auc: 0.9638 - val_loss: 0.4159 - val_roc_auc: 0.9087 - learning_rate: 0.0010
Epoch 15/10000
<span class="ansi-bold">78/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2413 - roc_auc: 0.9667
Epoch 15: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2390 - roc_auc: 0.9673 - val_loss: 0.4008 - val_roc_auc: 0.9108 - learning_rate: 1.0000e-04
Epoch 16/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2238 - roc_auc: 0.9693
Epoch 16: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2230 - roc_auc: 0.9697 - val_loss: 0.3957 - val_roc_auc: 0.9114 - learning_rate: 1.0000e-04
Epoch 17/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2091 - roc_auc: 0.9763
Epoch 17: val_loss did not improve from 0.38055
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2079 - roc_auc: 0.9767 - val_loss: 0.3864 - val_roc_auc: 0.9137 - learning_rate: 1.0000e-04
Epoch 18/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2051 - roc_auc: 0.9759
Epoch 18: val_loss improved from 0.38055 to 0.37843, saving model to /opt/src/output/exe023_stack_0928/model/model_tf_fold4.weights.h5
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2045 - roc_auc: 0.9760 - val_loss: 0.3784 - val_roc_auc: 0.9162 - learning_rate: 1.0000e-04
Epoch 19/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1743 - roc_auc: 0.9891
Epoch 19: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1745 - roc_auc: 0.9889 - val_loss: 0.3805 - val_roc_auc: 0.9144 - learning_rate: 1.0000e-04
Epoch 20/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.2019 - roc_auc: 0.9793
Epoch 20: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.2018 - roc_auc: 0.9793 - val_loss: 0.3786 - val_roc_auc: 0.9162 - learning_rate: 1.0000e-04
Epoch 21/10000
<span class="ansi-bold">77/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1793 - roc_auc: 0.9867
Epoch 21: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1795 - roc_auc: 0.9864 - val_loss: 0.3893 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-04
Epoch 22/10000
<span class="ansi-bold">75/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1978 - roc_auc: 0.9781
Epoch 22: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1971 - roc_auc: 0.9783 - val_loss: 0.3891 - val_roc_auc: 0.9138 - learning_rate: 1.0000e-04
Epoch 23/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1849 - roc_auc: 0.9842
Epoch 23: val_loss did not improve from 0.37843

Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1843 - roc_auc: 0.9842 - val_loss: 0.3887 - val_roc_auc: 0.9144 - learning_rate: 1.0000e-04
Epoch 24/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1905 - roc_auc: 0.9827
Epoch 24: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1898 - roc_auc: 0.9827 - val_loss: 0.3889 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-05
Epoch 25/10000
<span class="ansi-bold">76/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1977 - roc_auc: 0.9796
Epoch 25: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1972 - roc_auc: 0.9796 - val_loss: 0.3893 - val_roc_auc: 0.9143 - learning_rate: 1.0000e-05
Epoch 26/10000
<span class="ansi-bold">83/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1763 - roc_auc: 0.9860
Epoch 26: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1764 - roc_auc: 0.9860 - val_loss: 0.3897 - val_roc_auc: 0.9134 - learning_rate: 1.0000e-05
Epoch 27/10000
<span class="ansi-bold">84/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1774 - roc_auc: 0.9834
Epoch 27: val_loss did not improve from 0.37843
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1774 - roc_auc: 0.9834 - val_loss: 0.3897 - val_roc_auc: 0.9138 - learning_rate: 1.0000e-05
Epoch 28/10000
<span class="ansi-bold">79/85</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1886 - roc_auc: 0.9810
Epoch 28: val_loss did not improve from 0.37843

Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
<span class="ansi-bold">85/85</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.1870 - roc_auc: 0.9814 - val_loss: 0.3880 - val_roc_auc: 0.9138 - learning_rate: 1.0000e-05
Epoch 28: early stopping
Restoring model weights from the end of the best epoch: 18.
<span class="ansi-bold">22/22</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 6ms/step
<span class="ansi-bold">6/6</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 2ms/step 
[auc] tr:0.9505, va:0.9164
---------- result ----------
[[0.         0.95076643 0.88785965]
 [1.         0.94999606 0.93024561]
 [2.         0.94049371 0.92693169]
 [3.         0.94362977 0.9206327 ]
 [4.         0.9504888  0.91643337]]
[cv] tr:0.9471+-0.0042,         va:0.9164+-0.0042
[oof]0.9166
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=a30ea78b-cf27-47b1-b425-7e35e43d9636">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">df_train_las</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">"pred1"</span><span class="p">:</span> <span class="n">train_oof_lgbm</span><span class="p">[</span><span class="s2">"pred"</span><span class="p">],</span>
        <span class="s2">"pred2"</span><span class="p">:</span> <span class="n">train_oof_tf</span><span class="p">[</span><span class="s2">"pred"</span><span class="p">],</span>
        <span class="s2">"true"</span><span class="p">:</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="c1"># </span>
<span class="n">x_train_las</span> <span class="o">=</span> <span class="n">df_train_las</span><span class="p">[[</span><span class="s2">"pred1"</span><span class="p">,</span> <span class="s2">"pred2"</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=97ea5fff-5c5d-4969-af09-b314820565ab">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="n">oof_las</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train_las</span><span class="p">))</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">cv</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
    <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="n">x_train_las</span><span class="p">,</span> <span class="n">y_train</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 1.</span>
<span class="k">for</span> <span class="n">nfold</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dt_now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y%m</span><span class="si">%d</span><span class="s2"> %H:%M:%S"</span><span class="p">))</span>

    <span class="n">idx_tr</span><span class="p">,</span> <span class="n">idx_va</span> <span class="o">=</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cv</span><span class="p">[</span><span class="n">nfold</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">x_train_las</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">,</span> <span class="p">:],</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">idx_tr</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">x_va</span><span class="p">,</span> <span class="n">y_va</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">x_train_las</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_va</span><span class="p">,</span> <span class="p">:],</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">idx_va</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">x_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_va</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># # </span>
    <span class="c1"># fname_las = os.path.join(EXP_MODEL, f"model_las_fold{nfold}.pickle")</span>

    <span class="c1"># if not os.path.isfile(</span>
    <span class="c1">#     os.path.join(EXP_MODEL, f"model_las_fold{nfold}.pickle")</span>
    <span class="c1"># ):</span>
    <span class="c1"># if trained model, no training</span>
    <span class="c1"># train</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-------training start-------"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1">#     # </span>
    <span class="c1">#     with open(fname_las, "wb") as f:</span>
    <span class="c1">#         pickle.dump(model, f, protocol=4)</span>

    <span class="c1"># else:</span>
    <span class="c1">#     print("")</span>
    <span class="c1">#     with open(fname_las, "rb") as f:</span>
    <span class="c1">#         model = pickle.load(f)</span>

    <span class="c1"># evaluate</span>
    <span class="n">y_va_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_va</span><span class="p">)</span>
    <span class="n">oof_las</span><span class="p">[</span><span class="n">idx_va</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_va_pred</span>

<span class="n">df_train_las</span><span class="p">[</span><span class="s2">"pred_ensemble3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">oof_las</span>
<span class="n">df_train_las</span><span class="p">[</span><span class="s2">"pred_ensemble3"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_train_las</span><span class="p">[</span><span class="s2">"pred_ensemble3"</span><span class="p">]</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_train_las</span><span class="p">[[</span><span class="s2">"true"</span><span class="p">,</span> <span class="s2">"pred1"</span><span class="p">,</span> <span class="s2">"pred2"</span><span class="p">,</span> <span class="s2">"pred_ensemble3"</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>-------------------- 0 --------------------
20240928 12:58:48
(680, 2) (170, 2)
-------training start-------
-------------------- 1 --------------------
20240928 12:58:48
(680, 2) (170, 2)
-------training start-------
-------------------- 2 --------------------
20240928 12:58:48
(680, 2) (170, 2)
-------training start-------
-------------------- 3 --------------------
20240928 12:58:48
(680, 2) (170, 2)
-------training start-------
-------------------- 4 --------------------
20240928 12:58:48
(680, 2) (170, 2)
-------training start-------
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>true</th>
<th>pred1</th>
<th>pred2</th>
<th>pred_ensemble3</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>0</td>
<td>0.0515</td>
<td>0.1131</td>
<td>0.1049</td>
</tr>
<tr>
<th>1</th>
<td>1</td>
<td>0.9889</td>
<td>0.9848</td>
<td>0.9464</td>
</tr>
<tr>
<th>2</th>
<td>0</td>
<td>0.0262</td>
<td>0.2466</td>
<td>0.1395</td>
</tr>
<tr>
<th>3</th>
<td>1</td>
<td>0.9902</td>
<td>0.9554</td>
<td>0.9390</td>
</tr>
<tr>
<th>4</th>
<td>0</td>
<td>0.7165</td>
<td>0.5662</td>
<td>0.6725</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=63dc59b9-86b5-416d-b90f-28794678233c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_ensemble</span><span class="p">(</span><span class="n">input_df</span><span class="p">,</span> <span class="n">col_pred</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">'[auc] model1:</span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">input_df</span><span class="p">[</span><span class="s2">"true"</span><span class="p">],</span><span class="w"> </span><span class="n">input_df</span><span class="p">[</span><span class="s2">"pred1"</span><span class="p">])</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, model2:</span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">input_df</span><span class="p">[</span><span class="s2">"true"</span><span class="p">],</span><span class="w"> </span><span class="n">input_df</span><span class="p">[</span><span class="s2">"pred2"</span><span class="p">])</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">,  -&gt; ensemble:</span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">input_df</span><span class="p">[</span><span class="s2">"true"</span><span class="p">],</span><span class="w"> </span><span class="n">input_df</span><span class="p">[</span><span class="n">col_pred</span><span class="p">])</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">'</span>
    <span class="p">)</span>


<span class="n">evaluate_ensemble</span><span class="p">(</span><span class="n">df_train_las</span><span class="p">,</span> <span class="n">col_pred</span><span class="o">=</span><span class="s2">"pred_ensemble3"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[auc] model1:0.9420, model2:0.9166,  -&gt; ensemble:0.9405
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
